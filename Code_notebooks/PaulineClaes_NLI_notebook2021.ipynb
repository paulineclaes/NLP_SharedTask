{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "requested-destruction",
   "metadata": {},
   "source": [
    "# NLP Shared Task 2021\n",
    "\n",
    "## Native Language Identification\n",
    "Pauline Claes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "found-soccer",
   "metadata": {},
   "source": [
    "## 1. Loading modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "separate-australia",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/paulineclaes/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sn \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, plot_confusion_matrix, accuracy_score, f1_score\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk__word_tokenizer = word_tokenize\n",
    "nltk.download(\"punkt\")\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "from spacy.lang.en import English\n",
    "from nltk.corpus import stopwords\n",
    "stop_words_list = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "better-processor",
   "metadata": {},
   "source": [
    "## 2. Preprocessing of the train data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "promising-formula",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "criminal-basic",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TUR    1000\n",
       "KOR    1000\n",
       "JPN    1000\n",
       "ARA    1000\n",
       "FRA    1000\n",
       "TEL    1000\n",
       "ITA    1000\n",
       "SPA    1000\n",
       "HIN    1000\n",
       "DEU    1000\n",
       "ZHO    1000\n",
       "Name: Language, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Language.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fewer-crest",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>text</th>\n",
       "      <th>Language</th>\n",
       "      <th>Proficiency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88.txt</td>\n",
       "      <td>Some people might think that traveling in a gr...</td>\n",
       "      <td>KOR</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>278.txt</td>\n",
       "      <td>IThe importance and popularity of travelling i...</td>\n",
       "      <td>DEU</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>348.txt</td>\n",
       "      <td>It is an important decision, how to plan your ...</td>\n",
       "      <td>TUR</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>666.txt</td>\n",
       "      <td>Some people believe that young people can enjo...</td>\n",
       "      <td>ZHO</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>733.txt</td>\n",
       "      <td>Travelling is  usually considered as good recr...</td>\n",
       "      <td>TEL</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Filename                                               text Language  \\\n",
       "0   88.txt  Some people might think that traveling in a gr...      KOR   \n",
       "1  278.txt  IThe importance and popularity of travelling i...      DEU   \n",
       "2  348.txt  It is an important decision, how to plan your ...      TUR   \n",
       "3  666.txt  Some people believe that young people can enjo...      ZHO   \n",
       "4  733.txt  Travelling is  usually considered as good recr...      TEL   \n",
       "\n",
       "  Proficiency  \n",
       "0        high  \n",
       "1      medium  \n",
       "2        high  \n",
       "3      medium  \n",
       "4      medium  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "hollywood-strategy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tag(doc):\n",
    "    return \" \".join([token.tag_ for token in nlp(doc)])\n",
    "\n",
    "def lemmatizer(doc):\n",
    "    return \" \".join([token.lemma_ for token in nlp(doc)])\n",
    "\n",
    "def stop_words(doc):\n",
    "    return \" \".join([w for w in doc.lower().split() if w in stop_words_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "continuous-boating",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding pos_tag column\n",
    "df[\"pos_tags\"] = df.text.apply(pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "weekly-incidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding lemmatized text column\n",
    "df[\"lemma_text\"] = df.text.apply(lemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "transsexual-destruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding column with stop_words (function words)\n",
    "df[\"stop_words\"] = df.text.apply(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "given-proposal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding column with text length in number of words\n",
    "df[\"n_words\"] = df[\"text\"].str.split().str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "tired-anthony",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding column with text length in number of sentences\n",
    "df[\"n_sentences\"] = [text.count(\".\") for text in df[\"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "raising-recorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding column with text length in number of characters\n",
    "df[\"doc_length\"] = df.text.apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "formal-australia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving preprocessed DataFrame to CSV, to not have to run it again \n",
    "# every time. This df will be read in again, and we will move forward\n",
    "# with this dataframe.\n",
    "# df.to_csv(\"train_preprocessed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "absent-parallel",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train_preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "exciting-prayer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>text</th>\n",
       "      <th>Language</th>\n",
       "      <th>Proficiency</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>lemma_text</th>\n",
       "      <th>n_words</th>\n",
       "      <th>n_sentences</th>\n",
       "      <th>doc_length</th>\n",
       "      <th>stop_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88.txt</td>\n",
       "      <td>Some people might think that traveling in a gr...</td>\n",
       "      <td>KOR</td>\n",
       "      <td>high</td>\n",
       "      <td>DT NNS MD VB IN VBG IN DT NN VBN IN DT NN NN V...</td>\n",
       "      <td>some people may think that travel in a group l...</td>\n",
       "      <td>384</td>\n",
       "      <td>16</td>\n",
       "      <td>1940</td>\n",
       "      <td>some that in a by a is a a has its and does no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>278.txt</td>\n",
       "      <td>IThe importance and popularity of travelling i...</td>\n",
       "      <td>DEU</td>\n",
       "      <td>medium</td>\n",
       "      <td>IN NN CC NN IN VBG VBZ RB VBG , _SP NN VBZ JJ ...</td>\n",
       "      <td>ithe importance and popularity of travel be st...</td>\n",
       "      <td>321</td>\n",
       "      <td>13</td>\n",
       "      <td>1645</td>\n",
       "      <td>and of is is in to other and but the how to do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>348.txt</td>\n",
       "      <td>It is an important decision, how to plan your ...</td>\n",
       "      <td>TUR</td>\n",
       "      <td>high</td>\n",
       "      <td>PRP VBZ DT JJ NN , WRB TO VB PRP$ NN . DT NNS ...</td>\n",
       "      <td>-PRON- be an important decision , how to plan ...</td>\n",
       "      <td>360</td>\n",
       "      <td>15</td>\n",
       "      <td>2022</td>\n",
       "      <td>it is an how to your some to a of and their so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>666.txt</td>\n",
       "      <td>Some people believe that young people can enjo...</td>\n",
       "      <td>ZHO</td>\n",
       "      <td>medium</td>\n",
       "      <td>DT NNS VBP IN JJ NNS MD VB NN JJR IN JJR NN VB...</td>\n",
       "      <td>some people believe that young people can enjo...</td>\n",
       "      <td>347</td>\n",
       "      <td>26</td>\n",
       "      <td>1891</td>\n",
       "      <td>some that can more than from my of the is the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>733.txt</td>\n",
       "      <td>Travelling is  usually considered as good recr...</td>\n",
       "      <td>TEL</td>\n",
       "      <td>medium</td>\n",
       "      <td>NNP VBZ _SP RB VBN IN JJ NN _SP IN JJ NNS , IN...</td>\n",
       "      <td>Travelling be   usually consider as good recre...</td>\n",
       "      <td>349</td>\n",
       "      <td>13</td>\n",
       "      <td>1862</td>\n",
       "      <td>is as by as for and it is to have some and a w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Filename                                               text Language  \\\n",
       "0   88.txt  Some people might think that traveling in a gr...      KOR   \n",
       "1  278.txt  IThe importance and popularity of travelling i...      DEU   \n",
       "2  348.txt  It is an important decision, how to plan your ...      TUR   \n",
       "3  666.txt  Some people believe that young people can enjo...      ZHO   \n",
       "4  733.txt  Travelling is  usually considered as good recr...      TEL   \n",
       "\n",
       "  Proficiency                                           pos_tags  \\\n",
       "0        high  DT NNS MD VB IN VBG IN DT NN VBN IN DT NN NN V...   \n",
       "1      medium  IN NN CC NN IN VBG VBZ RB VBG , _SP NN VBZ JJ ...   \n",
       "2        high  PRP VBZ DT JJ NN , WRB TO VB PRP$ NN . DT NNS ...   \n",
       "3      medium  DT NNS VBP IN JJ NNS MD VB NN JJR IN JJR NN VB...   \n",
       "4      medium  NNP VBZ _SP RB VBN IN JJ NN _SP IN JJ NNS , IN...   \n",
       "\n",
       "                                          lemma_text  n_words  n_sentences  \\\n",
       "0  some people may think that travel in a group l...      384           16   \n",
       "1  ithe importance and popularity of travel be st...      321           13   \n",
       "2  -PRON- be an important decision , how to plan ...      360           15   \n",
       "3  some people believe that young people can enjo...      347           26   \n",
       "4  Travelling be   usually consider as good recre...      349           13   \n",
       "\n",
       "   doc_length                                         stop_words  \n",
       "0        1940  some that in a by a is a a has its and does no...  \n",
       "1        1645  and of is is in to other and but the how to do...  \n",
       "2        2022  it is an how to your some to a of and their so...  \n",
       "3        1891  some that can more than from my of the is the ...  \n",
       "4        1862  is as by as for and it is to have some and a w...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "direct-polyester",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11000 entries, 0 to 10999\n",
      "Data columns (total 10 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Filename     11000 non-null  object\n",
      " 1   text         11000 non-null  object\n",
      " 2   Language     11000 non-null  object\n",
      " 3   Proficiency  11000 non-null  object\n",
      " 4   pos_tags     11000 non-null  object\n",
      " 5   lemma_text   11000 non-null  object\n",
      " 6   n_words      11000 non-null  int64 \n",
      " 7   n_sentences  11000 non-null  int64 \n",
      " 8   doc_length   11000 non-null  int64 \n",
      " 9   stop_words   11000 non-null  object\n",
      "dtypes: int64(3), object(7)\n",
      "memory usage: 859.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "sixth-coaching",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_words</th>\n",
       "      <th>n_sentences</th>\n",
       "      <th>doc_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>11000.000000</td>\n",
       "      <td>11000.000000</td>\n",
       "      <td>11000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>315.093182</td>\n",
       "      <td>16.100909</td>\n",
       "      <td>1787.637182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>77.407150</td>\n",
       "      <td>5.716632</td>\n",
       "      <td>517.057569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>278.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>1567.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>315.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>1784.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>355.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>2017.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>799.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>29870.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            n_words   n_sentences    doc_length\n",
       "count  11000.000000  11000.000000  11000.000000\n",
       "mean     315.093182     16.100909   1787.637182\n",
       "std       77.407150      5.716632    517.057569\n",
       "min        2.000000      0.000000      9.000000\n",
       "25%      278.000000     12.000000   1567.000000\n",
       "50%      315.000000     16.000000   1784.000000\n",
       "75%      355.000000     20.000000   2017.000000\n",
       "max      799.000000     74.000000  29870.000000"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "exempt-constitutional",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HIN    1000\n",
       "SPA    1000\n",
       "FRA    1000\n",
       "JPN    1000\n",
       "TUR    1000\n",
       "DEU    1000\n",
       "KOR    1000\n",
       "ARA    1000\n",
       "ITA    1000\n",
       "ZHO    1000\n",
       "TEL    1000\n",
       "Name: Language, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Language.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "japanese-native",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.text.values\n",
    "y = df.Language.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "after-gambling",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_dev, y_train, y_dev = train_test_split(X,\n",
    "                                                  y, \n",
    "                                                  test_size=0.1,\n",
    "                                                  random_state=1,\n",
    "                                                  shuffle=True,\n",
    "                                                  stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cordless-place",
   "metadata": {},
   "source": [
    "## 3. Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "toxic-photograph",
   "metadata": {},
   "source": [
    "### 3.1. Dummy Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ready-induction",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "catholic-holder",
   "metadata": {},
   "source": [
    "#### 3.1.1. Dummy Classifier with CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "duplicate-simpson",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvect = CountVectorizer()\n",
    "dummy_clf = DummyClassifier(strategy=\"stratified\", random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "regulated-presentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cvect = cvect.fit_transform(X_train)\n",
    "X_dev_cvect = cvect.transform(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "afraid-clarity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DummyClassifier(random_state=1, strategy='stratified')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_clf.fit(X_train_cvect, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "exact-messenger",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_dummy = dummy_clf.predict(X_dev_cvect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "contained-southeast",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08363636363636363\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_dev, y_pred_dummy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "corrected-mount",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ARA       0.06      0.05      0.05       100\n",
      "         DEU       0.08      0.09      0.08       100\n",
      "         FRA       0.09      0.09      0.09       100\n",
      "         HIN       0.07      0.08      0.08       100\n",
      "         ITA       0.06      0.05      0.06       100\n",
      "         JPN       0.09      0.09      0.09       100\n",
      "         KOR       0.08      0.08      0.08       100\n",
      "         SPA       0.07      0.07      0.07       100\n",
      "         TEL       0.14      0.17      0.16       100\n",
      "         TUR       0.11      0.12      0.12       100\n",
      "         ZHO       0.03      0.03      0.03       100\n",
      "\n",
      "    accuracy                           0.08      1100\n",
      "   macro avg       0.08      0.08      0.08      1100\n",
      "weighted avg       0.08      0.08      0.08      1100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_dev, y_pred_dummy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removed-titanium",
   "metadata": {},
   "source": [
    "#### 3.1.2. Dummy Classifier with TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ignored-balance",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "dummy_clf = DummyClassifier(strategy=\"stratified\", random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "legitimate-clark",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_dev_tfidf = tfidf.transform(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "broken-drilling",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DummyClassifier(random_state=1, strategy='stratified')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_clf.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "indirect-position",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_dummy = dummy_clf.predict(X_dev_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "described-outdoors",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08363636363636363\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_dev, y_pred_dummy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "experimental-imperial",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ARA       0.06      0.05      0.05       100\n",
      "         DEU       0.08      0.09      0.08       100\n",
      "         FRA       0.09      0.09      0.09       100\n",
      "         HIN       0.07      0.08      0.08       100\n",
      "         ITA       0.06      0.05      0.06       100\n",
      "         JPN       0.09      0.09      0.09       100\n",
      "         KOR       0.08      0.08      0.08       100\n",
      "         SPA       0.07      0.07      0.07       100\n",
      "         TEL       0.14      0.17      0.16       100\n",
      "         TUR       0.11      0.12      0.12       100\n",
      "         ZHO       0.03      0.03      0.03       100\n",
      "\n",
      "    accuracy                           0.08      1100\n",
      "   macro avg       0.08      0.08      0.08      1100\n",
      "weighted avg       0.08      0.08      0.08      1100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_dev, y_pred_dummy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incredible-alexander",
   "metadata": {},
   "source": [
    "### 3.2. Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virgin-discussion",
   "metadata": {},
   "source": [
    "#### 3.2.1. SVM with CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "needed-finland",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "anonymous-thermal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.fit(X_train_cvect, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "mysterious-exclusive",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_svm = svm.predict(X_dev_cvect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "olympic-vatican",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6036363636363636\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_dev, y_pred_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "monthly-access",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ARA       0.55      0.55      0.55       100\n",
      "         DEU       0.76      0.72      0.74       100\n",
      "         FRA       0.60      0.61      0.60       100\n",
      "         HIN       0.56      0.58      0.57       100\n",
      "         ITA       0.66      0.66      0.66       100\n",
      "         JPN       0.51      0.60      0.55       100\n",
      "         KOR       0.59      0.52      0.55       100\n",
      "         SPA       0.52      0.54      0.53       100\n",
      "         TEL       0.64      0.68      0.66       100\n",
      "         TUR       0.68      0.58      0.63       100\n",
      "         ZHO       0.61      0.60      0.60       100\n",
      "\n",
      "    accuracy                           0.60      1100\n",
      "   macro avg       0.61      0.60      0.60      1100\n",
      "weighted avg       0.61      0.60      0.60      1100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_dev, y_pred_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thousand-commissioner",
   "metadata": {},
   "source": [
    "#### 3.2.2. SVM with TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fleet-probe",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "utility-thanksgiving",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "selected-loading",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_svm = svm.predict(X_dev_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "separated-decade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7227272727272728\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_dev, y_pred_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "touched-cigarette",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ARA       0.70      0.63      0.66       100\n",
      "         DEU       0.84      0.83      0.83       100\n",
      "         FRA       0.76      0.73      0.74       100\n",
      "         HIN       0.59      0.69      0.64       100\n",
      "         ITA       0.87      0.77      0.81       100\n",
      "         JPN       0.69      0.73      0.71       100\n",
      "         KOR       0.71      0.66      0.68       100\n",
      "         SPA       0.67      0.74      0.70       100\n",
      "         TEL       0.75      0.70      0.73       100\n",
      "         TUR       0.79      0.77      0.78       100\n",
      "         ZHO       0.64      0.70      0.67       100\n",
      "\n",
      "    accuracy                           0.72      1100\n",
      "   macro avg       0.73      0.72      0.72      1100\n",
      "weighted avg       0.73      0.72      0.72      1100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_dev, y_pred_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accompanied-johnson",
   "metadata": {},
   "source": [
    "### 3.3. BERT\n",
    "Please check separate notebook for BERT baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minus-canvas",
   "metadata": {},
   "source": [
    "##  4. Pipeline models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternative-cedar",
   "metadata": {},
   "source": [
    "### 4.1. Pipe 1. Logistic Regression and TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cubic-residence",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression() # default parameters\n",
    "tfidf = TfidfVectorizer() # default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "beginning-darwin",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe1 = make_pipeline(tfidf, log_reg, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "parallel-easter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ... (step 1 of 2) Processing tfidfvectorizer, total=   2.0s\n",
      "[Pipeline]  (step 2 of 2) Processing logisticregression, total=  10.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paulineclaes/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidfvectorizer', TfidfVectorizer()),\n",
       "                ('logisticregression', LogisticRegression())],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "third-wallet",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_pipe1 = pipe1.predict(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "harmful-execution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7190909090909091\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_dev, y_pred_pipe1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "median-paintball",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ARA       0.76      0.59      0.66       100\n",
      "         DEU       0.82      0.85      0.83       100\n",
      "         FRA       0.77      0.78      0.78       100\n",
      "         HIN       0.60      0.66      0.63       100\n",
      "         ITA       0.76      0.77      0.77       100\n",
      "         JPN       0.67      0.68      0.68       100\n",
      "         KOR       0.72      0.68      0.70       100\n",
      "         SPA       0.70      0.67      0.68       100\n",
      "         TEL       0.70      0.74      0.72       100\n",
      "         TUR       0.76      0.74      0.75       100\n",
      "         ZHO       0.68      0.75      0.71       100\n",
      "\n",
      "    accuracy                           0.72      1100\n",
      "   macro avg       0.72      0.72      0.72      1100\n",
      "weighted avg       0.72      0.72      0.72      1100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_dev, y_pred_pipe1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polished-reduction",
   "metadata": {},
   "source": [
    "#### 4.1.1. Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "complete-stone",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe1_params = [{'tfidfvectorizer__lowercase' : [True, False], \n",
    "                 'tfidfvectorizer__max_df' : [0.70, 0.75, 0.80, 0.85, 0.90, 1.0],\n",
    "                 'tfidfvectorizer__min_df' : [0.001, 0.01, 0.1],\n",
    "                 'tfidfvectorizer__ngram_range' : [(1,1), (1,2), (1,3), (1,4)],\n",
    "                 'tfidfvectorizer__norm' : ['l1', 'l2'],\n",
    "                 'tfidfvectorizer__tokenizer' : [nltk__word_tokenizer],\n",
    "                 'logisticregression__C' : [0.01, 0.1, 1.0, 10.0],\n",
    "                 'logisticregression__max_iter': [1000, 2500, 5000],\n",
    "                 'logisticregression__multi_class' : ['ovr', 'multinomial']}]\n",
    "\n",
    "rs_pipe1 = RandomizedSearchCV(estimator = pipe1,\n",
    "                              param_distributions = pipe1_params, \n",
    "                              n_iter = 15, \n",
    "                              scoring = 'accuracy', \n",
    "                             refit=True, \n",
    "                             cv=10, \n",
    "                             verbose=1,\n",
    "                              n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "demographic-reply",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 15 candidates, totalling 150 fits\n",
      "[Pipeline] ... (step 1 of 2) Processing tfidfvectorizer, total=  17.8s\n",
      "[Pipeline]  (step 2 of 2) Processing logisticregression, total=   2.6s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10,\n",
       "                   estimator=Pipeline(steps=[('tfidfvectorizer',\n",
       "                                              TfidfVectorizer()),\n",
       "                                             ('logisticregression',\n",
       "                                              LogisticRegression())],\n",
       "                                      verbose=True),\n",
       "                   n_iter=15, n_jobs=-1,\n",
       "                   param_distributions=[{'logisticregression__C': [0.01, 0.1,\n",
       "                                                                   1.0, 10.0],\n",
       "                                         'logisticregression__max_iter': [1000,\n",
       "                                                                          2500,\n",
       "                                                                          5000],\n",
       "                                         'logisticregression__multi_class': ['ovr',\n",
       "                                                                             'multinomial'],\n",
       "                                         'tfidfvectorizer__lowercase': [True,\n",
       "                                                                        False],\n",
       "                                         'tfidfvectorizer__max_df': [0.7, 0.75,\n",
       "                                                                     0.8, 0.85,\n",
       "                                                                     0.9, 1.0],\n",
       "                                         'tfidfvectorizer__min_df': [0.001,\n",
       "                                                                     0.01,\n",
       "                                                                     0.1],\n",
       "                                         'tfidfvectorizer__ngram_range': [(1,\n",
       "                                                                           1),\n",
       "                                                                          (1,\n",
       "                                                                           2),\n",
       "                                                                          (1,\n",
       "                                                                           3),\n",
       "                                                                          (1,\n",
       "                                                                           4)],\n",
       "                                         'tfidfvectorizer__norm': ['l1', 'l2'],\n",
       "                                         'tfidfvectorizer__tokenizer': [<function word_tokenize at 0x7fd5714a2160>]}],\n",
       "                   scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs_pipe1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "functioning-louisiana",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_pipe1_best = rs_pipe1.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "sensitive-behavior",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tfidfvectorizer__tokenizer': <function word_tokenize at 0x7fd5714a2160>, 'tfidfvectorizer__norm': 'l2', 'tfidfvectorizer__ngram_range': (1, 1), 'tfidfvectorizer__min_df': 0.001, 'tfidfvectorizer__max_df': 0.8, 'tfidfvectorizer__lowercase': False, 'logisticregression__multi_class': 'ovr', 'logisticregression__max_iter': 2500, 'logisticregression__C': 1.0}\n"
     ]
    }
   ],
   "source": [
    "print(rs_pipe1.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "physical-glory",
   "metadata": {},
   "source": [
    "### 4.2. Pipe 2. Stochastic Gradient Descent Classifier and TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "administrative-forth",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGDClassifier() # default parameters\n",
    "tfidf = TfidfVectorizer() # default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "paperback-journalism",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe2 = make_pipeline(tfidf, sgd, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "august-humanity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ... (step 1 of 2) Processing tfidfvectorizer, total=   1.7s\n",
      "[Pipeline] ..... (step 2 of 2) Processing sgdclassifier, total=   0.6s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidfvectorizer', TfidfVectorizer()),\n",
       "                ('sgdclassifier', SGDClassifier())],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "robust-restaurant",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_pipe2 = pipe2.predict(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "impossible-groove",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7554545454545455\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_dev, y_pred_pipe2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "arctic-departure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ARA       0.76      0.63      0.69       100\n",
      "         DEU       0.82      0.89      0.86       100\n",
      "         FRA       0.79      0.81      0.80       100\n",
      "         HIN       0.65      0.59      0.62       100\n",
      "         ITA       0.84      0.87      0.86       100\n",
      "         JPN       0.74      0.74      0.74       100\n",
      "         KOR       0.67      0.68      0.68       100\n",
      "         SPA       0.74      0.66      0.70       100\n",
      "         TEL       0.71      0.82      0.76       100\n",
      "         TUR       0.75      0.80      0.78       100\n",
      "         ZHO       0.81      0.82      0.82       100\n",
      "\n",
      "    accuracy                           0.76      1100\n",
      "   macro avg       0.75      0.76      0.75      1100\n",
      "weighted avg       0.75      0.76      0.75      1100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_dev, y_pred_pipe2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contrary-arctic",
   "metadata": {},
   "source": [
    "#### 4.2.1. Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "treated-council",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe2_params = [{'tfidfvectorizer__lowercase' : [True, False], \n",
    "                 'tfidfvectorizer__max_df' : [0.70, 0.75, 0.80, 0.85, 0.90, 1.0],\n",
    "                 'tfidfvectorizer__min_df' : [0.001, 0.01, 0.1],\n",
    "                 'tfidfvectorizer__ngram_range' : [(1,1), (1,2), (1,3), (1,4)],\n",
    "                 'tfidfvectorizer__norm' : ['l1', 'l2'],\n",
    "                 'tfidfvectorizer__tokenizer' : [nltk__word_tokenizer],\n",
    "                 'sgdclassifier__loss' : ['hinge', 'log', 'squared_hinge'],\n",
    "                 'sgdclassifier__penalty' : ['l2', 'l1', 'elasticnet'],\n",
    "                 'sgdclassifier__alpha' : [1e-4, 1e-3, 0.01, 0.1, 1.0],\n",
    "                 'sgdclassifier__max_iter': [1000, 2500, 5000]}] \n",
    "\n",
    "rs_pipe2 = RandomizedSearchCV(estimator = pipe2,\n",
    "                              param_distributions = pipe2_params, \n",
    "                              n_iter = 15, \n",
    "                              scoring = 'accuracy', \n",
    "                              refit=True, \n",
    "                              cv=10, \n",
    "                              verbose=1, \n",
    "                              n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "aggregate-colonial",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 15 candidates, totalling 150 fits\n",
      "[Pipeline] ... (step 1 of 2) Processing tfidfvectorizer, total=  31.6s\n",
      "[Pipeline] ..... (step 2 of 2) Processing sgdclassifier, total=   1.5s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10,\n",
       "                   estimator=Pipeline(steps=[('tfidfvectorizer',\n",
       "                                              TfidfVectorizer()),\n",
       "                                             ('sgdclassifier',\n",
       "                                              SGDClassifier())],\n",
       "                                      verbose=True),\n",
       "                   n_iter=15, n_jobs=-1,\n",
       "                   param_distributions=[{'sgdclassifier__alpha': [0.0001, 0.001,\n",
       "                                                                  0.01, 0.1,\n",
       "                                                                  1.0],\n",
       "                                         'sgdclassifier__loss': ['hinge', 'log',\n",
       "                                                                 'squared_hinge'],\n",
       "                                         'sgdclassifier__max_iter': [1000, 2500,\n",
       "                                                                     5000],\n",
       "                                         'sgdclassifier__penalt...',\n",
       "                                                                    'elasticnet'],\n",
       "                                         'tfidfvectorizer__lowercase': [True,\n",
       "                                                                        False],\n",
       "                                         'tfidfvectorizer__max_df': [0.7, 0.75,\n",
       "                                                                     0.8, 0.85,\n",
       "                                                                     0.9, 1.0],\n",
       "                                         'tfidfvectorizer__min_df': [0.001,\n",
       "                                                                     0.01,\n",
       "                                                                     0.1],\n",
       "                                         'tfidfvectorizer__ngram_range': [(1,\n",
       "                                                                           1),\n",
       "                                                                          (1,\n",
       "                                                                           2),\n",
       "                                                                          (1,\n",
       "                                                                           3),\n",
       "                                                                          (1,\n",
       "                                                                           4)],\n",
       "                                         'tfidfvectorizer__norm': ['l1', 'l2'],\n",
       "                                         'tfidfvectorizer__tokenizer': [<function word_tokenize at 0x7fd5714a2160>]}],\n",
       "                   scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs_pipe2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "sublime-circulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_pipe2_best = rs_pipe2.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "acute-narrow",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tfidfvectorizer__tokenizer': <function word_tokenize at 0x7fd5714a2160>, 'tfidfvectorizer__norm': 'l2', 'tfidfvectorizer__ngram_range': (1, 4), 'tfidfvectorizer__min_df': 0.1, 'tfidfvectorizer__max_df': 1.0, 'tfidfvectorizer__lowercase': True, 'sgdclassifier__penalty': 'l1', 'sgdclassifier__max_iter': 1000, 'sgdclassifier__loss': 'hinge', 'sgdclassifier__alpha': 0.0001}\n"
     ]
    }
   ],
   "source": [
    "print(rs_pipe2.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "public-brass",
   "metadata": {},
   "source": [
    "### 4.3. Pipe 3. LinearSVC and TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "prepared-music",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_svc = LinearSVC() # default parameters\n",
    "tfidf = TfidfVectorizer() # default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "radical-mongolia",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe3 = make_pipeline(tfidf, linear_svc, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "numerical-hostel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ... (step 1 of 2) Processing tfidfvectorizer, total=   1.7s\n",
      "[Pipeline] ......... (step 2 of 2) Processing linearsvc, total=   1.2s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidfvectorizer', TfidfVectorizer()),\n",
       "                ('linearsvc', LinearSVC())],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe3.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "artistic-lloyd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_pipe3 = pipe3.predict(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "casual-universe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7636363636363637\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_dev, y_pred_pipe3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "according-processing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ARA       0.72      0.64      0.68       100\n",
      "         DEU       0.87      0.87      0.87       100\n",
      "         FRA       0.79      0.84      0.81       100\n",
      "         HIN       0.71      0.68      0.69       100\n",
      "         ITA       0.88      0.86      0.87       100\n",
      "         JPN       0.72      0.74      0.73       100\n",
      "         KOR       0.69      0.69      0.69       100\n",
      "         SPA       0.77      0.71      0.74       100\n",
      "         TEL       0.73      0.80      0.76       100\n",
      "         TUR       0.75      0.78      0.76       100\n",
      "         ZHO       0.78      0.79      0.79       100\n",
      "\n",
      "    accuracy                           0.76      1100\n",
      "   macro avg       0.76      0.76      0.76      1100\n",
      "weighted avg       0.76      0.76      0.76      1100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_dev, y_pred_pipe3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reported-digit",
   "metadata": {},
   "source": [
    "#### 4.3.1. Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "organic-california",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_pipe3_params = [{'tfidfvectorizer__lowercase' : [True, False], \n",
    "                     'tfidfvectorizer__max_df' : [0.70, 0.75, 0.80, 0.85, 0.90, 1.0],\n",
    "                     'tfidfvectorizer__min_df' : [0.001, 0.01, 0.1],\n",
    "                     'tfidfvectorizer__ngram_range' : [(1,1), (1,2), (1,3), (1,4)],\n",
    "                     'tfidfvectorizer__norm' : ['l1', 'l2'],\n",
    "                     'tfidfvectorizer__tokenizer' : [nltk__word_tokenizer],\n",
    "                     'linearsvc__C' : [0.1, 1.0, 10.0],\n",
    "                     'linearsvc__max_iter' : [1000, 2500, 5000]}]\n",
    "\n",
    "rs_pipe3 = RandomizedSearchCV(estimator = pipe3, \n",
    "                              param_distributions=rs_pipe3_params, \n",
    "                              n_iter = 15, \n",
    "                              scoring='accuracy', \n",
    "                              refit=True, \n",
    "                              cv=10, \n",
    "                              verbose=2,\n",
    "                              random_state=1,\n",
    "                              n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "canadian-equity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 15 candidates, totalling 150 fits\n",
      "[Pipeline] ... (step 1 of 2) Processing tfidfvectorizer, total=  27.3s\n",
      "[Pipeline] ......... (step 2 of 2) Processing linearsvc, total=   3.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10,\n",
       "                   estimator=Pipeline(steps=[('tfidfvectorizer',\n",
       "                                              TfidfVectorizer()),\n",
       "                                             ('linearsvc', LinearSVC())],\n",
       "                                      verbose=True),\n",
       "                   n_iter=15, n_jobs=-1,\n",
       "                   param_distributions=[{'linearsvc__C': [0.1, 1.0, 10.0],\n",
       "                                         'linearsvc__max_iter': [1000, 2500,\n",
       "                                                                 5000],\n",
       "                                         'tfidfvectorizer__lowercase': [True,\n",
       "                                                                        False],\n",
       "                                         'tfidfvectorizer__max_df': [0.7, 0.75,\n",
       "                                                                     0.8, 0.85,\n",
       "                                                                     0.9, 1.0],\n",
       "                                         'tfidfvectorizer__min_df': [0.001,\n",
       "                                                                     0.01,\n",
       "                                                                     0.1],\n",
       "                                         'tfidfvectorizer__ngram_range': [(1,\n",
       "                                                                           1),\n",
       "                                                                          (1,\n",
       "                                                                           2),\n",
       "                                                                          (1,\n",
       "                                                                           3),\n",
       "                                                                          (1,\n",
       "                                                                           4)],\n",
       "                                         'tfidfvectorizer__norm': ['l1', 'l2'],\n",
       "                                         'tfidfvectorizer__tokenizer': [<function word_tokenize at 0x7fd5714a2160>]}],\n",
       "                   random_state=1, scoring='accuracy', verbose=2)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs_pipe3.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "committed-container",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_pipe3_best = rs_pipe3.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "frank-opinion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('tfidfvectorizer',\n",
      "                 TfidfVectorizer(lowercase=False, max_df=0.8, min_df=0.001,\n",
      "                                 ngram_range=(1, 3),\n",
      "                                 tokenizer=<function word_tokenize at 0x7fd5714a2160>)),\n",
      "                ('linearsvc', LinearSVC())],\n",
      "         verbose=True)\n"
     ]
    }
   ],
   "source": [
    "print(rs_pipe3.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cheap-movie",
   "metadata": {},
   "source": [
    "## 5. Stacked Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atomic-jurisdiction",
   "metadata": {},
   "source": [
    "### 5.1. Stacked 1. Logistic Regression, Stochastic Gradient Descent, LinearSVC, TfidfVectorizer. \n",
    "<u>Input feature:</u> *text* column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "eastern-auction",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "private-expansion",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(rs_pipe1.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "accomplished-instrumentation",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(rs_pipe2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "discrete-security",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(rs_pipe3.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "vocal-stamp",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_log_reg = TfidfVectorizer(tokenizer = nltk__word_tokenizer,\n",
    "                               norm=\"l2\", \n",
    "                               ngram_range=(1,1),\n",
    "                               min_df = 0.001, \n",
    "                               max_df = 0.8, \n",
    "                               lowercase=False)\n",
    "log_reg = LogisticRegression(multi_class=\"ovr\", \n",
    "                             max_iter=2500, \n",
    "                             C = 1.0,\n",
    "                             random_state=1)\n",
    "#####\n",
    "\n",
    "tfidf_sgd = TfidfVectorizer(tokenizer = nltk__word_tokenizer, \n",
    "                            norm=\"l2\", \n",
    "                            ngram_range=(1,4), \n",
    "                            min_df = 0.1, \n",
    "                            max_df = 1.0, \n",
    "                            lowercase=True)\n",
    "sgd = SGDClassifier(penalty = \"l1\", \n",
    "                    max_iter = 1000, \n",
    "                    loss = \"hinge\", \n",
    "                    random_state=1)\n",
    "\n",
    "#####\n",
    "\n",
    "tfidf_linear_svc = TfidfVectorizer(tokenizer = nltk__word_tokenizer,\n",
    "                                  norm = \"l2\", \n",
    "                                  ngram_range=(1,3), \n",
    "                                  min_df = 0.001, \n",
    "                                  max_df = 0.8,\n",
    "                                  lowercase=False)\n",
    "linear_svc = LinearSVC(max_iter = 1000, \n",
    "                       C = 1.0,\n",
    "                       random_state=1)\n",
    "\n",
    "#####\n",
    "\n",
    "final_logreg = LogisticRegression(multi_class=\"multinomial\",\n",
    "                                 max_iter=1000)\n",
    "\n",
    "stacked1 = StackingClassifier(estimators=[('pipe1', make_pipeline(tfidf_log_reg, log_reg, verbose=True)),\n",
    "                                          ('pipe2', make_pipeline(tfidf_sgd, sgd, verbose=True)),\n",
    "                                          ('pipe3', make_pipeline(tfidf_linear_svc, linear_svc, verbose=True))],\n",
    "                              final_estimator=final_logreg,\n",
    "                              verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "weighted-multimedia",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ... (step 1 of 2) Processing tfidfvectorizer, total=  18.0s\n",
      "[Pipeline]  (step 2 of 2) Processing logisticregression, total=   2.5s\n",
      "[Pipeline] ... (step 1 of 2) Processing tfidfvectorizer, total=  28.9s\n",
      "[Pipeline] ..... (step 2 of 2) Processing sgdclassifier, total=   1.4s\n",
      "[Pipeline] ... (step 1 of 2) Processing tfidfvectorizer, total=  23.8s\n",
      "[Pipeline] ......... (step 2 of 2) Processing linearsvc, total=   3.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ... (step 1 of 2) Processing tfidfvectorizer, total=  13.1s\n",
      "[Pipeline]  (step 2 of 2) Processing logisticregression, total=   2.1s\n",
      "[Pipeline] ... (step 1 of 2) Processing tfidfvectorizer, total=  12.8s\n",
      "[Pipeline]  (step 2 of 2) Processing logisticregression, total=   2.0s\n",
      "[Pipeline] ... (step 1 of 2) Processing tfidfvectorizer, total=  12.7s\n",
      "[Pipeline]  (step 2 of 2) Processing logisticregression, total=   4.2s\n",
      "[Pipeline] ... (step 1 of 2) Processing tfidfvectorizer, total=  13.0s\n",
      "[Pipeline]  (step 2 of 2) Processing logisticregression, total=   2.0s\n",
      "[Pipeline] ... (step 1 of 2) Processing tfidfvectorizer, total=  12.8s\n",
      "[Pipeline]  (step 2 of 2) Processing logisticregression, total=   2.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  1.5min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ... (step 1 of 2) Processing tfidfvectorizer, total=  23.9s\n",
      "[Pipeline] ..... (step 2 of 2) Processing sgdclassifier, total=   1.2s\n",
      "[Pipeline] ... (step 1 of 2) Processing tfidfvectorizer, total=  22.9s\n",
      "[Pipeline] ..... (step 2 of 2) Processing sgdclassifier, total=   1.3s\n",
      "[Pipeline] ... (step 1 of 2) Processing tfidfvectorizer, total=  22.9s\n",
      "[Pipeline] ..... (step 2 of 2) Processing sgdclassifier, total=   1.3s\n",
      "[Pipeline] ... (step 1 of 2) Processing tfidfvectorizer, total=  23.0s\n",
      "[Pipeline] ..... (step 2 of 2) Processing sgdclassifier, total=   1.2s\n",
      "[Pipeline] ... (step 1 of 2) Processing tfidfvectorizer, total=  23.2s\n",
      "[Pipeline] ..... (step 2 of 2) Processing sgdclassifier, total=   1.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  2.4min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ... (step 1 of 2) Processing tfidfvectorizer, total=  19.1s\n",
      "[Pipeline] ......... (step 2 of 2) Processing linearsvc, total=   2.2s\n",
      "[Pipeline] ... (step 1 of 2) Processing tfidfvectorizer, total=  19.0s\n",
      "[Pipeline] ......... (step 2 of 2) Processing linearsvc, total=   2.2s\n",
      "[Pipeline] ... (step 1 of 2) Processing tfidfvectorizer, total=  19.2s\n",
      "[Pipeline] ......... (step 2 of 2) Processing linearsvc, total=   2.2s\n",
      "[Pipeline] ... (step 1 of 2) Processing tfidfvectorizer, total=  18.4s\n",
      "[Pipeline] ......... (step 2 of 2) Processing linearsvc, total=   2.2s\n",
      "[Pipeline] ... (step 1 of 2) Processing tfidfvectorizer, total=  19.5s\n",
      "[Pipeline] ......... (step 2 of 2) Processing linearsvc, total=   2.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  2.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StackingClassifier(estimators=[('pipe1',\n",
       "                                Pipeline(steps=[('tfidfvectorizer',\n",
       "                                                 TfidfVectorizer(lowercase=False,\n",
       "                                                                 max_df=0.8,\n",
       "                                                                 min_df=0.001,\n",
       "                                                                 tokenizer=<function word_tokenize at 0x7fd5714a2160>)),\n",
       "                                                ('logisticregression',\n",
       "                                                 LogisticRegression(max_iter=2500,\n",
       "                                                                    multi_class='ovr',\n",
       "                                                                    random_state=1))],\n",
       "                                         verbose=True)),\n",
       "                               ('pipe2',\n",
       "                                Pipeline(steps=[('tfidfvectorizer',\n",
       "                                                 TfidfVectorizer(m...\n",
       "                                                 SGDClassifier(penalty='l1',\n",
       "                                                               random_state=1))],\n",
       "                                         verbose=True)),\n",
       "                               ('pipe3',\n",
       "                                Pipeline(steps=[('tfidfvectorizer',\n",
       "                                                 TfidfVectorizer(lowercase=False,\n",
       "                                                                 max_df=0.8,\n",
       "                                                                 min_df=0.001,\n",
       "                                                                 ngram_range=(1,\n",
       "                                                                              3),\n",
       "                                                                 tokenizer=<function word_tokenize at 0x7fd5714a2160>)),\n",
       "                                                ('linearsvc',\n",
       "                                                 LinearSVC(random_state=1))],\n",
       "                                         verbose=True))],\n",
       "                   final_estimator=LogisticRegression(max_iter=1000,\n",
       "                                                      multi_class='multinomial'),\n",
       "                   verbose=1)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "polar-dictionary",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_stacked1 = stacked1.predict(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "specified-energy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8290909090909091\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_dev, y_pred_stacked1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "charged-halloween",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ARA       0.80      0.77      0.79       100\n",
      "         DEU       0.95      0.96      0.96       100\n",
      "         FRA       0.82      0.86      0.84       100\n",
      "         HIN       0.76      0.78      0.77       100\n",
      "         ITA       0.89      0.88      0.88       100\n",
      "         JPN       0.77      0.81      0.79       100\n",
      "         KOR       0.82      0.72      0.77       100\n",
      "         SPA       0.83      0.81      0.82       100\n",
      "         TEL       0.82      0.78      0.80       100\n",
      "         TUR       0.84      0.86      0.85       100\n",
      "         ZHO       0.82      0.89      0.86       100\n",
      "\n",
      "    accuracy                           0.83      1100\n",
      "   macro avg       0.83      0.83      0.83      1100\n",
      "weighted avg       0.83      0.83      0.83      1100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_dev, y_pred_stacked1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "referenced-darkness",
   "metadata": {},
   "source": [
    "### 5.2. Stacked 2. Logistic Regression, Stochastic Gradient Descent Classifier, LinearSVC, TfidfVectorizer <u>(BEST PERFORMING CLASSIFIER)</u>\n",
    "<u>Input feature:</u> lemmatized text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "gentle-contact",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lemma = df.lemma_text.values\n",
    "y_lemma = df.Language.values\n",
    "\n",
    "X_train_lemma, X_dev_lemma, y_train_lemma, y_dev_lemma = train_test_split(X_lemma,\n",
    "                                                  y_lemma, \n",
    "                                                  test_size=0.1,\n",
    "                                                  random_state=1,\n",
    "                                                  shuffle=True,\n",
    "                                                  stratify=y_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "lucky-intensity",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_log_reg = TfidfVectorizer(tokenizer = nltk__word_tokenizer,\n",
    "                               norm=\"l2\", \n",
    "                               ngram_range=(1,1),\n",
    "                               min_df = 0.001, \n",
    "                               max_df = 0.8, \n",
    "                               lowercase=False)\n",
    "log_reg = LogisticRegression(multi_class=\"ovr\", \n",
    "                             max_iter=2500, \n",
    "                             C = 1.0,\n",
    "                             random_state=1)\n",
    "#####\n",
    "\n",
    "tfidf_sgd = TfidfVectorizer(tokenizer = nltk__word_tokenizer, \n",
    "                            norm=\"l2\", \n",
    "                            ngram_range=(1,4), \n",
    "                            min_df = 0.1, \n",
    "                            max_df = 1.0, \n",
    "                            lowercase=True)\n",
    "sgd = SGDClassifier(penalty = \"l1\", \n",
    "                    max_iter = 1000, \n",
    "                    loss = \"hinge\", \n",
    "                    alpha = 0.0001,\n",
    "                    random_state=1)\n",
    "\n",
    "#####\n",
    "\n",
    "tfidf_linear_svc = TfidfVectorizer(tokenizer = nltk__word_tokenizer,\n",
    "                                  norm = \"l2\", \n",
    "                                  ngram_range=(1,3), \n",
    "                                  min_df = 0.001, \n",
    "                                  max_df = 0.8,\n",
    "                                  lowercase=False)\n",
    "linear_svc = LinearSVC(max_iter = 1000, \n",
    "                       C = 1.0,\n",
    "                       random_state=1)\n",
    "\n",
    "#####\n",
    "\n",
    "final_logreg = LogisticRegression(multi_class=\"multinomial\",\n",
    "                                 max_iter=1000)\n",
    "\n",
    "stacked2 = StackingClassifier(estimators=[('pipe1', make_pipeline(tfidf_log_reg, log_reg, verbose=True)),\n",
    "                                          ('pipe2', make_pipeline(tfidf_sgd, sgd, verbose=True)),\n",
    "                                          ('pipe3', make_pipeline(tfidf_linear_svc, linear_svc, verbose=True))],\n",
    "                              final_estimator=final_logreg,\n",
    "                              verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "sitting-sharing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ... (step 1 of 2) Processing tfidfvectorizer, total=  18.8s\n",
      "[Pipeline]  (step 2 of 2) Processing logisticregression, total=   2.4s\n",
      "[Pipeline] ... (step 1 of 2) Processing tfidfvectorizer, total=  33.4s\n",
      "[Pipeline] ..... (step 2 of 2) Processing sgdclassifier, total=   1.8s\n",
      "[Pipeline] ... (step 1 of 2) Processing tfidfvectorizer, total=  27.0s\n",
      "[Pipeline] ......... (step 2 of 2) Processing linearsvc, total=   2.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ... (step 1 of 2) Processing tfidfvectorizer, total=  14.3s\n",
      "[Pipeline]  (step 2 of 2) Processing logisticregression, total=   1.8s\n",
      "[Pipeline] ... (step 1 of 2) Processing tfidfvectorizer, total=  14.7s\n",
      "[Pipeline]  (step 2 of 2) Processing logisticregression, total=   2.0s\n",
      "[Pipeline] ... (step 1 of 2) Processing tfidfvectorizer, total=  14.6s\n",
      "[Pipeline]  (step 2 of 2) Processing logisticregression, total=   1.9s\n",
      "[Pipeline] ... (step 1 of 2) Processing tfidfvectorizer, total=  14.3s\n",
      "[Pipeline]  (step 2 of 2) Processing logisticregression, total=   2.3s\n",
      "[Pipeline] ... (step 1 of 2) Processing tfidfvectorizer, total=  15.1s\n",
      "[Pipeline]  (step 2 of 2) Processing logisticregression, total=   1.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  1.7min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ... (step 1 of 2) Processing tfidfvectorizer, total=  27.2s\n",
      "[Pipeline] ..... (step 2 of 2) Processing sgdclassifier, total=   1.6s\n",
      "[Pipeline] ... (step 1 of 2) Processing tfidfvectorizer, total=  26.9s\n",
      "[Pipeline] ..... (step 2 of 2) Processing sgdclassifier, total=   1.6s\n",
      "[Pipeline] ... (step 1 of 2) Processing tfidfvectorizer, total=  26.5s\n",
      "[Pipeline] ..... (step 2 of 2) Processing sgdclassifier, total=   1.6s\n",
      "[Pipeline] ... (step 1 of 2) Processing tfidfvectorizer, total=  26.4s\n",
      "[Pipeline] ..... (step 2 of 2) Processing sgdclassifier, total=   1.3s\n",
      "[Pipeline] ... (step 1 of 2) Processing tfidfvectorizer, total=  26.2s\n",
      "[Pipeline] ..... (step 2 of 2) Processing sgdclassifier, total=   1.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  2.8min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ... (step 1 of 2) Processing tfidfvectorizer, total=  21.5s\n",
      "[Pipeline] ......... (step 2 of 2) Processing linearsvc, total=   2.3s\n",
      "[Pipeline] ... (step 1 of 2) Processing tfidfvectorizer, total=  20.9s\n",
      "[Pipeline] ......... (step 2 of 2) Processing linearsvc, total=   2.4s\n",
      "[Pipeline] ... (step 1 of 2) Processing tfidfvectorizer, total=  21.3s\n",
      "[Pipeline] ......... (step 2 of 2) Processing linearsvc, total=   2.2s\n",
      "[Pipeline] ... (step 1 of 2) Processing tfidfvectorizer, total=  20.7s\n",
      "[Pipeline] ......... (step 2 of 2) Processing linearsvc, total=   2.2s\n",
      "[Pipeline] ... (step 1 of 2) Processing tfidfvectorizer, total=  21.2s\n",
      "[Pipeline] ......... (step 2 of 2) Processing linearsvc, total=   2.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  2.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StackingClassifier(estimators=[('pipe1',\n",
       "                                Pipeline(steps=[('tfidfvectorizer',\n",
       "                                                 TfidfVectorizer(lowercase=False,\n",
       "                                                                 max_df=0.8,\n",
       "                                                                 min_df=0.001,\n",
       "                                                                 tokenizer=<function word_tokenize at 0x7fae3987cf70>)),\n",
       "                                                ('logisticregression',\n",
       "                                                 LogisticRegression(max_iter=2500,\n",
       "                                                                    multi_class='ovr',\n",
       "                                                                    random_state=1))],\n",
       "                                         verbose=True)),\n",
       "                               ('pipe2',\n",
       "                                Pipeline(steps=[('tfidfvectorizer',\n",
       "                                                 TfidfVectorizer(m...\n",
       "                                                 SGDClassifier(penalty='l1',\n",
       "                                                               random_state=1))],\n",
       "                                         verbose=True)),\n",
       "                               ('pipe3',\n",
       "                                Pipeline(steps=[('tfidfvectorizer',\n",
       "                                                 TfidfVectorizer(lowercase=False,\n",
       "                                                                 max_df=0.8,\n",
       "                                                                 min_df=0.001,\n",
       "                                                                 ngram_range=(1,\n",
       "                                                                              3),\n",
       "                                                                 tokenizer=<function word_tokenize at 0x7fae3987cf70>)),\n",
       "                                                ('linearsvc',\n",
       "                                                 LinearSVC(random_state=1))],\n",
       "                                         verbose=True))],\n",
       "                   final_estimator=LogisticRegression(max_iter=1000,\n",
       "                                                      multi_class='multinomial'),\n",
       "                   verbose=1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked2.fit(X_train_lemma, y_train_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "internal-western",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_stacked2 = stacked2.predict(X_dev_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dangerous-blowing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8336363636363636\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_dev_lemma, y_pred_stacked2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "better-equilibrium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ARA       0.82      0.79      0.81       100\n",
      "         DEU       0.90      0.93      0.92       100\n",
      "         FRA       0.81      0.87      0.84       100\n",
      "         HIN       0.80      0.77      0.79       100\n",
      "         ITA       0.89      0.90      0.90       100\n",
      "         JPN       0.83      0.80      0.82       100\n",
      "         KOR       0.80      0.77      0.79       100\n",
      "         SPA       0.84      0.77      0.80       100\n",
      "         TEL       0.82      0.83      0.83       100\n",
      "         TUR       0.84      0.86      0.85       100\n",
      "         ZHO       0.80      0.88      0.84       100\n",
      "\n",
      "    accuracy                           0.83      1100\n",
      "   macro avg       0.83      0.83      0.83      1100\n",
      "weighted avg       0.83      0.83      0.83      1100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_dev_lemma, y_pred_stacked2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "square-possibility",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "confusion_matrix = pd.crosstab(y_dev_lemma, y_pred_stacked2, rownames=['Actual'], colnames=['Predicted']) \n",
    "sn.heatmap(confusion_matrix, annot=True, cmap='Blues', fmt='g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "choice-competition",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Combining features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "matched-server",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train_preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "rational-morning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a development dataframe\n",
    "dev = df.groupby(\"Language\", group_keys=False).sample(n = 100, random_state=1)\n",
    "train = df.drop(dev.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "cross-calculator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9900, 10) (1100, 10)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape, dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "contained-tampa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ZHO    900\n",
       "ITA    900\n",
       "DEU    900\n",
       "TUR    900\n",
       "TEL    900\n",
       "SPA    900\n",
       "FRA    900\n",
       "ARA    900\n",
       "HIN    900\n",
       "KOR    900\n",
       "JPN    900\n",
       "Name: Language, dtype: int64"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.Language.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "breathing-cambridge",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ZHO    100\n",
       "KOR    100\n",
       "TUR    100\n",
       "SPA    100\n",
       "JPN    100\n",
       "ITA    100\n",
       "FRA    100\n",
       "DEU    100\n",
       "TEL    100\n",
       "ARA    100\n",
       "HIN    100\n",
       "Name: Language, dtype: int64"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev.Language.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrapped-lying",
   "metadata": {},
   "source": [
    "### 6.1. Text, text length in number of words, and character *n*-grams.\n",
    "- <u>Classifier:</u> Logistic Regression\n",
    "- <u>Input feature:</u> Text, text length in number of words, and character *n*-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "concrete-neighbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[[\"text\", \"n_words\"]]\n",
    "y_train = train[\"Language\"]\n",
    "\n",
    "X_dev = dev[[\"text\", \"n_words\"]]\n",
    "y_dev = dev[\"Language\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "studied-event",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_text = TfidfVectorizer(ngram_range=(1,2),  # article Malmasi et al. (2018)\n",
    "                               min_df = 0.001, \n",
    "                               max_df = 0.8, \n",
    "                               lowercase=False)\n",
    "\n",
    "tfidf_char = TfidfVectorizer(analyzer=\"char_wb\",\n",
    "                            ngram_range=(1,3))\n",
    "\n",
    "sc = StandardScaler(with_mean=False) # for numeric feature, with_mean=False to work with sparse vector\n",
    "\n",
    "preprocessor1 = make_column_transformer(\n",
    "    (sc, [\"n_words\"]),\n",
    "    (tfidf_text, \"text\"),\n",
    "    (tfidf_char, \"text\"),\n",
    "remainder=\"passthrough\")\n",
    "\n",
    "\n",
    "log_reg = LogisticRegression(max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "molecular-hindu",
   "metadata": {},
   "outputs": [],
   "source": [
    "combo1 = make_pipeline(preprocessor1, log_reg, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "lesser-latest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] . (step 1 of 2) Processing columntransformer, total=  24.0s\n",
      "[Pipeline]  (step 2 of 2) Processing logisticregression, total= 2.1min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('columntransformer',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('standardscaler',\n",
       "                                                  StandardScaler(with_mean=False),\n",
       "                                                  ['n_words']),\n",
       "                                                 ('tfidfvectorizer-1',\n",
       "                                                  TfidfVectorizer(lowercase=False,\n",
       "                                                                  max_df=0.8,\n",
       "                                                                  min_df=0.001,\n",
       "                                                                  ngram_range=(1,\n",
       "                                                                               2)),\n",
       "                                                  'text'),\n",
       "                                                 ('tfidfvectorizer-2',\n",
       "                                                  TfidfVectorizer(analyzer='char_wb',\n",
       "                                                                  ngram_range=(1,\n",
       "                                                                               3)),\n",
       "                                                  'text')])),\n",
       "                ('logisticregression', LogisticRegression(max_iter=1000))],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combo1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "plastic-fashion",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_combo1 = combo1.predict(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "statistical-folder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7327272727272728\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_dev, y_pred_combo1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "addressed-circulation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ARA       0.71      0.72      0.72       100\n",
      "         DEU       0.74      0.93      0.83       100\n",
      "         FRA       0.78      0.77      0.77       100\n",
      "         HIN       0.63      0.67      0.65       100\n",
      "         ITA       0.84      0.78      0.81       100\n",
      "         JPN       0.70      0.68      0.69       100\n",
      "         KOR       0.65      0.70      0.67       100\n",
      "         SPA       0.74      0.62      0.67       100\n",
      "         TEL       0.69      0.76      0.72       100\n",
      "         TUR       0.83      0.74      0.78       100\n",
      "         ZHO       0.79      0.69      0.74       100\n",
      "\n",
      "    accuracy                           0.73      1100\n",
      "   macro avg       0.74      0.73      0.73      1100\n",
      "weighted avg       0.74      0.73      0.73      1100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_dev, y_pred_combo1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "future-proceeding",
   "metadata": {},
   "source": [
    "### 6.2. Lemmatized text, function words and character *n*-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "annual-administrator",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[[\"lemma_text\", \"text\", \"stop_words\"]]\n",
    "y_train = train[\"Language\"]\n",
    "\n",
    "X_dev = dev[[\"lemma_text\", \"text\", \"stop_words\"]]\n",
    "y_dev = dev[\"Language\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "perceived-connection",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_lemma = TfidfVectorizer(ngram_range=(1,2),# article Malmasi et al. (2018)\n",
    "                              min_df=0.001,\n",
    "                              max_df = 0.8,\n",
    "                               lowercase=False)\n",
    "\n",
    "tfidf_char = TfidfVectorizer(analyzer=\"char_wb\",\n",
    "                            ngram_range=(1,3)) # article Malmasi et al. (2018), character uni-, bi- and trigrams\n",
    "\n",
    "tfidf_stop_words = TfidfVectorizer(ngram_range=(1,2)) # article Malmasi et al. (2018), function word uni- and bigrams.\n",
    "\n",
    "preprocessor2 = make_column_transformer(\n",
    "    (tfidf_lemma, \"lemma_text\"),\n",
    "    (tfidf_char, \"text\"),\n",
    "    (tfidf_stop_words, \"stop_words\")\n",
    ")\n",
    "\n",
    "logreg = LogisticRegression(max_iter=1000, verbose=1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "adverse-master",
   "metadata": {},
   "outputs": [],
   "source": [
    "combo2 = make_pipeline(preprocessor2, logreg, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "decimal-vulnerability",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] . (step 1 of 2) Processing columntransformer, total=  27.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline]  (step 2 of 2) Processing logisticregression, total= 1.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('columntransformer',\n",
       "                 ColumnTransformer(transformers=[('tfidfvectorizer-1',\n",
       "                                                  TfidfVectorizer(lowercase=False,\n",
       "                                                                  max_df=0.8,\n",
       "                                                                  min_df=0.001,\n",
       "                                                                  ngram_range=(1,\n",
       "                                                                               2)),\n",
       "                                                  'lemma_text'),\n",
       "                                                 ('tfidfvectorizer-2',\n",
       "                                                  TfidfVectorizer(analyzer='char_wb',\n",
       "                                                                  ngram_range=(1,\n",
       "                                                                               3)),\n",
       "                                                  'text'),\n",
       "                                                 ('tfidfvectorizer-3',\n",
       "                                                  TfidfVectorizer(ngram_range=(1,\n",
       "                                                                               2)),\n",
       "                                                  'stop_words')])),\n",
       "                ('logisticregression',\n",
       "                 LogisticRegression(max_iter=1000, random_state=1, verbose=1))],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combo2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "assumed-wayne",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_combo2 = combo2.predict(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "accepted-number",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7045454545454546\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_dev, y_pred_combo2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "mechanical-anxiety",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ARA       0.64      0.66      0.65       100\n",
      "         DEU       0.75      0.89      0.82       100\n",
      "         FRA       0.71      0.72      0.71       100\n",
      "         HIN       0.66      0.73      0.70       100\n",
      "         ITA       0.78      0.76      0.77       100\n",
      "         JPN       0.71      0.66      0.68       100\n",
      "         KOR       0.61      0.67      0.64       100\n",
      "         SPA       0.75      0.63      0.68       100\n",
      "         TEL       0.72      0.76      0.74       100\n",
      "         TUR       0.77      0.61      0.68       100\n",
      "         ZHO       0.66      0.66      0.66       100\n",
      "\n",
      "    accuracy                           0.70      1100\n",
      "   macro avg       0.71      0.70      0.70      1100\n",
      "weighted avg       0.71      0.70      0.70      1100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_dev, y_pred_combo2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inside-fortune",
   "metadata": {},
   "source": [
    "### 6.3. Lemmatized text, text length in number of words, and character *n*-grams\n",
    "- <u>Classifier:</u> best stacked classifier (obtaining 83%), with the best parameters for each of the classifiers (obtained during RandomizedSearchCV)\n",
    "    - Logistic Regression\n",
    "    - SGDClassifier\n",
    "    - LinearSVC\n",
    "- <u>Input features:</u> lemmatized text, text length in number of words, and character *n*-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eight-dragon",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[[\"text\", \"n_words\", \"lemma_text\"]]\n",
    "y_train = train[\"Language\"]\n",
    "\n",
    "X_dev = dev[[\"text\", \"n_words\", \"lemma_text\"]]\n",
    "y_dev = dev[\"Language\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "jewish-concern",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_text = TfidfVectorizer(ngram_range=(1,2),  \n",
    "                               min_df = 0.001, \n",
    "                               max_df = 0.8, \n",
    "                               lowercase=False)\n",
    "\n",
    "tfidf_lemma = TfidfVectorizer(ngram_range=(1,2),  \n",
    "                              min_df = 0.001,\n",
    "                              max_df = 0.8,\n",
    "                              lowercase=False)\n",
    "\n",
    "tfidf_char = TfidfVectorizer(analyzer=\"char_wb\",\n",
    "                            ngram_range=(1,3))\n",
    "\n",
    "sc = StandardScaler(with_mean=False) # for numeric feature, with_mean=False to work with sparse vector\n",
    "\n",
    "preprocessor = make_column_transformer(\n",
    "    (sc, [\"n_words\"]),\n",
    "    (tfidf_text, \"text\"),\n",
    "    (tfidf_char, \"text\"),\n",
    "    (tfidf_lemma, \"lemma_text\"),\n",
    "remainder=\"passthrough\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "periodic-translator",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(multi_class=\"ovr\", \n",
    "                             max_iter=2500, \n",
    "                             C = 1.0,\n",
    "                             random_state=1, verbose=1)\n",
    "\n",
    "linear_svc = LinearSVC(max_iter = 1000, \n",
    "                       C = 1.0,\n",
    "                       random_state=1, verbose=1)\n",
    "\n",
    "sgd = SGDClassifier(penalty = \"l1\", \n",
    "                    max_iter = 1000, \n",
    "                    loss = \"hinge\", \n",
    "                    alpha = 0.0001,\n",
    "                    random_state=1, verbose=1)\n",
    "\n",
    "\n",
    "final_logreg = LogisticRegression(multi_class=\"multinomial\",\n",
    "                                 max_iter=1000)\n",
    "\n",
    "stacked = StackingClassifier(estimators=[('log_reg', log_reg),\n",
    "                                          ('sgd', sgd),\n",
    "                                          ('linear_svc', linear_svc)],\n",
    "                              final_estimator=final_logreg,\n",
    "                              verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "appointed-wallace",
   "metadata": {},
   "outputs": [],
   "source": [
    "combo_stacked = make_pipeline(preprocessor, stacked, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "apart-expense",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] . (step 1 of 2) Processing columntransformer, total=  30.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  11 out of  11 | elapsed:  1.7min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 215.84, NNZs: 1197, Bias: -0.042733, T: 9900, Avg. loss: 2.881684\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 223.49, NNZs: 900, Bias: -0.044256, T: 19800, Avg. loss: 0.680845\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 225.65, NNZs: 781, Bias: -0.044462, T: 29700, Avg. loss: 0.461648\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 226.52, NNZs: 696, Bias: -0.028607, T: 39600, Avg. loss: 0.373388\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 226.85, NNZs: 651, Bias: -0.040382, T: 49500, Avg. loss: 0.322582\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 227.13, NNZs: 627, Bias: -0.036596, T: 59400, Avg. loss: 0.279266\n",
      "Total training time: 0.44 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 227.28, NNZs: 605, Bias: -0.042436, T: 69300, Avg. loss: 0.260378\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 227.39, NNZs: 570, Bias: -0.036749, T: 79200, Avg. loss: 0.247720\n",
      "Total training time: 0.57 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 227.46, NNZs: 552, Bias: -0.035461, T: 89100, Avg. loss: 0.230515\n",
      "Total training time: 0.63 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 227.54, NNZs: 541, Bias: -0.034663, T: 99000, Avg. loss: 0.220632\n",
      "Total training time: 0.71 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 227.54, NNZs: 527, Bias: -0.038416, T: 108900, Avg. loss: 0.218213\n",
      "Total training time: 0.78 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 227.56, NNZs: 515, Bias: -0.038231, T: 118800, Avg. loss: 0.208141\n",
      "Total training time: 0.84 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 227.57, NNZs: 502, Bias: -0.035872, T: 128700, Avg. loss: 0.202105\n",
      "Total training time: 0.91 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 227.59, NNZs: 498, Bias: -0.036589, T: 138600, Avg. loss: 0.199126\n",
      "Total training time: 0.98 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 227.60, NNZs: 493, Bias: -0.033180, T: 148500, Avg. loss: 0.195501\n",
      "Total training time: 1.05 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 227.60, NNZs: 483, Bias: -0.038265, T: 158400, Avg. loss: 0.190053\n",
      "Total training time: 1.11 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 227.62, NNZs: 481, Bias: -0.035151, T: 168300, Avg. loss: 0.184573\n",
      "Total training time: 1.18 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 227.63, NNZs: 477, Bias: -0.036176, T: 178200, Avg. loss: 0.179989\n",
      "Total training time: 1.24 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 227.64, NNZs: 470, Bias: -0.036635, T: 188100, Avg. loss: 0.179794\n",
      "Total training time: 1.31 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 227.65, NNZs: 464, Bias: -0.033531, T: 198000, Avg. loss: 0.178857\n",
      "Total training time: 1.38 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 227.65, NNZs: 461, Bias: -0.033544, T: 207900, Avg. loss: 0.176898\n",
      "Total training time: 1.45 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 227.67, NNZs: 455, Bias: -0.034928, T: 217800, Avg. loss: 0.174612\n",
      "Total training time: 1.51 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 227.68, NNZs: 451, Bias: -0.033561, T: 227700, Avg. loss: 0.172818\n",
      "Total training time: 1.58 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 227.68, NNZs: 451, Bias: -0.033130, T: 237600, Avg. loss: 0.169213\n",
      "Total training time: 1.65 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 227.70, NNZs: 450, Bias: -0.034342, T: 247500, Avg. loss: 0.168001\n",
      "Total training time: 1.72 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 227.71, NNZs: 444, Bias: -0.034324, T: 257400, Avg. loss: 0.166778\n",
      "Total training time: 1.78 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 227.71, NNZs: 441, Bias: -0.034669, T: 267300, Avg. loss: 0.164748\n",
      "Total training time: 1.85 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 227.72, NNZs: 441, Bias: -0.034623, T: 277200, Avg. loss: 0.165531\n",
      "Total training time: 1.92 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 227.73, NNZs: 441, Bias: -0.034610, T: 287100, Avg. loss: 0.162553\n",
      "Total training time: 1.99 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 227.74, NNZs: 441, Bias: -0.034942, T: 297000, Avg. loss: 0.164242\n",
      "Total training time: 2.06 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 227.74, NNZs: 436, Bias: -0.033913, T: 306900, Avg. loss: 0.160825\n",
      "Total training time: 2.12 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 227.75, NNZs: 433, Bias: -0.034197, T: 316800, Avg. loss: 0.160666\n",
      "Total training time: 2.19 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 227.77, NNZs: 427, Bias: -0.034181, T: 326700, Avg. loss: 0.158269\n",
      "Total training time: 2.26 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 227.78, NNZs: 426, Bias: -0.035372, T: 336600, Avg. loss: 0.160189\n",
      "Total training time: 2.33 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 227.79, NNZs: 425, Bias: -0.033930, T: 346500, Avg. loss: 0.157442\n",
      "Total training time: 2.39 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 227.80, NNZs: 423, Bias: -0.033924, T: 356400, Avg. loss: 0.155747\n",
      "Total training time: 2.46 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 227.81, NNZs: 422, Bias: -0.033347, T: 366300, Avg. loss: 0.156053\n",
      "Total training time: 2.53 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 227.82, NNZs: 421, Bias: -0.034415, T: 376200, Avg. loss: 0.155550\n",
      "Total training time: 2.59 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 227.83, NNZs: 421, Bias: -0.033902, T: 386100, Avg. loss: 0.154809\n",
      "Total training time: 2.66 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 227.84, NNZs: 421, Bias: -0.032129, T: 396000, Avg. loss: 0.154229\n",
      "Total training time: 2.73 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 227.85, NNZs: 419, Bias: -0.033098, T: 405900, Avg. loss: 0.152921\n",
      "Total training time: 2.80 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 227.86, NNZs: 419, Bias: -0.033585, T: 415800, Avg. loss: 0.153305\n",
      "Total training time: 2.86 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 227.87, NNZs: 418, Bias: -0.031932, T: 425700, Avg. loss: 0.151553\n",
      "Total training time: 2.93 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 227.88, NNZs: 415, Bias: -0.032628, T: 435600, Avg. loss: 0.151340\n",
      "Total training time: 3.00 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 227.89, NNZs: 414, Bias: -0.032845, T: 445500, Avg. loss: 0.152817\n",
      "Total training time: 3.06 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 227.90, NNZs: 413, Bias: -0.033276, T: 455400, Avg. loss: 0.149357\n",
      "Total training time: 3.13 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 227.91, NNZs: 412, Bias: -0.032405, T: 465300, Avg. loss: 0.150748\n",
      "Total training time: 3.20 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 227.91, NNZs: 410, Bias: -0.032202, T: 475200, Avg. loss: 0.149593\n",
      "Total training time: 3.27 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 227.93, NNZs: 408, Bias: -0.033850, T: 485100, Avg. loss: 0.149256\n",
      "Total training time: 3.34 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 227.94, NNZs: 407, Bias: -0.033037, T: 495000, Avg. loss: 0.149733\n",
      "Total training time: 3.40 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 227.94, NNZs: 405, Bias: -0.032049, T: 504900, Avg. loss: 0.149102\n",
      "Total training time: 3.47 seconds.\n",
      "Convergence after 51 epochs took 3.47 seconds\n",
      "-- Epoch 1\n",
      "Norm: 246.10, NNZs: 1543, Bias: -0.321033, T: 9900, Avg. loss: 3.473170\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 258.77, NNZs: 1202, Bias: -0.256503, T: 19800, Avg. loss: 0.645745\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 262.34, NNZs: 1037, Bias: -0.228322, T: 29700, Avg. loss: 0.351310\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 264.19, NNZs: 943, Bias: -0.218883, T: 39600, Avg. loss: 0.241112\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 265.24, NNZs: 862, Bias: -0.207515, T: 49500, Avg. loss: 0.199318\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 265.92, NNZs: 795, Bias: -0.204014, T: 59400, Avg. loss: 0.166296\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 266.49, NNZs: 749, Bias: -0.203935, T: 69300, Avg. loss: 0.155229\n",
      "Total training time: 0.52 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 266.90, NNZs: 713, Bias: -0.202537, T: 79200, Avg. loss: 0.155478\n",
      "Total training time: 0.59 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 267.26, NNZs: 687, Bias: -0.204432, T: 89100, Avg. loss: 0.138239\n",
      "Total training time: 0.65 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 267.55, NNZs: 670, Bias: -0.202928, T: 99000, Avg. loss: 0.139193\n",
      "Total training time: 0.72 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 267.78, NNZs: 646, Bias: -0.198060, T: 108900, Avg. loss: 0.136315\n",
      "Total training time: 0.80 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 267.98, NNZs: 631, Bias: -0.196465, T: 118800, Avg. loss: 0.130485\n",
      "Total training time: 0.88 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 268.17, NNZs: 620, Bias: -0.198911, T: 128700, Avg. loss: 0.126769\n",
      "Total training time: 0.95 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 268.31, NNZs: 612, Bias: -0.198233, T: 138600, Avg. loss: 0.129939\n",
      "Total training time: 1.03 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 268.44, NNZs: 605, Bias: -0.200987, T: 148500, Avg. loss: 0.128046\n",
      "Total training time: 1.10 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 268.57, NNZs: 600, Bias: -0.198987, T: 158400, Avg. loss: 0.127323\n",
      "Total training time: 1.18 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 268.69, NNZs: 593, Bias: -0.196587, T: 168300, Avg. loss: 0.123788\n",
      "Total training time: 1.24 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 268.80, NNZs: 582, Bias: -0.199386, T: 178200, Avg. loss: 0.116523\n",
      "Total training time: 1.31 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 268.89, NNZs: 577, Bias: -0.195592, T: 188100, Avg. loss: 0.119086\n",
      "Total training time: 1.38 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 268.98, NNZs: 565, Bias: -0.199715, T: 198000, Avg. loss: 0.119120\n",
      "Total training time: 1.44 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 269.06, NNZs: 555, Bias: -0.198218, T: 207900, Avg. loss: 0.118419\n",
      "Total training time: 1.51 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 269.13, NNZs: 549, Bias: -0.197744, T: 217800, Avg. loss: 0.116656\n",
      "Total training time: 1.57 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 269.20, NNZs: 542, Bias: -0.198194, T: 227700, Avg. loss: 0.116549\n",
      "Total training time: 1.64 seconds.\n",
      "Convergence after 23 epochs took 1.64 seconds\n",
      "-- Epoch 1\n",
      "Norm: 245.22, NNZs: 1443, Bias: -0.139118, T: 9900, Avg. loss: 3.159812\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 257.17, NNZs: 1050, Bias: -0.121706, T: 19800, Avg. loss: 0.597350\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 261.41, NNZs: 899, Bias: -0.151461, T: 29700, Avg. loss: 0.397273\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 263.49, NNZs: 818, Bias: -0.157251, T: 39600, Avg. loss: 0.306376\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 264.65, NNZs: 758, Bias: -0.169820, T: 49500, Avg. loss: 0.264421\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 265.42, NNZs: 711, Bias: -0.165044, T: 59400, Avg. loss: 0.232840\n",
      "Total training time: 0.42 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 265.98, NNZs: 676, Bias: -0.172530, T: 69300, Avg. loss: 0.220534\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 266.36, NNZs: 645, Bias: -0.170334, T: 79200, Avg. loss: 0.200431\n",
      "Total training time: 0.55 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 266.67, NNZs: 620, Bias: -0.173667, T: 89100, Avg. loss: 0.194474\n",
      "Total training time: 0.62 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 266.93, NNZs: 604, Bias: -0.176880, T: 99000, Avg. loss: 0.185549\n",
      "Total training time: 0.69 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 267.12, NNZs: 582, Bias: -0.180632, T: 108900, Avg. loss: 0.178069\n",
      "Total training time: 0.75 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 267.29, NNZs: 569, Bias: -0.181554, T: 118800, Avg. loss: 0.168743\n",
      "Total training time: 0.82 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 267.43, NNZs: 555, Bias: -0.184779, T: 128700, Avg. loss: 0.169431\n",
      "Total training time: 0.89 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 267.57, NNZs: 546, Bias: -0.182681, T: 138600, Avg. loss: 0.162626\n",
      "Total training time: 0.96 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 267.68, NNZs: 536, Bias: -0.186209, T: 148500, Avg. loss: 0.162250\n",
      "Total training time: 1.02 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 267.77, NNZs: 531, Bias: -0.187543, T: 158400, Avg. loss: 0.158062\n",
      "Total training time: 1.09 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 267.86, NNZs: 523, Bias: -0.190604, T: 168300, Avg. loss: 0.153450\n",
      "Total training time: 1.15 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 267.94, NNZs: 518, Bias: -0.188952, T: 178200, Avg. loss: 0.151225\n",
      "Total training time: 1.22 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 268.02, NNZs: 509, Bias: -0.192255, T: 188100, Avg. loss: 0.153135\n",
      "Total training time: 1.28 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 268.09, NNZs: 505, Bias: -0.194829, T: 198000, Avg. loss: 0.150868\n",
      "Total training time: 1.35 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 268.15, NNZs: 500, Bias: -0.196317, T: 207900, Avg. loss: 0.150643\n",
      "Total training time: 1.42 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 268.21, NNZs: 493, Bias: -0.198618, T: 217800, Avg. loss: 0.145281\n",
      "Total training time: 1.48 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 268.27, NNZs: 488, Bias: -0.199073, T: 227700, Avg. loss: 0.144328\n",
      "Total training time: 1.55 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 268.32, NNZs: 486, Bias: -0.199448, T: 237600, Avg. loss: 0.147634\n",
      "Total training time: 1.61 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 268.37, NNZs: 483, Bias: -0.201498, T: 247500, Avg. loss: 0.145631\n",
      "Total training time: 1.68 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 268.41, NNZs: 478, Bias: -0.200734, T: 257400, Avg. loss: 0.143745\n",
      "Total training time: 1.74 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 268.46, NNZs: 475, Bias: -0.204150, T: 267300, Avg. loss: 0.140824\n",
      "Total training time: 1.80 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 268.50, NNZs: 472, Bias: -0.204141, T: 277200, Avg. loss: 0.139456\n",
      "Total training time: 1.87 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 268.54, NNZs: 468, Bias: -0.204161, T: 287100, Avg. loss: 0.140801\n",
      "Total training time: 1.93 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 268.58, NNZs: 465, Bias: -0.206519, T: 297000, Avg. loss: 0.140104\n",
      "Total training time: 2.00 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 268.62, NNZs: 461, Bias: -0.205875, T: 306900, Avg. loss: 0.137053\n",
      "Total training time: 2.06 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 268.65, NNZs: 458, Bias: -0.206850, T: 316800, Avg. loss: 0.137943\n",
      "Total training time: 2.12 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 268.69, NNZs: 455, Bias: -0.207806, T: 326700, Avg. loss: 0.137389\n",
      "Total training time: 2.18 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 268.72, NNZs: 454, Bias: -0.210501, T: 336600, Avg. loss: 0.136540\n",
      "Total training time: 2.24 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 268.75, NNZs: 452, Bias: -0.211663, T: 346500, Avg. loss: 0.136366\n",
      "Total training time: 2.31 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 268.79, NNZs: 449, Bias: -0.210808, T: 356400, Avg. loss: 0.135074\n",
      "Total training time: 2.37 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 268.81, NNZs: 449, Bias: -0.211654, T: 366300, Avg. loss: 0.137312\n",
      "Total training time: 2.43 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 268.84, NNZs: 445, Bias: -0.212694, T: 376200, Avg. loss: 0.137342\n",
      "Total training time: 2.50 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 268.86, NNZs: 441, Bias: -0.213743, T: 386100, Avg. loss: 0.135206\n",
      "Total training time: 2.56 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 268.89, NNZs: 437, Bias: -0.213221, T: 396000, Avg. loss: 0.134461\n",
      "Total training time: 2.63 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 268.91, NNZs: 437, Bias: -0.215216, T: 405900, Avg. loss: 0.132809\n",
      "Total training time: 2.69 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 268.94, NNZs: 435, Bias: -0.215468, T: 415800, Avg. loss: 0.133914\n",
      "Total training time: 2.75 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 268.96, NNZs: 432, Bias: -0.215958, T: 425700, Avg. loss: 0.133531\n",
      "Total training time: 2.81 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 268.98, NNZs: 433, Bias: -0.216185, T: 435600, Avg. loss: 0.132057\n",
      "Total training time: 2.88 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 269.01, NNZs: 433, Bias: -0.216421, T: 445500, Avg. loss: 0.132586\n",
      "Total training time: 2.94 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 269.03, NNZs: 432, Bias: -0.217301, T: 455400, Avg. loss: 0.133326\n",
      "Total training time: 3.00 seconds.\n",
      "Convergence after 46 epochs took 3.00 seconds\n",
      "-- Epoch 1\n",
      "Norm: 240.37, NNZs: 1717, Bias: -0.233445, T: 9900, Avg. loss: 3.908785\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 249.48, NNZs: 1227, Bias: -0.080368, T: 19800, Avg. loss: 0.790688\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 253.31, NNZs: 1025, Bias: -0.071307, T: 29700, Avg. loss: 0.442482\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 255.06, NNZs: 901, Bias: -0.067885, T: 39600, Avg. loss: 0.360321\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 256.17, NNZs: 812, Bias: -0.056318, T: 49500, Avg. loss: 0.293998\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 256.89, NNZs: 739, Bias: -0.061400, T: 59400, Avg. loss: 0.264170\n",
      "Total training time: 0.44 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 257.39, NNZs: 697, Bias: -0.059362, T: 69300, Avg. loss: 0.244329\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 257.72, NNZs: 658, Bias: -0.059992, T: 79200, Avg. loss: 0.247305\n",
      "Total training time: 0.57 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 258.03, NNZs: 627, Bias: -0.072524, T: 89100, Avg. loss: 0.227147\n",
      "Total training time: 0.63 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 258.23, NNZs: 602, Bias: -0.068189, T: 99000, Avg. loss: 0.223670\n",
      "Total training time: 0.70 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 258.43, NNZs: 588, Bias: -0.069902, T: 108900, Avg. loss: 0.212623\n",
      "Total training time: 0.76 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 258.60, NNZs: 570, Bias: -0.070642, T: 118800, Avg. loss: 0.208987\n",
      "Total training time: 0.83 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 258.74, NNZs: 554, Bias: -0.066643, T: 128700, Avg. loss: 0.200545\n",
      "Total training time: 0.90 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 258.87, NNZs: 537, Bias: -0.068283, T: 138600, Avg. loss: 0.190068\n",
      "Total training time: 0.96 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 258.99, NNZs: 530, Bias: -0.069764, T: 148500, Avg. loss: 0.187186\n",
      "Total training time: 1.02 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 259.08, NNZs: 516, Bias: -0.075592, T: 158400, Avg. loss: 0.188143\n",
      "Total training time: 1.08 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 259.18, NNZs: 505, Bias: -0.076731, T: 168300, Avg. loss: 0.187340\n",
      "Total training time: 1.15 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 259.26, NNZs: 500, Bias: -0.075576, T: 178200, Avg. loss: 0.180605\n",
      "Total training time: 1.22 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 259.34, NNZs: 495, Bias: -0.076647, T: 188100, Avg. loss: 0.175993\n",
      "Total training time: 1.28 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 259.41, NNZs: 487, Bias: -0.078198, T: 198000, Avg. loss: 0.180931\n",
      "Total training time: 1.34 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 259.47, NNZs: 481, Bias: -0.078284, T: 207900, Avg. loss: 0.178513\n",
      "Total training time: 1.41 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 259.54, NNZs: 474, Bias: -0.079663, T: 217800, Avg. loss: 0.172059\n",
      "Total training time: 1.49 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 259.60, NNZs: 468, Bias: -0.082373, T: 227700, Avg. loss: 0.170501\n",
      "Total training time: 1.56 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 259.65, NNZs: 466, Bias: -0.083204, T: 237600, Avg. loss: 0.168763\n",
      "Total training time: 1.62 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 259.71, NNZs: 461, Bias: -0.084795, T: 247500, Avg. loss: 0.170707\n",
      "Total training time: 1.69 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 259.75, NNZs: 455, Bias: -0.085935, T: 257400, Avg. loss: 0.170657\n",
      "Total training time: 1.76 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 259.79, NNZs: 455, Bias: -0.084457, T: 267300, Avg. loss: 0.171801\n",
      "Total training time: 1.84 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 259.83, NNZs: 449, Bias: -0.087018, T: 277200, Avg. loss: 0.167032\n",
      "Total training time: 1.91 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 259.88, NNZs: 447, Bias: -0.086345, T: 287100, Avg. loss: 0.170932\n",
      "Total training time: 1.98 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 259.91, NNZs: 444, Bias: -0.088055, T: 297000, Avg. loss: 0.167588\n",
      "Total training time: 2.04 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 259.95, NNZs: 439, Bias: -0.090354, T: 306900, Avg. loss: 0.165350\n",
      "Total training time: 2.11 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 259.98, NNZs: 437, Bias: -0.091948, T: 316800, Avg. loss: 0.166370\n",
      "Total training time: 2.18 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 260.02, NNZs: 433, Bias: -0.091628, T: 326700, Avg. loss: 0.165544\n",
      "Total training time: 2.24 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 260.05, NNZs: 430, Bias: -0.092496, T: 336600, Avg. loss: 0.163024\n",
      "Total training time: 2.31 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 260.08, NNZs: 428, Bias: -0.093610, T: 346500, Avg. loss: 0.160406\n",
      "Total training time: 2.38 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 260.11, NNZs: 427, Bias: -0.095267, T: 356400, Avg. loss: 0.162394\n",
      "Total training time: 2.44 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 260.14, NNZs: 424, Bias: -0.093363, T: 366300, Avg. loss: 0.163582\n",
      "Total training time: 2.51 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 260.17, NNZs: 418, Bias: -0.094996, T: 376200, Avg. loss: 0.163177\n",
      "Total training time: 2.57 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 260.19, NNZs: 418, Bias: -0.096322, T: 386100, Avg. loss: 0.161163\n",
      "Total training time: 2.64 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 260.22, NNZs: 418, Bias: -0.096839, T: 396000, Avg. loss: 0.159915\n",
      "Total training time: 2.71 seconds.\n",
      "Convergence after 40 epochs took 2.71 seconds\n",
      "-- Epoch 1\n",
      "Norm: 245.55, NNZs: 1724, Bias: -0.084553, T: 9900, Avg. loss: 2.747338\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 252.63, NNZs: 1313, Bias: -0.128060, T: 19800, Avg. loss: 0.594216\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 254.54, NNZs: 1086, Bias: -0.162848, T: 29700, Avg. loss: 0.365744\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 255.31, NNZs: 968, Bias: -0.140736, T: 39600, Avg. loss: 0.296733\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 255.72, NNZs: 890, Bias: -0.141991, T: 49500, Avg. loss: 0.249373\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 255.99, NNZs: 833, Bias: -0.138133, T: 59400, Avg. loss: 0.223229\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 256.13, NNZs: 788, Bias: -0.133079, T: 69300, Avg. loss: 0.205221\n",
      "Total training time: 0.53 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 256.23, NNZs: 748, Bias: -0.130196, T: 79200, Avg. loss: 0.198753\n",
      "Total training time: 0.60 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 256.35, NNZs: 720, Bias: -0.131220, T: 89100, Avg. loss: 0.181477\n",
      "Total training time: 0.67 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 256.41, NNZs: 702, Bias: -0.132090, T: 99000, Avg. loss: 0.172649\n",
      "Total training time: 0.73 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 256.47, NNZs: 683, Bias: -0.126424, T: 108900, Avg. loss: 0.163228\n",
      "Total training time: 0.80 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 256.52, NNZs: 669, Bias: -0.125439, T: 118800, Avg. loss: 0.158061\n",
      "Total training time: 0.87 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 256.55, NNZs: 656, Bias: -0.121972, T: 128700, Avg. loss: 0.153544\n",
      "Total training time: 0.94 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 256.60, NNZs: 646, Bias: -0.123418, T: 138600, Avg. loss: 0.145751\n",
      "Total training time: 1.01 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 256.64, NNZs: 636, Bias: -0.117158, T: 148500, Avg. loss: 0.144765\n",
      "Total training time: 1.08 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 256.67, NNZs: 628, Bias: -0.114605, T: 158400, Avg. loss: 0.138931\n",
      "Total training time: 1.14 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 256.71, NNZs: 617, Bias: -0.110246, T: 168300, Avg. loss: 0.137680\n",
      "Total training time: 1.21 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 256.74, NNZs: 611, Bias: -0.111395, T: 178200, Avg. loss: 0.136223\n",
      "Total training time: 1.28 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 256.77, NNZs: 604, Bias: -0.107565, T: 188100, Avg. loss: 0.128857\n",
      "Total training time: 1.35 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 256.80, NNZs: 598, Bias: -0.106041, T: 198000, Avg. loss: 0.128709\n",
      "Total training time: 1.41 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 256.83, NNZs: 592, Bias: -0.105000, T: 207900, Avg. loss: 0.126856\n",
      "Total training time: 1.48 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 256.87, NNZs: 588, Bias: -0.105500, T: 217800, Avg. loss: 0.126343\n",
      "Total training time: 1.55 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 256.90, NNZs: 582, Bias: -0.105573, T: 227700, Avg. loss: 0.121759\n",
      "Total training time: 1.61 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 256.93, NNZs: 578, Bias: -0.106431, T: 237600, Avg. loss: 0.123883\n",
      "Total training time: 1.68 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 256.96, NNZs: 573, Bias: -0.105983, T: 247500, Avg. loss: 0.121518\n",
      "Total training time: 1.74 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 256.98, NNZs: 569, Bias: -0.107554, T: 257400, Avg. loss: 0.119211\n",
      "Total training time: 1.81 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 257.01, NNZs: 562, Bias: -0.106425, T: 267300, Avg. loss: 0.119623\n",
      "Total training time: 1.88 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 257.04, NNZs: 560, Bias: -0.105719, T: 277200, Avg. loss: 0.119750\n",
      "Total training time: 1.94 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 257.07, NNZs: 556, Bias: -0.106019, T: 287100, Avg. loss: 0.114788\n",
      "Total training time: 2.01 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 257.10, NNZs: 549, Bias: -0.104283, T: 297000, Avg. loss: 0.116807\n",
      "Total training time: 2.07 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 257.12, NNZs: 547, Bias: -0.101642, T: 306900, Avg. loss: 0.117415\n",
      "Total training time: 2.15 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 257.15, NNZs: 541, Bias: -0.101968, T: 316800, Avg. loss: 0.114843\n",
      "Total training time: 2.22 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 257.17, NNZs: 537, Bias: -0.100720, T: 326700, Avg. loss: 0.113719\n",
      "Total training time: 2.29 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 257.20, NNZs: 535, Bias: -0.100705, T: 336600, Avg. loss: 0.113276\n",
      "Total training time: 2.36 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 257.22, NNZs: 530, Bias: -0.100696, T: 346500, Avg. loss: 0.111299\n",
      "Total training time: 2.42 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 257.25, NNZs: 528, Bias: -0.100972, T: 356400, Avg. loss: 0.110661\n",
      "Total training time: 2.49 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 257.28, NNZs: 528, Bias: -0.100418, T: 366300, Avg. loss: 0.111542\n",
      "Total training time: 2.56 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 257.30, NNZs: 525, Bias: -0.100140, T: 376200, Avg. loss: 0.110672\n",
      "Total training time: 2.64 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 257.32, NNZs: 525, Bias: -0.099886, T: 386100, Avg. loss: 0.111263\n",
      "Total training time: 2.71 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 257.34, NNZs: 524, Bias: -0.098636, T: 396000, Avg. loss: 0.109200\n",
      "Total training time: 2.77 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 257.37, NNZs: 522, Bias: -0.098643, T: 405900, Avg. loss: 0.110318\n",
      "Total training time: 2.85 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 257.39, NNZs: 520, Bias: -0.097425, T: 415800, Avg. loss: 0.108187\n",
      "Total training time: 2.91 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 257.42, NNZs: 520, Bias: -0.097201, T: 425700, Avg. loss: 0.108688\n",
      "Total training time: 2.98 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 257.44, NNZs: 519, Bias: -0.098811, T: 435600, Avg. loss: 0.108832\n",
      "Total training time: 3.05 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 257.46, NNZs: 514, Bias: -0.097216, T: 445500, Avg. loss: 0.107880\n",
      "Total training time: 3.12 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 257.49, NNZs: 512, Bias: -0.096553, T: 455400, Avg. loss: 0.106638\n",
      "Total training time: 3.20 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 257.51, NNZs: 509, Bias: -0.096778, T: 465300, Avg. loss: 0.107961\n",
      "Total training time: 3.27 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 257.53, NNZs: 506, Bias: -0.096348, T: 475200, Avg. loss: 0.107620\n",
      "Total training time: 3.34 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 257.55, NNZs: 506, Bias: -0.097169, T: 485100, Avg. loss: 0.107270\n",
      "Total training time: 3.40 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 257.57, NNZs: 506, Bias: -0.096353, T: 495000, Avg. loss: 0.105805\n",
      "Total training time: 3.47 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 257.60, NNZs: 506, Bias: -0.096147, T: 504900, Avg. loss: 0.105002\n",
      "Total training time: 3.54 seconds.\n",
      "Convergence after 51 epochs took 3.54 seconds\n",
      "-- Epoch 1\n",
      "Norm: 222.15, NNZs: 1384, Bias: -0.106446, T: 9900, Avg. loss: 2.811253\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 230.44, NNZs: 1006, Bias: -0.145210, T: 19800, Avg. loss: 0.582386\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 233.09, NNZs: 868, Bias: -0.148072, T: 29700, Avg. loss: 0.372970\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 234.27, NNZs: 783, Bias: -0.135268, T: 39600, Avg. loss: 0.304696\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 234.96, NNZs: 713, Bias: -0.144068, T: 49500, Avg. loss: 0.267956\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 235.40, NNZs: 668, Bias: -0.146976, T: 59400, Avg. loss: 0.238772\n",
      "Total training time: 0.43 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 235.71, NNZs: 633, Bias: -0.143472, T: 69300, Avg. loss: 0.212075\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 235.95, NNZs: 613, Bias: -0.134458, T: 79200, Avg. loss: 0.203358\n",
      "Total training time: 0.57 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 236.12, NNZs: 589, Bias: -0.143833, T: 89100, Avg. loss: 0.197448\n",
      "Total training time: 0.64 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 236.27, NNZs: 561, Bias: -0.138887, T: 99000, Avg. loss: 0.181210\n",
      "Total training time: 0.70 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 236.38, NNZs: 546, Bias: -0.138941, T: 108900, Avg. loss: 0.180365\n",
      "Total training time: 0.77 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 236.48, NNZs: 530, Bias: -0.138144, T: 118800, Avg. loss: 0.176823\n",
      "Total training time: 0.84 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 236.56, NNZs: 515, Bias: -0.135122, T: 128700, Avg. loss: 0.175131\n",
      "Total training time: 0.90 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 236.62, NNZs: 505, Bias: -0.139548, T: 138600, Avg. loss: 0.170143\n",
      "Total training time: 0.96 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 236.69, NNZs: 498, Bias: -0.140853, T: 148500, Avg. loss: 0.167745\n",
      "Total training time: 1.03 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 236.74, NNZs: 495, Bias: -0.138943, T: 158400, Avg. loss: 0.165385\n",
      "Total training time: 1.10 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 236.79, NNZs: 488, Bias: -0.139591, T: 168300, Avg. loss: 0.161547\n",
      "Total training time: 1.16 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 236.85, NNZs: 476, Bias: -0.140675, T: 178200, Avg. loss: 0.157026\n",
      "Total training time: 1.22 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 236.90, NNZs: 473, Bias: -0.140101, T: 188100, Avg. loss: 0.154530\n",
      "Total training time: 1.29 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 236.95, NNZs: 467, Bias: -0.138080, T: 198000, Avg. loss: 0.156336\n",
      "Total training time: 1.36 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 236.99, NNZs: 461, Bias: -0.138561, T: 207900, Avg. loss: 0.152018\n",
      "Total training time: 1.42 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 237.03, NNZs: 452, Bias: -0.139450, T: 217800, Avg. loss: 0.152513\n",
      "Total training time: 1.49 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 237.07, NNZs: 446, Bias: -0.139888, T: 227700, Avg. loss: 0.146822\n",
      "Total training time: 1.55 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 237.11, NNZs: 444, Bias: -0.138230, T: 237600, Avg. loss: 0.150330\n",
      "Total training time: 1.61 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 237.15, NNZs: 441, Bias: -0.139100, T: 247500, Avg. loss: 0.146687\n",
      "Total training time: 1.68 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 237.18, NNZs: 433, Bias: -0.138768, T: 257400, Avg. loss: 0.147422\n",
      "Total training time: 1.74 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 237.20, NNZs: 429, Bias: -0.138811, T: 267300, Avg. loss: 0.144451\n",
      "Total training time: 1.80 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 237.23, NNZs: 428, Bias: -0.140268, T: 277200, Avg. loss: 0.144573\n",
      "Total training time: 1.87 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 237.26, NNZs: 426, Bias: -0.140925, T: 287100, Avg. loss: 0.141993\n",
      "Total training time: 1.94 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 237.30, NNZs: 424, Bias: -0.140590, T: 297000, Avg. loss: 0.142580\n",
      "Total training time: 2.00 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 237.32, NNZs: 418, Bias: -0.142219, T: 306900, Avg. loss: 0.139952\n",
      "Total training time: 2.07 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 237.35, NNZs: 415, Bias: -0.142175, T: 316800, Avg. loss: 0.139894\n",
      "Total training time: 2.13 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 237.38, NNZs: 414, Bias: -0.141545, T: 326700, Avg. loss: 0.139376\n",
      "Total training time: 2.20 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 237.41, NNZs: 411, Bias: -0.140071, T: 336600, Avg. loss: 0.137779\n",
      "Total training time: 2.26 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 237.43, NNZs: 410, Bias: -0.140926, T: 346500, Avg. loss: 0.137701\n",
      "Total training time: 2.32 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 237.46, NNZs: 412, Bias: -0.139255, T: 356400, Avg. loss: 0.139026\n",
      "Total training time: 2.39 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 237.48, NNZs: 403, Bias: -0.142276, T: 366300, Avg. loss: 0.138147\n",
      "Total training time: 2.46 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 237.51, NNZs: 402, Bias: -0.140674, T: 376200, Avg. loss: 0.136175\n",
      "Total training time: 2.52 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 237.53, NNZs: 402, Bias: -0.141726, T: 386100, Avg. loss: 0.137482\n",
      "Total training time: 2.59 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 237.55, NNZs: 401, Bias: -0.141186, T: 396000, Avg. loss: 0.136906\n",
      "Total training time: 2.66 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 237.57, NNZs: 399, Bias: -0.142187, T: 405900, Avg. loss: 0.135768\n",
      "Total training time: 2.72 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 237.60, NNZs: 400, Bias: -0.141447, T: 415800, Avg. loss: 0.134817\n",
      "Total training time: 2.79 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 237.62, NNZs: 399, Bias: -0.140989, T: 425700, Avg. loss: 0.136637\n",
      "Total training time: 2.85 seconds.\n",
      "Convergence after 43 epochs took 2.85 seconds\n",
      "-- Epoch 1\n",
      "Norm: 239.97, NNZs: 1422, Bias: -0.090336, T: 9900, Avg. loss: 3.019238\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 252.58, NNZs: 954, Bias: -0.100155, T: 19800, Avg. loss: 0.412057\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 257.29, NNZs: 777, Bias: -0.083603, T: 29700, Avg. loss: 0.256561\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 259.84, NNZs: 668, Bias: -0.080222, T: 39600, Avg. loss: 0.204189\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 261.61, NNZs: 608, Bias: -0.079703, T: 49500, Avg. loss: 0.169491\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 262.85, NNZs: 578, Bias: -0.075661, T: 59400, Avg. loss: 0.164097\n",
      "Total training time: 0.42 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 263.68, NNZs: 545, Bias: -0.072466, T: 69300, Avg. loss: 0.174811\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 264.47, NNZs: 517, Bias: -0.083043, T: 79200, Avg. loss: 0.142600\n",
      "Total training time: 0.56 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 265.10, NNZs: 498, Bias: -0.079146, T: 89100, Avg. loss: 0.141198\n",
      "Total training time: 0.62 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 265.62, NNZs: 482, Bias: -0.077064, T: 99000, Avg. loss: 0.138741\n",
      "Total training time: 0.69 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 266.07, NNZs: 470, Bias: -0.080020, T: 108900, Avg. loss: 0.141076\n",
      "Total training time: 0.76 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 266.47, NNZs: 464, Bias: -0.079935, T: 118800, Avg. loss: 0.134610\n",
      "Total training time: 0.82 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 266.82, NNZs: 459, Bias: -0.080635, T: 128700, Avg. loss: 0.134167\n",
      "Total training time: 0.89 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 267.14, NNZs: 449, Bias: -0.076964, T: 138600, Avg. loss: 0.133623\n",
      "Total training time: 0.95 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 267.41, NNZs: 438, Bias: -0.080522, T: 148500, Avg. loss: 0.134695\n",
      "Total training time: 1.02 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 267.66, NNZs: 430, Bias: -0.077402, T: 158400, Avg. loss: 0.134041\n",
      "Total training time: 1.08 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 267.89, NNZs: 419, Bias: -0.083535, T: 168300, Avg. loss: 0.134185\n",
      "Total training time: 1.15 seconds.\n",
      "Convergence after 17 epochs took 1.15 seconds\n",
      "-- Epoch 1\n",
      "Norm: 248.42, NNZs: 1232, Bias: -0.160559, T: 9900, Avg. loss: 3.393423\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 263.98, NNZs: 909, Bias: -0.162817, T: 19800, Avg. loss: 0.538709\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 270.35, NNZs: 785, Bias: -0.156310, T: 29700, Avg. loss: 0.317068\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 273.47, NNZs: 716, Bias: -0.140786, T: 39600, Avg. loss: 0.279037\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 275.48, NNZs: 673, Bias: -0.144319, T: 49500, Avg. loss: 0.233793\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 276.88, NNZs: 641, Bias: -0.149908, T: 59400, Avg. loss: 0.220453\n",
      "Total training time: 0.44 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 277.85, NNZs: 619, Bias: -0.151250, T: 69300, Avg. loss: 0.222358\n",
      "Total training time: 0.51 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 278.61, NNZs: 592, Bias: -0.153499, T: 79200, Avg. loss: 0.209236\n",
      "Total training time: 0.58 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 279.22, NNZs: 582, Bias: -0.152085, T: 89100, Avg. loss: 0.201556\n",
      "Total training time: 0.65 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 279.69, NNZs: 569, Bias: -0.152077, T: 99000, Avg. loss: 0.201995\n",
      "Total training time: 0.72 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 280.11, NNZs: 554, Bias: -0.152009, T: 108900, Avg. loss: 0.194510\n",
      "Total training time: 0.78 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 280.48, NNZs: 537, Bias: -0.153599, T: 118800, Avg. loss: 0.181958\n",
      "Total training time: 0.85 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 280.80, NNZs: 534, Bias: -0.154241, T: 128700, Avg. loss: 0.176788\n",
      "Total training time: 0.92 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 281.06, NNZs: 530, Bias: -0.150636, T: 138600, Avg. loss: 0.182664\n",
      "Total training time: 0.99 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 281.30, NNZs: 522, Bias: -0.154890, T: 148500, Avg. loss: 0.184410\n",
      "Total training time: 1.06 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 281.51, NNZs: 519, Bias: -0.154892, T: 158400, Avg. loss: 0.179030\n",
      "Total training time: 1.13 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 281.69, NNZs: 516, Bias: -0.156752, T: 168300, Avg. loss: 0.177771\n",
      "Total training time: 1.19 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 281.87, NNZs: 513, Bias: -0.156728, T: 178200, Avg. loss: 0.176072\n",
      "Total training time: 1.26 seconds.\n",
      "Convergence after 18 epochs took 1.26 seconds\n",
      "-- Epoch 1\n",
      "Norm: 256.84, NNZs: 1595, Bias: 0.023535, T: 9900, Avg. loss: 2.777141\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 266.80, NNZs: 1089, Bias: 0.004900, T: 19800, Avg. loss: 0.437473\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 270.14, NNZs: 885, Bias: 0.030611, T: 29700, Avg. loss: 0.261510\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 271.69, NNZs: 793, Bias: 0.028030, T: 39600, Avg. loss: 0.260822\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 272.50, NNZs: 727, Bias: 0.028647, T: 49500, Avg. loss: 0.220748\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 273.12, NNZs: 675, Bias: 0.027249, T: 59400, Avg. loss: 0.189904\n",
      "Total training time: 0.44 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 273.56, NNZs: 628, Bias: 0.036383, T: 69300, Avg. loss: 0.179816\n",
      "Total training time: 0.51 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 273.86, NNZs: 601, Bias: 0.034871, T: 79200, Avg. loss: 0.175392\n",
      "Total training time: 0.57 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 274.12, NNZs: 577, Bias: 0.034872, T: 89100, Avg. loss: 0.159433\n",
      "Total training time: 0.63 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 274.30, NNZs: 552, Bias: 0.039173, T: 99000, Avg. loss: 0.160476\n",
      "Total training time: 0.70 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 274.47, NNZs: 539, Bias: 0.038116, T: 108900, Avg. loss: 0.150217\n",
      "Total training time: 0.77 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 274.62, NNZs: 528, Bias: 0.039898, T: 118800, Avg. loss: 0.148134\n",
      "Total training time: 0.83 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 274.76, NNZs: 515, Bias: 0.042188, T: 128700, Avg. loss: 0.146125\n",
      "Total training time: 0.90 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 274.87, NNZs: 505, Bias: 0.040640, T: 138600, Avg. loss: 0.135950\n",
      "Total training time: 0.97 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 274.98, NNZs: 496, Bias: 0.040141, T: 148500, Avg. loss: 0.139699\n",
      "Total training time: 1.04 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 275.08, NNZs: 480, Bias: 0.044052, T: 158400, Avg. loss: 0.132084\n",
      "Total training time: 1.10 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 275.18, NNZs: 474, Bias: 0.044572, T: 168300, Avg. loss: 0.128280\n",
      "Total training time: 1.17 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 275.27, NNZs: 469, Bias: 0.045659, T: 178200, Avg. loss: 0.127737\n",
      "Total training time: 1.23 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 275.35, NNZs: 455, Bias: 0.043523, T: 188100, Avg. loss: 0.130810\n",
      "Total training time: 1.30 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 275.42, NNZs: 450, Bias: 0.045168, T: 198000, Avg. loss: 0.130247\n",
      "Total training time: 1.37 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 275.50, NNZs: 441, Bias: 0.046660, T: 207900, Avg. loss: 0.124144\n",
      "Total training time: 1.43 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 275.56, NNZs: 435, Bias: 0.048006, T: 217800, Avg. loss: 0.125554\n",
      "Total training time: 1.50 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 275.62, NNZs: 432, Bias: 0.047500, T: 227700, Avg. loss: 0.123192\n",
      "Total training time: 1.56 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 275.68, NNZs: 429, Bias: 0.047077, T: 237600, Avg. loss: 0.123040\n",
      "Total training time: 1.63 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 275.74, NNZs: 425, Bias: 0.046688, T: 247500, Avg. loss: 0.121804\n",
      "Total training time: 1.70 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 275.79, NNZs: 422, Bias: 0.045549, T: 257400, Avg. loss: 0.119135\n",
      "Total training time: 1.77 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 275.84, NNZs: 419, Bias: 0.048630, T: 267300, Avg. loss: 0.121255\n",
      "Total training time: 1.83 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 275.89, NNZs: 415, Bias: 0.047207, T: 277200, Avg. loss: 0.120861\n",
      "Total training time: 1.90 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 275.94, NNZs: 411, Bias: 0.048997, T: 287100, Avg. loss: 0.119318\n",
      "Total training time: 1.97 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 275.98, NNZs: 409, Bias: 0.050326, T: 297000, Avg. loss: 0.116090\n",
      "Total training time: 2.04 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 276.03, NNZs: 408, Bias: 0.048035, T: 306900, Avg. loss: 0.118453\n",
      "Total training time: 2.10 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 276.07, NNZs: 405, Bias: 0.049323, T: 316800, Avg. loss: 0.116050\n",
      "Total training time: 2.17 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 276.11, NNZs: 402, Bias: 0.051472, T: 326700, Avg. loss: 0.119188\n",
      "Total training time: 2.23 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 276.15, NNZs: 401, Bias: 0.051716, T: 336600, Avg. loss: 0.114502\n",
      "Total training time: 2.30 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 276.19, NNZs: 399, Bias: 0.051095, T: 346500, Avg. loss: 0.114741\n",
      "Total training time: 2.37 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 276.23, NNZs: 399, Bias: 0.052203, T: 356400, Avg. loss: 0.114770\n",
      "Total training time: 2.43 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 276.27, NNZs: 397, Bias: 0.049434, T: 366300, Avg. loss: 0.112174\n",
      "Total training time: 2.49 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 276.30, NNZs: 396, Bias: 0.051579, T: 376200, Avg. loss: 0.112523\n",
      "Total training time: 2.55 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 276.34, NNZs: 394, Bias: 0.050266, T: 386100, Avg. loss: 0.110652\n",
      "Total training time: 2.62 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 276.38, NNZs: 392, Bias: 0.052817, T: 396000, Avg. loss: 0.113734\n",
      "Total training time: 2.69 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 276.42, NNZs: 387, Bias: 0.051300, T: 405900, Avg. loss: 0.110584\n",
      "Total training time: 2.76 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 276.45, NNZs: 382, Bias: 0.053474, T: 415800, Avg. loss: 0.113466\n",
      "Total training time: 2.82 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 276.49, NNZs: 380, Bias: 0.051792, T: 425700, Avg. loss: 0.111087\n",
      "Total training time: 2.89 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 276.52, NNZs: 380, Bias: 0.051341, T: 435600, Avg. loss: 0.111836\n",
      "Total training time: 2.95 seconds.\n",
      "Convergence after 44 epochs took 2.95 seconds\n",
      "-- Epoch 1\n",
      "Norm: 245.90, NNZs: 1276, Bias: -0.058568, T: 9900, Avg. loss: 3.221694\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 262.05, NNZs: 978, Bias: -0.035644, T: 19800, Avg. loss: 0.483818\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 267.41, NNZs: 836, Bias: -0.036098, T: 29700, Avg. loss: 0.327538\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 270.58, NNZs: 769, Bias: -0.028428, T: 39600, Avg. loss: 0.233017\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 272.45, NNZs: 724, Bias: -0.030471, T: 49500, Avg. loss: 0.230482\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 273.84, NNZs: 696, Bias: -0.026975, T: 59400, Avg. loss: 0.188954\n",
      "Total training time: 0.42 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 274.83, NNZs: 668, Bias: -0.022761, T: 69300, Avg. loss: 0.185859\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 275.60, NNZs: 649, Bias: -0.029321, T: 79200, Avg. loss: 0.179463\n",
      "Total training time: 0.56 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 276.21, NNZs: 623, Bias: -0.024325, T: 89100, Avg. loss: 0.174047\n",
      "Total training time: 0.63 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 276.73, NNZs: 616, Bias: -0.026307, T: 99000, Avg. loss: 0.166924\n",
      "Total training time: 0.70 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 277.18, NNZs: 604, Bias: -0.024260, T: 108900, Avg. loss: 0.164080\n",
      "Total training time: 0.77 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 277.55, NNZs: 599, Bias: -0.020850, T: 118800, Avg. loss: 0.156031\n",
      "Total training time: 0.83 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 277.87, NNZs: 589, Bias: -0.020845, T: 128700, Avg. loss: 0.162116\n",
      "Total training time: 0.90 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 278.16, NNZs: 587, Bias: -0.018679, T: 138600, Avg. loss: 0.153703\n",
      "Total training time: 0.96 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 278.41, NNZs: 581, Bias: -0.020806, T: 148500, Avg. loss: 0.153921\n",
      "Total training time: 1.03 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 278.64, NNZs: 575, Bias: -0.023909, T: 158400, Avg. loss: 0.150908\n",
      "Total training time: 1.09 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 278.87, NNZs: 571, Bias: -0.020305, T: 168300, Avg. loss: 0.138528\n",
      "Total training time: 1.16 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 279.06, NNZs: 569, Bias: -0.020329, T: 178200, Avg. loss: 0.144925\n",
      "Total training time: 1.23 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 279.23, NNZs: 562, Bias: -0.022898, T: 188100, Avg. loss: 0.147311\n",
      "Total training time: 1.30 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 279.40, NNZs: 561, Bias: -0.020761, T: 198000, Avg. loss: 0.142495\n",
      "Total training time: 1.36 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 279.55, NNZs: 559, Bias: -0.021205, T: 207900, Avg. loss: 0.140112\n",
      "Total training time: 1.43 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 279.69, NNZs: 557, Bias: -0.020729, T: 217800, Avg. loss: 0.136901\n",
      "Total training time: 1.49 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 279.83, NNZs: 557, Bias: -0.019846, T: 227700, Avg. loss: 0.138192\n",
      "Total training time: 1.57 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 279.95, NNZs: 551, Bias: -0.019853, T: 237600, Avg. loss: 0.138507\n",
      "Total training time: 1.63 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 280.07, NNZs: 551, Bias: -0.019058, T: 247500, Avg. loss: 0.136362\n",
      "Total training time: 1.70 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 280.18, NNZs: 553, Bias: -0.021398, T: 257400, Avg. loss: 0.137837\n",
      "Total training time: 1.77 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 280.28, NNZs: 550, Bias: -0.019474, T: 267300, Avg. loss: 0.137597\n",
      "Total training time: 1.83 seconds.\n",
      "Convergence after 27 epochs took 1.83 seconds\n",
      "-- Epoch 1\n",
      "Norm: 246.05, NNZs: 1473, Bias: -0.157266, T: 9900, Avg. loss: 3.090600\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 260.33, NNZs: 1104, Bias: -0.148023, T: 19800, Avg. loss: 0.494623\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 265.12, NNZs: 943, Bias: -0.123586, T: 29700, Avg. loss: 0.311995\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 267.69, NNZs: 856, Bias: -0.116948, T: 39600, Avg. loss: 0.236158\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 269.36, NNZs: 801, Bias: -0.109652, T: 49500, Avg. loss: 0.208150\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 270.51, NNZs: 758, Bias: -0.099294, T: 59400, Avg. loss: 0.192641\n",
      "Total training time: 0.44 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 271.37, NNZs: 730, Bias: -0.101523, T: 69300, Avg. loss: 0.189178\n",
      "Total training time: 0.51 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 271.99, NNZs: 712, Bias: -0.098112, T: 79200, Avg. loss: 0.183842\n",
      "Total training time: 0.58 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 272.48, NNZs: 694, Bias: -0.097603, T: 89100, Avg. loss: 0.175214\n",
      "Total training time: 0.65 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 272.88, NNZs: 681, Bias: -0.100932, T: 99000, Avg. loss: 0.168166\n",
      "Total training time: 0.72 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 273.22, NNZs: 660, Bias: -0.101008, T: 108900, Avg. loss: 0.167713\n",
      "Total training time: 0.78 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 273.53, NNZs: 652, Bias: -0.102689, T: 118800, Avg. loss: 0.165552\n",
      "Total training time: 0.85 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 273.80, NNZs: 646, Bias: -0.096338, T: 128700, Avg. loss: 0.158579\n",
      "Total training time: 0.93 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 274.02, NNZs: 631, Bias: -0.100943, T: 138600, Avg. loss: 0.152947\n",
      "Total training time: 0.99 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 274.23, NNZs: 623, Bias: -0.102884, T: 148500, Avg. loss: 0.151367\n",
      "Total training time: 1.06 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 274.41, NNZs: 611, Bias: -0.102159, T: 158400, Avg. loss: 0.151575\n",
      "Total training time: 1.13 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 274.59, NNZs: 608, Bias: -0.097955, T: 168300, Avg. loss: 0.151194\n",
      "Total training time: 1.20 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 274.75, NNZs: 598, Bias: -0.101936, T: 178200, Avg. loss: 0.143932\n",
      "Total training time: 1.27 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 274.89, NNZs: 594, Bias: -0.100894, T: 188100, Avg. loss: 0.148848\n",
      "Total training time: 1.34 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 275.01, NNZs: 592, Bias: -0.099905, T: 198000, Avg. loss: 0.146280\n",
      "Total training time: 1.41 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 275.14, NNZs: 590, Bias: -0.102813, T: 207900, Avg. loss: 0.138486\n",
      "Total training time: 1.47 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 275.25, NNZs: 590, Bias: -0.103278, T: 217800, Avg. loss: 0.147709\n",
      "Total training time: 1.54 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 275.36, NNZs: 584, Bias: -0.101901, T: 227700, Avg. loss: 0.142886\n",
      "Total training time: 1.61 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 275.46, NNZs: 579, Bias: -0.101900, T: 237600, Avg. loss: 0.143009\n",
      "Total training time: 1.67 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 275.55, NNZs: 574, Bias: -0.101487, T: 247500, Avg. loss: 0.142770\n",
      "Total training time: 1.74 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 275.64, NNZs: 572, Bias: -0.101853, T: 257400, Avg. loss: 0.138400\n",
      "Total training time: 1.81 seconds.\n",
      "Convergence after 26 epochs took 1.81 seconds\n",
      "[LibLinear]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  11 out of  11 | elapsed:   26.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  11 out of  11 | elapsed:  1.3min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  11 out of  11 | elapsed:  1.5min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  11 out of  11 | elapsed:  1.4min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  11 out of  11 | elapsed:  1.3min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  11 out of  11 | elapsed:  1.2min finished\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  6.7min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 209.40, NNZs: 1390, Bias: -0.002837, T: 7920, Avg. loss: 3.258963\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 220.32, NNZs: 1043, Bias: -0.040210, T: 15840, Avg. loss: 0.829842\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 223.58, NNZs: 903, Bias: -0.064208, T: 23760, Avg. loss: 0.527697\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 225.33, NNZs: 819, Bias: -0.052933, T: 31680, Avg. loss: 0.400498\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 226.12, NNZs: 763, Bias: -0.054929, T: 39600, Avg. loss: 0.349444\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 226.68, NNZs: 729, Bias: -0.046120, T: 47520, Avg. loss: 0.303912\n",
      "Total training time: 0.41 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 227.00, NNZs: 683, Bias: -0.054079, T: 55440, Avg. loss: 0.275094\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 227.29, NNZs: 650, Bias: -0.053773, T: 63360, Avg. loss: 0.253266\n",
      "Total training time: 0.54 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 227.43, NNZs: 629, Bias: -0.053947, T: 71280, Avg. loss: 0.239224\n",
      "Total training time: 0.59 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 227.61, NNZs: 612, Bias: -0.051392, T: 79200, Avg. loss: 0.229400\n",
      "Total training time: 0.65 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 227.69, NNZs: 591, Bias: -0.054849, T: 87120, Avg. loss: 0.221712\n",
      "Total training time: 0.70 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 227.81, NNZs: 583, Bias: -0.051523, T: 95040, Avg. loss: 0.203561\n",
      "Total training time: 0.76 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 227.89, NNZs: 570, Bias: -0.047660, T: 102960, Avg. loss: 0.200710\n",
      "Total training time: 0.82 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 227.97, NNZs: 565, Bias: -0.048755, T: 110880, Avg. loss: 0.194795\n",
      "Total training time: 0.87 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 228.03, NNZs: 553, Bias: -0.051389, T: 118800, Avg. loss: 0.196724\n",
      "Total training time: 0.93 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 228.08, NNZs: 541, Bias: -0.053669, T: 126720, Avg. loss: 0.185602\n",
      "Total training time: 0.99 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 228.13, NNZs: 539, Bias: -0.053619, T: 134640, Avg. loss: 0.187783\n",
      "Total training time: 1.05 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 228.18, NNZs: 534, Bias: -0.050774, T: 142560, Avg. loss: 0.180292\n",
      "Total training time: 1.10 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 228.22, NNZs: 529, Bias: -0.054777, T: 150480, Avg. loss: 0.178033\n",
      "Total training time: 1.15 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 228.25, NNZs: 525, Bias: -0.052252, T: 158400, Avg. loss: 0.180264\n",
      "Total training time: 1.21 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 228.28, NNZs: 517, Bias: -0.052249, T: 166320, Avg. loss: 0.174174\n",
      "Total training time: 1.27 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 228.31, NNZs: 512, Bias: -0.052808, T: 174240, Avg. loss: 0.172921\n",
      "Total training time: 1.33 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 228.34, NNZs: 504, Bias: -0.051215, T: 182160, Avg. loss: 0.168613\n",
      "Total training time: 1.40 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 228.37, NNZs: 499, Bias: -0.051787, T: 190080, Avg. loss: 0.166521\n",
      "Total training time: 1.45 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 228.39, NNZs: 494, Bias: -0.054822, T: 198000, Avg. loss: 0.165808\n",
      "Total training time: 1.51 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 228.42, NNZs: 489, Bias: -0.053347, T: 205920, Avg. loss: 0.163624\n",
      "Total training time: 1.57 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 228.44, NNZs: 488, Bias: -0.053349, T: 213840, Avg. loss: 0.161176\n",
      "Total training time: 1.63 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 228.46, NNZs: 483, Bias: -0.051524, T: 221760, Avg. loss: 0.158416\n",
      "Total training time: 1.68 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 228.48, NNZs: 482, Bias: -0.055927, T: 229680, Avg. loss: 0.159847\n",
      "Total training time: 1.74 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 228.50, NNZs: 479, Bias: -0.052506, T: 237600, Avg. loss: 0.160058\n",
      "Total training time: 1.79 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 228.52, NNZs: 477, Bias: -0.053321, T: 245520, Avg. loss: 0.157224\n",
      "Total training time: 1.84 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 228.54, NNZs: 474, Bias: -0.052495, T: 253440, Avg. loss: 0.156973\n",
      "Total training time: 1.90 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 228.56, NNZs: 467, Bias: -0.052144, T: 261360, Avg. loss: 0.154391\n",
      "Total training time: 1.95 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 228.57, NNZs: 466, Bias: -0.053254, T: 269280, Avg. loss: 0.152420\n",
      "Total training time: 2.01 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 228.59, NNZs: 464, Bias: -0.052140, T: 277200, Avg. loss: 0.153142\n",
      "Total training time: 2.06 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 228.61, NNZs: 465, Bias: -0.052865, T: 285120, Avg. loss: 0.153583\n",
      "Total training time: 2.12 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 228.63, NNZs: 464, Bias: -0.052863, T: 293040, Avg. loss: 0.151159\n",
      "Total training time: 2.18 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 228.65, NNZs: 460, Bias: -0.052214, T: 300960, Avg. loss: 0.151404\n",
      "Total training time: 2.23 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 228.67, NNZs: 458, Bias: -0.051919, T: 308880, Avg. loss: 0.150420\n",
      "Total training time: 2.29 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 228.69, NNZs: 458, Bias: -0.052566, T: 316800, Avg. loss: 0.149790\n",
      "Total training time: 2.35 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 228.70, NNZs: 456, Bias: -0.051662, T: 324720, Avg. loss: 0.149101\n",
      "Total training time: 2.40 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 228.72, NNZs: 452, Bias: -0.051963, T: 332640, Avg. loss: 0.148675\n",
      "Total training time: 2.46 seconds.\n",
      "Convergence after 42 epochs took 2.46 seconds\n",
      "-- Epoch 1\n",
      "Norm: 237.43, NNZs: 1856, Bias: -0.284332, T: 7920, Avg. loss: 4.165637\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 253.82, NNZs: 1374, Bias: -0.249351, T: 15840, Avg. loss: 0.756896\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 258.46, NNZs: 1138, Bias: -0.220323, T: 23760, Avg. loss: 0.367399\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 260.70, NNZs: 1004, Bias: -0.209193, T: 31680, Avg. loss: 0.276888\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 262.03, NNZs: 912, Bias: -0.220405, T: 39600, Avg. loss: 0.227572\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 262.95, NNZs: 837, Bias: -0.211311, T: 47520, Avg. loss: 0.205121\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 263.58, NNZs: 797, Bias: -0.209235, T: 55440, Avg. loss: 0.182009\n",
      "Total training time: 0.43 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 264.04, NNZs: 752, Bias: -0.209207, T: 63360, Avg. loss: 0.177333\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 264.41, NNZs: 714, Bias: -0.207600, T: 71280, Avg. loss: 0.162303\n",
      "Total training time: 0.55 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 264.72, NNZs: 682, Bias: -0.211352, T: 79200, Avg. loss: 0.159111\n",
      "Total training time: 0.62 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 264.99, NNZs: 666, Bias: -0.206234, T: 87120, Avg. loss: 0.142610\n",
      "Total training time: 0.67 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 265.21, NNZs: 648, Bias: -0.209397, T: 95040, Avg. loss: 0.148410\n",
      "Total training time: 0.73 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 265.41, NNZs: 638, Bias: -0.209274, T: 102960, Avg. loss: 0.145456\n",
      "Total training time: 0.79 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 265.58, NNZs: 629, Bias: -0.201914, T: 110880, Avg. loss: 0.138122\n",
      "Total training time: 0.85 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 265.72, NNZs: 611, Bias: -0.207968, T: 118800, Avg. loss: 0.131908\n",
      "Total training time: 0.90 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 265.84, NNZs: 604, Bias: -0.205668, T: 126720, Avg. loss: 0.139262\n",
      "Total training time: 0.95 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 265.95, NNZs: 596, Bias: -0.206538, T: 134640, Avg. loss: 0.136862\n",
      "Total training time: 1.01 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 266.05, NNZs: 588, Bias: -0.207323, T: 142560, Avg. loss: 0.135393\n",
      "Total training time: 1.06 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 266.15, NNZs: 577, Bias: -0.207955, T: 150480, Avg. loss: 0.125959\n",
      "Total training time: 1.11 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 266.23, NNZs: 568, Bias: -0.209844, T: 158400, Avg. loss: 0.130692\n",
      "Total training time: 1.16 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 266.30, NNZs: 564, Bias: -0.209790, T: 166320, Avg. loss: 0.130056\n",
      "Total training time: 1.21 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 266.38, NNZs: 554, Bias: -0.207479, T: 174240, Avg. loss: 0.127466\n",
      "Total training time: 1.26 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 266.45, NNZs: 552, Bias: -0.208615, T: 182160, Avg. loss: 0.123036\n",
      "Total training time: 1.32 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 266.52, NNZs: 543, Bias: -0.212828, T: 190080, Avg. loss: 0.124197\n",
      "Total training time: 1.38 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 266.58, NNZs: 539, Bias: -0.209689, T: 198000, Avg. loss: 0.123045\n",
      "Total training time: 1.43 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 266.63, NNZs: 539, Bias: -0.210687, T: 205920, Avg. loss: 0.123948\n",
      "Total training time: 1.48 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 266.69, NNZs: 532, Bias: -0.210208, T: 213840, Avg. loss: 0.121528\n",
      "Total training time: 1.53 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 266.74, NNZs: 525, Bias: -0.208370, T: 221760, Avg. loss: 0.116881\n",
      "Total training time: 1.59 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 266.79, NNZs: 523, Bias: -0.208387, T: 229680, Avg. loss: 0.117230\n",
      "Total training time: 1.64 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 266.84, NNZs: 516, Bias: -0.210107, T: 237600, Avg. loss: 0.115074\n",
      "Total training time: 1.70 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 266.88, NNZs: 515, Bias: -0.210519, T: 245520, Avg. loss: 0.118113\n",
      "Total training time: 1.76 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 266.92, NNZs: 513, Bias: -0.209754, T: 253440, Avg. loss: 0.117565\n",
      "Total training time: 1.82 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 266.97, NNZs: 507, Bias: -0.210520, T: 261360, Avg. loss: 0.113852\n",
      "Total training time: 1.88 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 267.01, NNZs: 504, Bias: -0.210874, T: 269280, Avg. loss: 0.114988\n",
      "Total training time: 1.95 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 267.05, NNZs: 501, Bias: -0.211234, T: 277200, Avg. loss: 0.114918\n",
      "Total training time: 2.01 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 267.09, NNZs: 498, Bias: -0.211584, T: 285120, Avg. loss: 0.114280\n",
      "Total training time: 2.07 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 267.12, NNZs: 493, Bias: -0.212610, T: 293040, Avg. loss: 0.112636\n",
      "Total training time: 2.14 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 267.16, NNZs: 493, Bias: -0.212270, T: 300960, Avg. loss: 0.115280\n",
      "Total training time: 2.20 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 267.19, NNZs: 490, Bias: -0.212937, T: 308880, Avg. loss: 0.115985\n",
      "Total training time: 2.26 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 267.23, NNZs: 489, Bias: -0.212623, T: 316800, Avg. loss: 0.113997\n",
      "Total training time: 2.32 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 267.26, NNZs: 487, Bias: -0.211678, T: 324720, Avg. loss: 0.111065\n",
      "Total training time: 2.38 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 267.29, NNZs: 487, Bias: -0.211084, T: 332640, Avg. loss: 0.110711\n",
      "Total training time: 2.43 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 267.32, NNZs: 482, Bias: -0.213150, T: 340560, Avg. loss: 0.110411\n",
      "Total training time: 2.48 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 267.35, NNZs: 482, Bias: -0.212573, T: 348480, Avg. loss: 0.111453\n",
      "Total training time: 2.54 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 267.38, NNZs: 483, Bias: -0.212308, T: 356400, Avg. loss: 0.113145\n",
      "Total training time: 2.59 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 267.41, NNZs: 479, Bias: -0.213139, T: 364320, Avg. loss: 0.110005\n",
      "Total training time: 2.65 seconds.\n",
      "Convergence after 46 epochs took 2.65 seconds\n",
      "-- Epoch 1\n",
      "Norm: 238.82, NNZs: 1446, Bias: -0.097220, T: 7920, Avg. loss: 3.514580\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 254.15, NNZs: 1046, Bias: -0.113721, T: 15840, Avg. loss: 0.752733\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 259.15, NNZs: 903, Bias: -0.131406, T: 23760, Avg. loss: 0.467656\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 261.77, NNZs: 814, Bias: -0.155984, T: 31680, Avg. loss: 0.365744\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 263.32, NNZs: 758, Bias: -0.158400, T: 39600, Avg. loss: 0.297869\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 264.29, NNZs: 710, Bias: -0.169300, T: 47520, Avg. loss: 0.270640\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 264.95, NNZs: 680, Bias: -0.171072, T: 55440, Avg. loss: 0.239760\n",
      "Total training time: 0.41 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 265.51, NNZs: 653, Bias: -0.174350, T: 63360, Avg. loss: 0.219928\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 265.94, NNZs: 633, Bias: -0.180227, T: 71280, Avg. loss: 0.199527\n",
      "Total training time: 0.53 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 266.29, NNZs: 613, Bias: -0.178786, T: 79200, Avg. loss: 0.197389\n",
      "Total training time: 0.58 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 266.59, NNZs: 595, Bias: -0.179981, T: 87120, Avg. loss: 0.187199\n",
      "Total training time: 0.64 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 266.84, NNZs: 583, Bias: -0.179883, T: 95040, Avg. loss: 0.178576\n",
      "Total training time: 0.69 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 267.04, NNZs: 573, Bias: -0.181812, T: 102960, Avg. loss: 0.181008\n",
      "Total training time: 0.75 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 267.23, NNZs: 562, Bias: -0.181863, T: 110880, Avg. loss: 0.178744\n",
      "Total training time: 0.81 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 267.38, NNZs: 555, Bias: -0.186131, T: 118800, Avg. loss: 0.168895\n",
      "Total training time: 0.87 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 267.50, NNZs: 546, Bias: -0.186149, T: 126720, Avg. loss: 0.168319\n",
      "Total training time: 0.92 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 267.62, NNZs: 545, Bias: -0.185386, T: 134640, Avg. loss: 0.160250\n",
      "Total training time: 0.97 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 267.73, NNZs: 539, Bias: -0.187465, T: 142560, Avg. loss: 0.161619\n",
      "Total training time: 1.02 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 267.84, NNZs: 531, Bias: -0.190099, T: 150480, Avg. loss: 0.157284\n",
      "Total training time: 1.08 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 267.95, NNZs: 527, Bias: -0.186321, T: 158400, Avg. loss: 0.152758\n",
      "Total training time: 1.14 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 268.03, NNZs: 522, Bias: -0.190054, T: 166320, Avg. loss: 0.153196\n",
      "Total training time: 1.19 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 268.11, NNZs: 519, Bias: -0.193008, T: 174240, Avg. loss: 0.150860\n",
      "Total training time: 1.24 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 268.18, NNZs: 512, Bias: -0.193535, T: 182160, Avg. loss: 0.149772\n",
      "Total training time: 1.29 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 268.26, NNZs: 508, Bias: -0.192470, T: 190080, Avg. loss: 0.149146\n",
      "Total training time: 1.34 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 268.32, NNZs: 502, Bias: -0.192493, T: 198000, Avg. loss: 0.147233\n",
      "Total training time: 1.40 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 268.38, NNZs: 500, Bias: -0.194961, T: 205920, Avg. loss: 0.145035\n",
      "Total training time: 1.45 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 268.44, NNZs: 495, Bias: -0.196869, T: 213840, Avg. loss: 0.141606\n",
      "Total training time: 1.51 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 268.50, NNZs: 491, Bias: -0.199563, T: 221760, Avg. loss: 0.142028\n",
      "Total training time: 1.56 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 268.55, NNZs: 488, Bias: -0.197800, T: 229680, Avg. loss: 0.140899\n",
      "Total training time: 1.61 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 268.60, NNZs: 485, Bias: -0.199064, T: 237600, Avg. loss: 0.142020\n",
      "Total training time: 1.67 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 268.65, NNZs: 481, Bias: -0.200668, T: 245520, Avg. loss: 0.143467\n",
      "Total training time: 1.72 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 268.69, NNZs: 477, Bias: -0.201059, T: 253440, Avg. loss: 0.138390\n",
      "Total training time: 1.77 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 268.74, NNZs: 473, Bias: -0.201077, T: 261360, Avg. loss: 0.138795\n",
      "Total training time: 1.83 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 268.78, NNZs: 473, Bias: -0.201448, T: 269280, Avg. loss: 0.138339\n",
      "Total training time: 1.88 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 268.82, NNZs: 468, Bias: -0.202549, T: 277200, Avg. loss: 0.136861\n",
      "Total training time: 1.93 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 268.86, NNZs: 465, Bias: -0.204327, T: 285120, Avg. loss: 0.136514\n",
      "Total training time: 1.98 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 268.90, NNZs: 465, Bias: -0.204987, T: 293040, Avg. loss: 0.137802\n",
      "Total training time: 2.03 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 268.94, NNZs: 459, Bias: -0.203332, T: 300960, Avg. loss: 0.134601\n",
      "Total training time: 2.09 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 268.97, NNZs: 457, Bias: -0.204977, T: 308880, Avg. loss: 0.135428\n",
      "Total training time: 2.14 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 269.00, NNZs: 457, Bias: -0.205951, T: 316800, Avg. loss: 0.133973\n",
      "Total training time: 2.19 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 269.04, NNZs: 453, Bias: -0.207502, T: 324720, Avg. loss: 0.135153\n",
      "Total training time: 2.25 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 269.07, NNZs: 451, Bias: -0.207497, T: 332640, Avg. loss: 0.133721\n",
      "Total training time: 2.31 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 269.10, NNZs: 448, Bias: -0.208378, T: 340560, Avg. loss: 0.130931\n",
      "Total training time: 2.37 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 269.13, NNZs: 447, Bias: -0.208667, T: 348480, Avg. loss: 0.132424\n",
      "Total training time: 2.43 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 269.16, NNZs: 447, Bias: -0.210081, T: 356400, Avg. loss: 0.132854\n",
      "Total training time: 2.49 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 269.19, NNZs: 446, Bias: -0.210355, T: 364320, Avg. loss: 0.131864\n",
      "Total training time: 2.56 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 269.21, NNZs: 445, Bias: -0.210616, T: 372240, Avg. loss: 0.130400\n",
      "Total training time: 2.62 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 269.24, NNZs: 445, Bias: -0.212205, T: 380160, Avg. loss: 0.132265\n",
      "Total training time: 2.68 seconds.\n",
      "Convergence after 48 epochs took 2.68 seconds\n",
      "-- Epoch 1\n",
      "Norm: 230.59, NNZs: 1745, Bias: -0.230652, T: 7920, Avg. loss: 4.362176\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 242.44, NNZs: 1294, Bias: -0.099431, T: 15840, Avg. loss: 1.031217\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 246.81, NNZs: 1079, Bias: -0.066625, T: 23760, Avg. loss: 0.541763\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 249.28, NNZs: 949, Bias: -0.062464, T: 31680, Avg. loss: 0.405456\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 250.65, NNZs: 864, Bias: -0.055734, T: 39600, Avg. loss: 0.322320\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 251.63, NNZs: 802, Bias: -0.066695, T: 47520, Avg. loss: 0.275104\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 252.29, NNZs: 755, Bias: -0.059016, T: 55440, Avg. loss: 0.274601\n",
      "Total training time: 0.43 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 252.87, NNZs: 709, Bias: -0.068263, T: 63360, Avg. loss: 0.225699\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 253.26, NNZs: 684, Bias: -0.066795, T: 71280, Avg. loss: 0.240504\n",
      "Total training time: 0.54 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 253.59, NNZs: 658, Bias: -0.070965, T: 79200, Avg. loss: 0.226923\n",
      "Total training time: 0.59 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 253.85, NNZs: 640, Bias: -0.071041, T: 87120, Avg. loss: 0.217288\n",
      "Total training time: 0.65 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 254.09, NNZs: 606, Bias: -0.075392, T: 95040, Avg. loss: 0.203197\n",
      "Total training time: 0.71 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 254.29, NNZs: 596, Bias: -0.072627, T: 102960, Avg. loss: 0.206822\n",
      "Total training time: 0.77 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 254.47, NNZs: 583, Bias: -0.082827, T: 110880, Avg. loss: 0.194763\n",
      "Total training time: 0.83 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 254.62, NNZs: 571, Bias: -0.080209, T: 118800, Avg. loss: 0.196826\n",
      "Total training time: 0.89 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 254.75, NNZs: 556, Bias: -0.081794, T: 126720, Avg. loss: 0.194203\n",
      "Total training time: 0.95 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 254.86, NNZs: 543, Bias: -0.082577, T: 134640, Avg. loss: 0.187059\n",
      "Total training time: 1.01 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 254.97, NNZs: 532, Bias: -0.082654, T: 142560, Avg. loss: 0.183024\n",
      "Total training time: 1.07 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 255.07, NNZs: 521, Bias: -0.085995, T: 150480, Avg. loss: 0.174903\n",
      "Total training time: 1.13 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 255.17, NNZs: 518, Bias: -0.087270, T: 158400, Avg. loss: 0.178425\n",
      "Total training time: 1.19 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 255.25, NNZs: 511, Bias: -0.086652, T: 166320, Avg. loss: 0.179833\n",
      "Total training time: 1.25 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 255.34, NNZs: 504, Bias: -0.088327, T: 174240, Avg. loss: 0.170312\n",
      "Total training time: 1.32 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 255.41, NNZs: 495, Bias: -0.089490, T: 182160, Avg. loss: 0.175777\n",
      "Total training time: 1.38 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 255.48, NNZs: 489, Bias: -0.092104, T: 190080, Avg. loss: 0.171943\n",
      "Total training time: 1.44 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 255.54, NNZs: 486, Bias: -0.094672, T: 198000, Avg. loss: 0.173727\n",
      "Total training time: 1.50 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 255.60, NNZs: 484, Bias: -0.094670, T: 205920, Avg. loss: 0.176057\n",
      "Total training time: 1.56 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 255.64, NNZs: 479, Bias: -0.096538, T: 213840, Avg. loss: 0.174508\n",
      "Total training time: 1.62 seconds.\n",
      "Convergence after 27 epochs took 1.62 seconds\n",
      "-- Epoch 1\n",
      "Norm: 235.73, NNZs: 1827, Bias: -0.105087, T: 7920, Avg. loss: 3.116405\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 246.98, NNZs: 1353, Bias: -0.133291, T: 15840, Avg. loss: 0.623731\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 250.17, NNZs: 1149, Bias: -0.134571, T: 23760, Avg. loss: 0.407831\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 251.50, NNZs: 1015, Bias: -0.128590, T: 31680, Avg. loss: 0.335033\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 252.29, NNZs: 927, Bias: -0.135778, T: 39600, Avg. loss: 0.282200\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 252.70, NNZs: 862, Bias: -0.138637, T: 47520, Avg. loss: 0.252788\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 253.04, NNZs: 821, Bias: -0.135931, T: 55440, Avg. loss: 0.231261\n",
      "Total training time: 0.44 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 253.25, NNZs: 781, Bias: -0.134298, T: 63360, Avg. loss: 0.201913\n",
      "Total training time: 0.51 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 253.46, NNZs: 752, Bias: -0.127500, T: 71280, Avg. loss: 0.185150\n",
      "Total training time: 0.57 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 253.61, NNZs: 737, Bias: -0.129915, T: 79200, Avg. loss: 0.177485\n",
      "Total training time: 0.64 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 253.74, NNZs: 724, Bias: -0.129835, T: 87120, Avg. loss: 0.172406\n",
      "Total training time: 0.69 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 253.85, NNZs: 710, Bias: -0.124500, T: 95040, Avg. loss: 0.163751\n",
      "Total training time: 0.74 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 253.94, NNZs: 698, Bias: -0.123351, T: 102960, Avg. loss: 0.164717\n",
      "Total training time: 0.80 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 254.00, NNZs: 685, Bias: -0.124310, T: 110880, Avg. loss: 0.159151\n",
      "Total training time: 0.85 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 254.07, NNZs: 674, Bias: -0.122588, T: 118800, Avg. loss: 0.153132\n",
      "Total training time: 0.90 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 254.13, NNZs: 657, Bias: -0.120934, T: 126720, Avg. loss: 0.146926\n",
      "Total training time: 0.95 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 254.19, NNZs: 647, Bias: -0.117225, T: 134640, Avg. loss: 0.143250\n",
      "Total training time: 1.01 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 254.23, NNZs: 638, Bias: -0.118789, T: 142560, Avg. loss: 0.142109\n",
      "Total training time: 1.06 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 254.28, NNZs: 635, Bias: -0.120070, T: 150480, Avg. loss: 0.137245\n",
      "Total training time: 1.11 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 254.33, NNZs: 626, Bias: -0.118805, T: 158400, Avg. loss: 0.136132\n",
      "Total training time: 1.16 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 254.38, NNZs: 619, Bias: -0.119399, T: 166320, Avg. loss: 0.133097\n",
      "Total training time: 1.21 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 254.42, NNZs: 608, Bias: -0.118823, T: 174240, Avg. loss: 0.132142\n",
      "Total training time: 1.26 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 254.47, NNZs: 606, Bias: -0.121037, T: 182160, Avg. loss: 0.127284\n",
      "Total training time: 1.31 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 254.51, NNZs: 598, Bias: -0.118890, T: 190080, Avg. loss: 0.126016\n",
      "Total training time: 1.37 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 254.55, NNZs: 588, Bias: -0.116892, T: 198000, Avg. loss: 0.127981\n",
      "Total training time: 1.42 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 254.58, NNZs: 582, Bias: -0.116882, T: 205920, Avg. loss: 0.125496\n",
      "Total training time: 1.48 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 254.62, NNZs: 578, Bias: -0.117409, T: 213840, Avg. loss: 0.123413\n",
      "Total training time: 1.53 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 254.65, NNZs: 575, Bias: -0.116915, T: 221760, Avg. loss: 0.121501\n",
      "Total training time: 1.58 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 254.68, NNZs: 570, Bias: -0.116959, T: 229680, Avg. loss: 0.119066\n",
      "Total training time: 1.64 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 254.72, NNZs: 569, Bias: -0.116942, T: 237600, Avg. loss: 0.117619\n",
      "Total training time: 1.69 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 254.76, NNZs: 564, Bias: -0.116537, T: 245520, Avg. loss: 0.115893\n",
      "Total training time: 1.74 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 254.79, NNZs: 563, Bias: -0.114950, T: 253440, Avg. loss: 0.118431\n",
      "Total training time: 1.79 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 254.82, NNZs: 562, Bias: -0.117272, T: 261360, Avg. loss: 0.115150\n",
      "Total training time: 1.84 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 254.85, NNZs: 559, Bias: -0.116115, T: 269280, Avg. loss: 0.115333\n",
      "Total training time: 1.89 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 254.89, NNZs: 557, Bias: -0.114681, T: 277200, Avg. loss: 0.112736\n",
      "Total training time: 1.94 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 254.91, NNZs: 550, Bias: -0.114328, T: 285120, Avg. loss: 0.113474\n",
      "Total training time: 1.99 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 254.94, NNZs: 546, Bias: -0.112281, T: 293040, Avg. loss: 0.111766\n",
      "Total training time: 2.04 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 254.97, NNZs: 544, Bias: -0.113309, T: 300960, Avg. loss: 0.112395\n",
      "Total training time: 2.10 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 255.00, NNZs: 543, Bias: -0.115576, T: 308880, Avg. loss: 0.108808\n",
      "Total training time: 2.15 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 255.03, NNZs: 541, Bias: -0.115247, T: 316800, Avg. loss: 0.109930\n",
      "Total training time: 2.20 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 255.06, NNZs: 540, Bias: -0.114925, T: 324720, Avg. loss: 0.108480\n",
      "Total training time: 2.26 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 255.08, NNZs: 537, Bias: -0.113735, T: 332640, Avg. loss: 0.108884\n",
      "Total training time: 2.31 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 255.11, NNZs: 535, Bias: -0.114900, T: 340560, Avg. loss: 0.108266\n",
      "Total training time: 2.36 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 255.14, NNZs: 533, Bias: -0.114308, T: 348480, Avg. loss: 0.107741\n",
      "Total training time: 2.41 seconds.\n",
      "Convergence after 44 epochs took 2.41 seconds\n",
      "-- Epoch 1\n",
      "Norm: 219.29, NNZs: 1537, Bias: -0.102352, T: 7920, Avg. loss: 3.042785\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 230.56, NNZs: 1104, Bias: -0.102414, T: 15840, Avg. loss: 0.654102\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 233.86, NNZs: 938, Bias: -0.115251, T: 23760, Avg. loss: 0.428588\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 235.49, NNZs: 830, Bias: -0.131929, T: 31680, Avg. loss: 0.348344\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 236.30, NNZs: 769, Bias: -0.132396, T: 39600, Avg. loss: 0.299179\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 236.90, NNZs: 717, Bias: -0.131173, T: 47520, Avg. loss: 0.262175\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 237.31, NNZs: 670, Bias: -0.125032, T: 55440, Avg. loss: 0.232246\n",
      "Total training time: 0.41 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 237.63, NNZs: 648, Bias: -0.131147, T: 63360, Avg. loss: 0.215067\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 237.82, NNZs: 628, Bias: -0.128161, T: 71280, Avg. loss: 0.213302\n",
      "Total training time: 0.52 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 237.98, NNZs: 600, Bias: -0.131991, T: 79200, Avg. loss: 0.191677\n",
      "Total training time: 0.58 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 238.11, NNZs: 577, Bias: -0.136292, T: 87120, Avg. loss: 0.190195\n",
      "Total training time: 0.63 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 238.23, NNZs: 564, Bias: -0.128675, T: 95040, Avg. loss: 0.185079\n",
      "Total training time: 0.69 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 238.33, NNZs: 552, Bias: -0.132800, T: 102960, Avg. loss: 0.176859\n",
      "Total training time: 0.74 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 238.41, NNZs: 539, Bias: -0.135337, T: 110880, Avg. loss: 0.173169\n",
      "Total training time: 0.80 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 238.51, NNZs: 525, Bias: -0.131839, T: 118800, Avg. loss: 0.166246\n",
      "Total training time: 0.87 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 238.57, NNZs: 521, Bias: -0.131014, T: 126720, Avg. loss: 0.163738\n",
      "Total training time: 0.93 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 238.63, NNZs: 513, Bias: -0.130235, T: 134640, Avg. loss: 0.164334\n",
      "Total training time: 0.99 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 238.69, NNZs: 506, Bias: -0.130311, T: 142560, Avg. loss: 0.159744\n",
      "Total training time: 1.06 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 238.73, NNZs: 504, Bias: -0.129711, T: 150480, Avg. loss: 0.159466\n",
      "Total training time: 1.12 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 238.79, NNZs: 501, Bias: -0.130443, T: 158400, Avg. loss: 0.154018\n",
      "Total training time: 1.19 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 238.84, NNZs: 491, Bias: -0.132265, T: 166320, Avg. loss: 0.150449\n",
      "Total training time: 1.25 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 238.89, NNZs: 487, Bias: -0.131605, T: 174240, Avg. loss: 0.148033\n",
      "Total training time: 1.30 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 238.94, NNZs: 480, Bias: -0.132693, T: 182160, Avg. loss: 0.147647\n",
      "Total training time: 1.36 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 238.97, NNZs: 479, Bias: -0.132167, T: 190080, Avg. loss: 0.146974\n",
      "Total training time: 1.43 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 239.02, NNZs: 472, Bias: -0.131175, T: 198000, Avg. loss: 0.147041\n",
      "Total training time: 1.49 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 239.06, NNZs: 467, Bias: -0.132664, T: 205920, Avg. loss: 0.143353\n",
      "Total training time: 1.55 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 239.10, NNZs: 459, Bias: -0.132234, T: 213840, Avg. loss: 0.143734\n",
      "Total training time: 1.61 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 239.13, NNZs: 458, Bias: -0.134058, T: 221760, Avg. loss: 0.140827\n",
      "Total training time: 1.68 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 239.16, NNZs: 453, Bias: -0.135796, T: 229680, Avg. loss: 0.140299\n",
      "Total training time: 1.74 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 239.19, NNZs: 449, Bias: -0.137849, T: 237600, Avg. loss: 0.137738\n",
      "Total training time: 1.80 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 239.23, NNZs: 445, Bias: -0.134895, T: 245520, Avg. loss: 0.140101\n",
      "Total training time: 1.86 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 239.26, NNZs: 446, Bias: -0.135676, T: 253440, Avg. loss: 0.137815\n",
      "Total training time: 1.92 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 239.29, NNZs: 444, Bias: -0.134890, T: 261360, Avg. loss: 0.138217\n",
      "Total training time: 1.97 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 239.32, NNZs: 442, Bias: -0.133761, T: 269280, Avg. loss: 0.135414\n",
      "Total training time: 2.03 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 239.34, NNZs: 439, Bias: -0.134115, T: 277200, Avg. loss: 0.137132\n",
      "Total training time: 2.08 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 239.37, NNZs: 438, Bias: -0.135843, T: 285120, Avg. loss: 0.136669\n",
      "Total training time: 2.14 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 239.40, NNZs: 438, Bias: -0.134485, T: 293040, Avg. loss: 0.135599\n",
      "Total training time: 2.19 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 239.42, NNZs: 435, Bias: -0.133505, T: 300960, Avg. loss: 0.133495\n",
      "Total training time: 2.24 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 239.45, NNZs: 432, Bias: -0.135475, T: 308880, Avg. loss: 0.131961\n",
      "Total training time: 2.30 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 239.47, NNZs: 430, Bias: -0.135792, T: 316800, Avg. loss: 0.134243\n",
      "Total training time: 2.36 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 239.50, NNZs: 429, Bias: -0.135199, T: 324720, Avg. loss: 0.132169\n",
      "Total training time: 2.41 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 239.52, NNZs: 429, Bias: -0.136409, T: 332640, Avg. loss: 0.131410\n",
      "Total training time: 2.46 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 239.55, NNZs: 429, Bias: -0.136718, T: 340560, Avg. loss: 0.129779\n",
      "Total training time: 2.52 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 239.57, NNZs: 427, Bias: -0.136431, T: 348480, Avg. loss: 0.132655\n",
      "Total training time: 2.58 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 239.59, NNZs: 426, Bias: -0.136164, T: 356400, Avg. loss: 0.130374\n",
      "Total training time: 2.63 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 239.61, NNZs: 425, Bias: -0.137264, T: 364320, Avg. loss: 0.133357\n",
      "Total training time: 2.68 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 239.63, NNZs: 424, Bias: -0.137009, T: 372240, Avg. loss: 0.129799\n",
      "Total training time: 2.74 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 239.65, NNZs: 422, Bias: -0.138058, T: 380160, Avg. loss: 0.131007\n",
      "Total training time: 2.79 seconds.\n",
      "Convergence after 48 epochs took 2.79 seconds\n",
      "-- Epoch 1\n",
      "Norm: 237.57, NNZs: 1637, Bias: -0.071649, T: 7920, Avg. loss: 3.443990\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 252.13, NNZs: 1084, Bias: -0.073979, T: 15840, Avg. loss: 0.549984\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 257.75, NNZs: 894, Bias: -0.049881, T: 23760, Avg. loss: 0.314328\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 260.58, NNZs: 789, Bias: -0.061518, T: 31680, Avg. loss: 0.238021\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 262.65, NNZs: 725, Bias: -0.052156, T: 39600, Avg. loss: 0.173451\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 264.05, NNZs: 685, Bias: -0.045198, T: 47520, Avg. loss: 0.191783\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 265.03, NNZs: 658, Bias: -0.045032, T: 55440, Avg. loss: 0.196876\n",
      "Total training time: 0.46 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 265.87, NNZs: 633, Bias: -0.054818, T: 63360, Avg. loss: 0.162189\n",
      "Total training time: 0.53 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 266.64, NNZs: 610, Bias: -0.047651, T: 71280, Avg. loss: 0.137572\n",
      "Total training time: 0.59 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 267.28, NNZs: 587, Bias: -0.055557, T: 79200, Avg. loss: 0.133682\n",
      "Total training time: 0.66 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 267.81, NNZs: 574, Bias: -0.061000, T: 87120, Avg. loss: 0.130954\n",
      "Total training time: 0.71 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 268.27, NNZs: 554, Bias: -0.048999, T: 95040, Avg. loss: 0.131042\n",
      "Total training time: 0.77 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 268.69, NNZs: 540, Bias: -0.060090, T: 102960, Avg. loss: 0.128575\n",
      "Total training time: 0.82 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 269.07, NNZs: 525, Bias: -0.058077, T: 110880, Avg. loss: 0.126616\n",
      "Total training time: 0.87 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 269.40, NNZs: 521, Bias: -0.059749, T: 118800, Avg. loss: 0.127405\n",
      "Total training time: 0.93 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 269.70, NNZs: 515, Bias: -0.058162, T: 126720, Avg. loss: 0.126881\n",
      "Total training time: 0.98 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 269.98, NNZs: 504, Bias: -0.061223, T: 134640, Avg. loss: 0.126420\n",
      "Total training time: 1.03 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 270.24, NNZs: 494, Bias: -0.060482, T: 142560, Avg. loss: 0.126564\n",
      "Total training time: 1.08 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 270.48, NNZs: 493, Bias: -0.063172, T: 150480, Avg. loss: 0.126117\n",
      "Total training time: 1.13 seconds.\n",
      "Convergence after 19 epochs took 1.13 seconds\n",
      "-- Epoch 1\n",
      "Norm: 246.40, NNZs: 1523, Bias: -0.194461, T: 7920, Avg. loss: 4.040096\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 261.35, NNZs: 1056, Bias: -0.144736, T: 15840, Avg. loss: 0.674839\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 268.21, NNZs: 896, Bias: -0.143811, T: 23760, Avg. loss: 0.402362\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 272.01, NNZs: 795, Bias: -0.145257, T: 31680, Avg. loss: 0.278049\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 274.40, NNZs: 731, Bias: -0.155859, T: 39600, Avg. loss: 0.242403\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 275.92, NNZs: 697, Bias: -0.142788, T: 47520, Avg. loss: 0.244817\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 277.12, NNZs: 660, Bias: -0.147032, T: 55440, Avg. loss: 0.208688\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 278.10, NNZs: 634, Bias: -0.150320, T: 63360, Avg. loss: 0.197104\n",
      "Total training time: 0.52 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 278.85, NNZs: 620, Bias: -0.148865, T: 71280, Avg. loss: 0.198061\n",
      "Total training time: 0.58 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 279.44, NNZs: 604, Bias: -0.156596, T: 79200, Avg. loss: 0.195524\n",
      "Total training time: 0.65 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 279.90, NNZs: 594, Bias: -0.148265, T: 87120, Avg. loss: 0.196003\n",
      "Total training time: 0.71 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 280.31, NNZs: 588, Bias: -0.150674, T: 95040, Avg. loss: 0.184190\n",
      "Total training time: 0.76 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 280.68, NNZs: 582, Bias: -0.152632, T: 102960, Avg. loss: 0.180386\n",
      "Total training time: 0.82 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 281.00, NNZs: 572, Bias: -0.151703, T: 110880, Avg. loss: 0.181287\n",
      "Total training time: 0.87 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 281.26, NNZs: 561, Bias: -0.151838, T: 118800, Avg. loss: 0.184015\n",
      "Total training time: 0.93 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 281.51, NNZs: 557, Bias: -0.154298, T: 126720, Avg. loss: 0.174255\n",
      "Total training time: 0.98 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 281.73, NNZs: 548, Bias: -0.155000, T: 134640, Avg. loss: 0.177207\n",
      "Total training time: 1.03 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 281.94, NNZs: 542, Bias: -0.155692, T: 142560, Avg. loss: 0.176289\n",
      "Total training time: 1.09 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 282.13, NNZs: 537, Bias: -0.156317, T: 150480, Avg. loss: 0.169239\n",
      "Total training time: 1.14 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 282.29, NNZs: 533, Bias: -0.158177, T: 158400, Avg. loss: 0.169847\n",
      "Total training time: 1.19 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 282.44, NNZs: 533, Bias: -0.158143, T: 166320, Avg. loss: 0.172863\n",
      "Total training time: 1.25 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 282.57, NNZs: 533, Bias: -0.158658, T: 174240, Avg. loss: 0.168712\n",
      "Total training time: 1.30 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 282.70, NNZs: 531, Bias: -0.158059, T: 182160, Avg. loss: 0.165106\n",
      "Total training time: 1.36 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 282.82, NNZs: 533, Bias: -0.160204, T: 190080, Avg. loss: 0.171105\n",
      "Total training time: 1.41 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 282.93, NNZs: 525, Bias: -0.158158, T: 198000, Avg. loss: 0.165837\n",
      "Total training time: 1.47 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 283.05, NNZs: 523, Bias: -0.160090, T: 205920, Avg. loss: 0.158975\n",
      "Total training time: 1.52 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 283.15, NNZs: 520, Bias: -0.159148, T: 213840, Avg. loss: 0.162459\n",
      "Total training time: 1.57 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 283.25, NNZs: 517, Bias: -0.162299, T: 221760, Avg. loss: 0.155584\n",
      "Total training time: 1.62 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 283.35, NNZs: 516, Bias: -0.160088, T: 229680, Avg. loss: 0.162920\n",
      "Total training time: 1.67 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 283.43, NNZs: 515, Bias: -0.160561, T: 237600, Avg. loss: 0.159806\n",
      "Total training time: 1.73 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 283.52, NNZs: 512, Bias: -0.161790, T: 245520, Avg. loss: 0.153852\n",
      "Total training time: 1.78 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 283.60, NNZs: 512, Bias: -0.162592, T: 253440, Avg. loss: 0.158524\n",
      "Total training time: 1.83 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 283.67, NNZs: 512, Bias: -0.165272, T: 261360, Avg. loss: 0.157929\n",
      "Total training time: 1.88 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 283.74, NNZs: 509, Bias: -0.162657, T: 269280, Avg. loss: 0.161147\n",
      "Total training time: 1.93 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 283.82, NNZs: 508, Bias: -0.164115, T: 277200, Avg. loss: 0.155165\n",
      "Total training time: 1.98 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 283.88, NNZs: 507, Bias: -0.165554, T: 285120, Avg. loss: 0.157081\n",
      "Total training time: 2.03 seconds.\n",
      "Convergence after 36 epochs took 2.03 seconds\n",
      "-- Epoch 1\n",
      "Norm: 259.19, NNZs: 1974, Bias: -0.007739, T: 7920, Avg. loss: 3.342782\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 269.79, NNZs: 1338, Bias: 0.026868, T: 15840, Avg. loss: 0.564588\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 273.60, NNZs: 1103, Bias: 0.036628, T: 23760, Avg. loss: 0.360134\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 275.41, NNZs: 968, Bias: 0.038624, T: 31680, Avg. loss: 0.261200\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 276.39, NNZs: 869, Bias: 0.038341, T: 39600, Avg. loss: 0.234557\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 276.99, NNZs: 807, Bias: 0.038373, T: 47520, Avg. loss: 0.208488\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 277.52, NNZs: 757, Bias: 0.041979, T: 55440, Avg. loss: 0.187900\n",
      "Total training time: 0.42 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 277.92, NNZs: 720, Bias: 0.043583, T: 63360, Avg. loss: 0.188066\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 278.15, NNZs: 690, Bias: 0.045070, T: 71280, Avg. loss: 0.183157\n",
      "Total training time: 0.52 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 278.36, NNZs: 667, Bias: 0.043546, T: 79200, Avg. loss: 0.164735\n",
      "Total training time: 0.57 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 278.52, NNZs: 652, Bias: 0.038043, T: 87120, Avg. loss: 0.165215\n",
      "Total training time: 0.62 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 278.67, NNZs: 633, Bias: 0.041526, T: 95040, Avg. loss: 0.156786\n",
      "Total training time: 0.67 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 278.79, NNZs: 620, Bias: 0.045445, T: 102960, Avg. loss: 0.150160\n",
      "Total training time: 0.73 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 278.91, NNZs: 607, Bias: 0.039904, T: 110880, Avg. loss: 0.144735\n",
      "Total training time: 0.78 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 279.01, NNZs: 593, Bias: 0.045046, T: 118800, Avg. loss: 0.137342\n",
      "Total training time: 0.83 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 279.10, NNZs: 578, Bias: 0.045005, T: 126720, Avg. loss: 0.140353\n",
      "Total training time: 0.88 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 279.18, NNZs: 571, Bias: 0.046500, T: 134640, Avg. loss: 0.137740\n",
      "Total training time: 0.94 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 279.24, NNZs: 563, Bias: 0.045899, T: 142560, Avg. loss: 0.139808\n",
      "Total training time: 0.99 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 279.31, NNZs: 554, Bias: 0.046562, T: 150480, Avg. loss: 0.130309\n",
      "Total training time: 1.04 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 279.37, NNZs: 540, Bias: 0.049115, T: 158400, Avg. loss: 0.127874\n",
      "Total training time: 1.09 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 279.45, NNZs: 531, Bias: 0.047793, T: 166320, Avg. loss: 0.123651\n",
      "Total training time: 1.14 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 279.50, NNZs: 524, Bias: 0.047818, T: 174240, Avg. loss: 0.129878\n",
      "Total training time: 1.19 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 279.56, NNZs: 517, Bias: 0.048980, T: 182160, Avg. loss: 0.126759\n",
      "Total training time: 1.24 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 279.61, NNZs: 511, Bias: 0.050061, T: 190080, Avg. loss: 0.125810\n",
      "Total training time: 1.29 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 279.66, NNZs: 501, Bias: 0.047528, T: 198000, Avg. loss: 0.121687\n",
      "Total training time: 1.34 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 279.70, NNZs: 495, Bias: 0.049053, T: 205920, Avg. loss: 0.122663\n",
      "Total training time: 1.39 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 279.75, NNZs: 494, Bias: 0.050936, T: 213840, Avg. loss: 0.120834\n",
      "Total training time: 1.44 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 279.80, NNZs: 483, Bias: 0.050438, T: 221760, Avg. loss: 0.118483\n",
      "Total training time: 1.49 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 279.84, NNZs: 479, Bias: 0.049570, T: 229680, Avg. loss: 0.117647\n",
      "Total training time: 1.54 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 279.88, NNZs: 475, Bias: 0.050448, T: 237600, Avg. loss: 0.115414\n",
      "Total training time: 1.59 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 279.92, NNZs: 473, Bias: 0.052093, T: 245520, Avg. loss: 0.117205\n",
      "Total training time: 1.64 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 279.96, NNZs: 471, Bias: 0.051686, T: 253440, Avg. loss: 0.116594\n",
      "Total training time: 1.69 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 280.00, NNZs: 468, Bias: 0.050504, T: 261360, Avg. loss: 0.109686\n",
      "Total training time: 1.74 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 280.04, NNZs: 466, Bias: 0.050518, T: 269280, Avg. loss: 0.111609\n",
      "Total training time: 1.79 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 280.08, NNZs: 458, Bias: 0.050901, T: 277200, Avg. loss: 0.112553\n",
      "Total training time: 1.84 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 280.11, NNZs: 453, Bias: 0.053708, T: 285120, Avg. loss: 0.107915\n",
      "Total training time: 1.89 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 280.15, NNZs: 453, Bias: 0.051276, T: 293040, Avg. loss: 0.108296\n",
      "Total training time: 1.94 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 280.19, NNZs: 449, Bias: 0.050625, T: 300960, Avg. loss: 0.107531\n",
      "Total training time: 1.99 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 280.23, NNZs: 447, Bias: 0.051314, T: 308880, Avg. loss: 0.111745\n",
      "Total training time: 2.04 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 280.26, NNZs: 445, Bias: 0.051004, T: 316800, Avg. loss: 0.106973\n",
      "Total training time: 2.09 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 280.29, NNZs: 442, Bias: 0.051335, T: 324720, Avg. loss: 0.106348\n",
      "Total training time: 2.14 seconds.\n",
      "Convergence after 41 epochs took 2.14 seconds\n",
      "-- Epoch 1\n",
      "Norm: 245.30, NNZs: 1542, Bias: -0.093722, T: 7920, Avg. loss: 3.827576\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 261.45, NNZs: 1108, Bias: -0.034388, T: 15840, Avg. loss: 0.697881\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 268.59, NNZs: 942, Bias: -0.037639, T: 23760, Avg. loss: 0.272724\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 272.40, NNZs: 855, Bias: -0.036758, T: 31680, Avg. loss: 0.217659\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 274.81, NNZs: 793, Bias: -0.018216, T: 39600, Avg. loss: 0.182723\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 276.57, NNZs: 729, Bias: -0.021289, T: 47520, Avg. loss: 0.153033\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 277.90, NNZs: 706, Bias: -0.017605, T: 55440, Avg. loss: 0.141190\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 278.97, NNZs: 688, Bias: -0.013095, T: 63360, Avg. loss: 0.134445\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 279.84, NNZs: 664, Bias: -0.018993, T: 71280, Avg. loss: 0.128294\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 280.56, NNZs: 655, Bias: -0.014786, T: 79200, Avg. loss: 0.133760\n",
      "Total training time: 0.55 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 281.18, NNZs: 644, Bias: -0.014856, T: 87120, Avg. loss: 0.122255\n",
      "Total training time: 0.60 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 281.70, NNZs: 625, Bias: -0.012833, T: 95040, Avg. loss: 0.130551\n",
      "Total training time: 0.65 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 282.19, NNZs: 610, Bias: -0.013846, T: 102960, Avg. loss: 0.119374\n",
      "Total training time: 0.70 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 282.62, NNZs: 605, Bias: -0.010219, T: 110880, Avg. loss: 0.118389\n",
      "Total training time: 0.75 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 283.01, NNZs: 601, Bias: -0.010385, T: 118800, Avg. loss: 0.117545\n",
      "Total training time: 0.81 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 283.36, NNZs: 596, Bias: -0.012080, T: 126720, Avg. loss: 0.117376\n",
      "Total training time: 0.86 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 283.67, NNZs: 595, Bias: -0.011362, T: 134640, Avg. loss: 0.116297\n",
      "Total training time: 0.91 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 283.96, NNZs: 589, Bias: -0.014205, T: 142560, Avg. loss: 0.116433\n",
      "Total training time: 0.96 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 284.22, NNZs: 586, Bias: -0.010758, T: 150480, Avg. loss: 0.118139\n",
      "Total training time: 1.01 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 284.46, NNZs: 581, Bias: -0.008281, T: 158400, Avg. loss: 0.116294\n",
      "Total training time: 1.06 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 284.68, NNZs: 576, Bias: -0.009601, T: 166320, Avg. loss: 0.121187\n",
      "Total training time: 1.12 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 284.87, NNZs: 573, Bias: -0.009642, T: 174240, Avg. loss: 0.128193\n",
      "Total training time: 1.17 seconds.\n",
      "Convergence after 22 epochs took 1.17 seconds\n",
      "-- Epoch 1\n",
      "Norm: 247.32, NNZs: 1766, Bias: -0.142641, T: 7920, Avg. loss: 3.503035\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 265.06, NNZs: 1324, Bias: -0.106701, T: 15840, Avg. loss: 0.581569\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 270.93, NNZs: 1136, Bias: -0.096340, T: 23760, Avg. loss: 0.340726\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 273.89, NNZs: 1037, Bias: -0.089557, T: 31680, Avg. loss: 0.282812\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 275.55, NNZs: 973, Bias: -0.096487, T: 39600, Avg. loss: 0.238816\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 276.64, NNZs: 914, Bias: -0.078297, T: 47520, Avg. loss: 0.234642\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 277.46, NNZs: 879, Bias: -0.082325, T: 55440, Avg. loss: 0.207365\n",
      "Total training time: 0.43 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 278.09, NNZs: 849, Bias: -0.081894, T: 63360, Avg. loss: 0.186054\n",
      "Total training time: 0.48 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 278.59, NNZs: 822, Bias: -0.076145, T: 71280, Avg. loss: 0.188240\n",
      "Total training time: 0.54 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 279.02, NNZs: 792, Bias: -0.079814, T: 79200, Avg. loss: 0.163600\n",
      "Total training time: 0.59 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 279.37, NNZs: 771, Bias: -0.080696, T: 87120, Avg. loss: 0.171754\n",
      "Total training time: 0.64 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 279.68, NNZs: 758, Bias: -0.077164, T: 95040, Avg. loss: 0.161946\n",
      "Total training time: 0.70 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 279.93, NNZs: 742, Bias: -0.072238, T: 102960, Avg. loss: 0.161165\n",
      "Total training time: 0.75 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 280.15, NNZs: 724, Bias: -0.072339, T: 110880, Avg. loss: 0.152323\n",
      "Total training time: 0.81 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 280.36, NNZs: 713, Bias: -0.073274, T: 118800, Avg. loss: 0.152557\n",
      "Total training time: 0.87 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 280.55, NNZs: 706, Bias: -0.073299, T: 126720, Avg. loss: 0.150859\n",
      "Total training time: 0.93 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 280.72, NNZs: 694, Bias: -0.075555, T: 134640, Avg. loss: 0.154112\n",
      "Total training time: 0.98 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 280.87, NNZs: 691, Bias: -0.072588, T: 142560, Avg. loss: 0.142416\n",
      "Total training time: 1.03 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 281.01, NNZs: 682, Bias: -0.071922, T: 150480, Avg. loss: 0.144105\n",
      "Total training time: 1.08 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 281.14, NNZs: 676, Bias: -0.070065, T: 158400, Avg. loss: 0.140971\n",
      "Total training time: 1.13 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 281.26, NNZs: 670, Bias: -0.072567, T: 166320, Avg. loss: 0.141786\n",
      "Total training time: 1.18 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 281.38, NNZs: 657, Bias: -0.073119, T: 174240, Avg. loss: 0.137009\n",
      "Total training time: 1.23 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 281.49, NNZs: 652, Bias: -0.073106, T: 182160, Avg. loss: 0.134584\n",
      "Total training time: 1.28 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 281.60, NNZs: 647, Bias: -0.072069, T: 190080, Avg. loss: 0.137853\n",
      "Total training time: 1.33 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 281.70, NNZs: 645, Bias: -0.074120, T: 198000, Avg. loss: 0.136114\n",
      "Total training time: 1.38 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 281.78, NNZs: 637, Bias: -0.072617, T: 205920, Avg. loss: 0.134862\n",
      "Total training time: 1.44 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 281.87, NNZs: 632, Bias: -0.074033, T: 213840, Avg. loss: 0.134246\n",
      "Total training time: 1.49 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 281.95, NNZs: 623, Bias: -0.073108, T: 221760, Avg. loss: 0.136802\n",
      "Total training time: 1.54 seconds.\n",
      "Convergence after 28 epochs took 1.54 seconds\n",
      "-- Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  11 out of  11 | elapsed:   22.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 209.39, NNZs: 1379, Bias: -0.013709, T: 7920, Avg. loss: 3.284280\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 220.86, NNZs: 1070, Bias: -0.047656, T: 15840, Avg. loss: 0.786801\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 224.17, NNZs: 905, Bias: -0.042786, T: 23760, Avg. loss: 0.520838\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 225.56, NNZs: 822, Bias: -0.045202, T: 31680, Avg. loss: 0.405723\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 226.34, NNZs: 747, Bias: -0.035437, T: 39600, Avg. loss: 0.350419\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 226.79, NNZs: 707, Bias: -0.036293, T: 47520, Avg. loss: 0.313664\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 227.09, NNZs: 678, Bias: -0.033269, T: 55440, Avg. loss: 0.280052\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 227.31, NNZs: 655, Bias: -0.039897, T: 63360, Avg. loss: 0.259302\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 227.44, NNZs: 632, Bias: -0.045324, T: 71280, Avg. loss: 0.247370\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 227.55, NNZs: 611, Bias: -0.041528, T: 79200, Avg. loss: 0.237328\n",
      "Total training time: 0.55 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 227.63, NNZs: 591, Bias: -0.041438, T: 87120, Avg. loss: 0.227099\n",
      "Total training time: 0.60 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 227.73, NNZs: 583, Bias: -0.038325, T: 95040, Avg. loss: 0.213456\n",
      "Total training time: 0.65 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 227.78, NNZs: 564, Bias: -0.041336, T: 102960, Avg. loss: 0.211865\n",
      "Total training time: 0.70 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 227.82, NNZs: 556, Bias: -0.035832, T: 110880, Avg. loss: 0.204271\n",
      "Total training time: 0.75 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 227.85, NNZs: 544, Bias: -0.039322, T: 118800, Avg. loss: 0.202649\n",
      "Total training time: 0.80 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 227.89, NNZs: 536, Bias: -0.036952, T: 126720, Avg. loss: 0.194795\n",
      "Total training time: 0.85 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 227.92, NNZs: 523, Bias: -0.040692, T: 134640, Avg. loss: 0.190956\n",
      "Total training time: 0.91 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 227.95, NNZs: 519, Bias: -0.034975, T: 142560, Avg. loss: 0.188374\n",
      "Total training time: 0.95 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 227.98, NNZs: 512, Bias: -0.037654, T: 150480, Avg. loss: 0.185000\n",
      "Total training time: 1.00 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 228.00, NNZs: 507, Bias: -0.035165, T: 158400, Avg. loss: 0.184716\n",
      "Total training time: 1.06 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 228.02, NNZs: 505, Bias: -0.033399, T: 166320, Avg. loss: 0.179488\n",
      "Total training time: 1.10 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 228.05, NNZs: 502, Bias: -0.034661, T: 174240, Avg. loss: 0.177931\n",
      "Total training time: 1.16 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 228.07, NNZs: 500, Bias: -0.034173, T: 182160, Avg. loss: 0.174061\n",
      "Total training time: 1.21 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 228.09, NNZs: 497, Bias: -0.037376, T: 190080, Avg. loss: 0.173197\n",
      "Total training time: 1.26 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 228.11, NNZs: 495, Bias: -0.037342, T: 198000, Avg. loss: 0.168776\n",
      "Total training time: 1.31 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 228.13, NNZs: 490, Bias: -0.036312, T: 205920, Avg. loss: 0.170389\n",
      "Total training time: 1.36 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 228.15, NNZs: 486, Bias: -0.035823, T: 213840, Avg. loss: 0.165967\n",
      "Total training time: 1.41 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 228.16, NNZs: 483, Bias: -0.034477, T: 221760, Avg. loss: 0.167005\n",
      "Total training time: 1.46 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 228.17, NNZs: 482, Bias: -0.037099, T: 229680, Avg. loss: 0.166210\n",
      "Total training time: 1.52 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 228.19, NNZs: 478, Bias: -0.036223, T: 237600, Avg. loss: 0.164000\n",
      "Total training time: 1.56 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 228.20, NNZs: 480, Bias: -0.035339, T: 245520, Avg. loss: 0.165703\n",
      "Total training time: 1.62 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 228.21, NNZs: 479, Bias: -0.036099, T: 253440, Avg. loss: 0.158430\n",
      "Total training time: 1.67 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 228.23, NNZs: 477, Bias: -0.034185, T: 261360, Avg. loss: 0.158677\n",
      "Total training time: 1.71 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 228.24, NNZs: 476, Bias: -0.034505, T: 269280, Avg. loss: 0.159216\n",
      "Total training time: 1.77 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 228.26, NNZs: 472, Bias: -0.034495, T: 277200, Avg. loss: 0.158816\n",
      "Total training time: 1.81 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 228.27, NNZs: 469, Bias: -0.034490, T: 285120, Avg. loss: 0.157015\n",
      "Total training time: 1.86 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 228.28, NNZs: 468, Bias: -0.033801, T: 293040, Avg. loss: 0.156194\n",
      "Total training time: 1.91 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 228.29, NNZs: 465, Bias: -0.035497, T: 300960, Avg. loss: 0.156175\n",
      "Total training time: 1.96 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 228.31, NNZs: 465, Bias: -0.034171, T: 308880, Avg. loss: 0.154473\n",
      "Total training time: 2.01 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 228.32, NNZs: 463, Bias: -0.033536, T: 316800, Avg. loss: 0.153155\n",
      "Total training time: 2.06 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 228.33, NNZs: 458, Bias: -0.033241, T: 324720, Avg. loss: 0.154156\n",
      "Total training time: 2.11 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 228.34, NNZs: 456, Bias: -0.033539, T: 332640, Avg. loss: 0.152804\n",
      "Total training time: 2.16 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 228.35, NNZs: 451, Bias: -0.034109, T: 340560, Avg. loss: 0.153550\n",
      "Total training time: 2.21 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 228.37, NNZs: 450, Bias: -0.033242, T: 348480, Avg. loss: 0.151556\n",
      "Total training time: 2.26 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 228.38, NNZs: 447, Bias: -0.032398, T: 356400, Avg. loss: 0.153769\n",
      "Total training time: 2.31 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 228.39, NNZs: 446, Bias: -0.032945, T: 364320, Avg. loss: 0.150940\n",
      "Total training time: 2.36 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 228.40, NNZs: 443, Bias: -0.033759, T: 372240, Avg. loss: 0.150597\n",
      "Total training time: 2.41 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 228.41, NNZs: 442, Bias: -0.032957, T: 380160, Avg. loss: 0.149551\n",
      "Total training time: 2.46 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 228.42, NNZs: 443, Bias: -0.032964, T: 388080, Avg. loss: 0.150452\n",
      "Total training time: 2.51 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 228.44, NNZs: 441, Bias: -0.031945, T: 396000, Avg. loss: 0.150249\n",
      "Total training time: 2.56 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 228.45, NNZs: 438, Bias: -0.033447, T: 403920, Avg. loss: 0.149031\n",
      "Total training time: 2.61 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 228.46, NNZs: 437, Bias: -0.033202, T: 411840, Avg. loss: 0.147141\n",
      "Total training time: 2.66 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 228.47, NNZs: 435, Bias: -0.032014, T: 419760, Avg. loss: 0.146460\n",
      "Total training time: 2.71 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 228.48, NNZs: 434, Bias: -0.032502, T: 427680, Avg. loss: 0.148552\n",
      "Total training time: 2.76 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 228.49, NNZs: 433, Bias: -0.032973, T: 435600, Avg. loss: 0.146536\n",
      "Total training time: 2.81 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 228.50, NNZs: 433, Bias: -0.033636, T: 443520, Avg. loss: 0.145254\n",
      "Total training time: 2.86 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 228.51, NNZs: 431, Bias: -0.032736, T: 451440, Avg. loss: 0.147342\n",
      "Total training time: 2.91 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 228.53, NNZs: 430, Bias: -0.032741, T: 459360, Avg. loss: 0.143983\n",
      "Total training time: 2.96 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 228.54, NNZs: 430, Bias: -0.032302, T: 467280, Avg. loss: 0.145202\n",
      "Total training time: 3.01 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 228.55, NNZs: 429, Bias: -0.032309, T: 475200, Avg. loss: 0.144531\n",
      "Total training time: 3.06 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 228.56, NNZs: 429, Bias: -0.032316, T: 483120, Avg. loss: 0.143416\n",
      "Total training time: 3.11 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 228.57, NNZs: 427, Bias: -0.032309, T: 491040, Avg. loss: 0.141510\n",
      "Total training time: 3.16 seconds.\n",
      "-- Epoch 63\n",
      "Norm: 228.58, NNZs: 426, Bias: -0.032103, T: 498960, Avg. loss: 0.142928\n",
      "Total training time: 3.21 seconds.\n",
      "-- Epoch 64\n",
      "Norm: 228.59, NNZs: 426, Bias: -0.032096, T: 506880, Avg. loss: 0.143740\n",
      "Total training time: 3.26 seconds.\n",
      "-- Epoch 65\n",
      "Norm: 228.60, NNZs: 426, Bias: -0.031898, T: 514800, Avg. loss: 0.142749\n",
      "Total training time: 3.31 seconds.\n",
      "-- Epoch 66\n",
      "Norm: 228.61, NNZs: 424, Bias: -0.031712, T: 522720, Avg. loss: 0.142774\n",
      "Total training time: 3.36 seconds.\n",
      "-- Epoch 67\n",
      "Norm: 228.62, NNZs: 421, Bias: -0.031715, T: 530640, Avg. loss: 0.143424\n",
      "Total training time: 3.41 seconds.\n",
      "Convergence after 67 epochs took 3.41 seconds\n",
      "-- Epoch 1\n",
      "Norm: 241.88, NNZs: 1782, Bias: -0.258805, T: 7920, Avg. loss: 4.130410\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 259.10, NNZs: 1356, Bias: -0.258585, T: 15840, Avg. loss: 0.642060\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 264.47, NNZs: 1154, Bias: -0.218273, T: 23760, Avg. loss: 0.329175\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 267.03, NNZs: 999, Bias: -0.210082, T: 31680, Avg. loss: 0.238205\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 268.46, NNZs: 915, Bias: -0.203275, T: 39600, Avg. loss: 0.188116\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 269.52, NNZs: 843, Bias: -0.201628, T: 47520, Avg. loss: 0.156384\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 270.27, NNZs: 795, Bias: -0.207324, T: 55440, Avg. loss: 0.150414\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 270.85, NNZs: 753, Bias: -0.200562, T: 63360, Avg. loss: 0.141894\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 271.33, NNZs: 722, Bias: -0.196363, T: 71280, Avg. loss: 0.129763\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 271.74, NNZs: 700, Bias: -0.206630, T: 79200, Avg. loss: 0.128042\n",
      "Total training time: 0.55 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 272.08, NNZs: 673, Bias: -0.197895, T: 87120, Avg. loss: 0.121435\n",
      "Total training time: 0.60 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 272.36, NNZs: 664, Bias: -0.196862, T: 95040, Avg. loss: 0.123863\n",
      "Total training time: 0.65 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 272.60, NNZs: 644, Bias: -0.200756, T: 102960, Avg. loss: 0.124139\n",
      "Total training time: 0.70 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 272.79, NNZs: 634, Bias: -0.196177, T: 110880, Avg. loss: 0.125874\n",
      "Total training time: 0.75 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 272.98, NNZs: 615, Bias: -0.197021, T: 118800, Avg. loss: 0.116089\n",
      "Total training time: 0.81 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 273.14, NNZs: 603, Bias: -0.195560, T: 126720, Avg. loss: 0.128494\n",
      "Total training time: 0.86 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 273.30, NNZs: 589, Bias: -0.194845, T: 134640, Avg. loss: 0.114528\n",
      "Total training time: 0.91 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 273.44, NNZs: 581, Bias: -0.197051, T: 142560, Avg. loss: 0.119753\n",
      "Total training time: 0.96 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 273.56, NNZs: 574, Bias: -0.197690, T: 150480, Avg. loss: 0.114040\n",
      "Total training time: 1.01 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 273.69, NNZs: 568, Bias: -0.197646, T: 158400, Avg. loss: 0.111450\n",
      "Total training time: 1.06 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 273.81, NNZs: 562, Bias: -0.201796, T: 166320, Avg. loss: 0.112697\n",
      "Total training time: 1.11 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 273.91, NNZs: 556, Bias: -0.198744, T: 174240, Avg. loss: 0.113186\n",
      "Total training time: 1.16 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 274.01, NNZs: 548, Bias: -0.199291, T: 182160, Avg. loss: 0.115964\n",
      "Total training time: 1.21 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 274.10, NNZs: 546, Bias: -0.199786, T: 190080, Avg. loss: 0.113162\n",
      "Total training time: 1.27 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 274.19, NNZs: 544, Bias: -0.198741, T: 198000, Avg. loss: 0.112583\n",
      "Total training time: 1.32 seconds.\n",
      "Convergence after 25 epochs took 1.32 seconds\n",
      "-- Epoch 1\n",
      "Norm: 242.18, NNZs: 1449, Bias: -0.148608, T: 7920, Avg. loss: 3.623164\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 257.66, NNZs: 1091, Bias: -0.177904, T: 15840, Avg. loss: 0.759261\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 262.58, NNZs: 959, Bias: -0.169828, T: 23760, Avg. loss: 0.447441\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 265.01, NNZs: 880, Bias: -0.187040, T: 31680, Avg. loss: 0.365663\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 266.44, NNZs: 822, Bias: -0.190625, T: 39600, Avg. loss: 0.312395\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 267.49, NNZs: 775, Bias: -0.190726, T: 47520, Avg. loss: 0.266046\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 268.09, NNZs: 739, Bias: -0.185331, T: 55440, Avg. loss: 0.240311\n",
      "Total training time: 0.43 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 268.63, NNZs: 724, Bias: -0.200137, T: 63360, Avg. loss: 0.220987\n",
      "Total training time: 0.48 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 269.03, NNZs: 697, Bias: -0.197142, T: 71280, Avg. loss: 0.203928\n",
      "Total training time: 0.53 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 269.34, NNZs: 672, Bias: -0.202417, T: 79200, Avg. loss: 0.201630\n",
      "Total training time: 0.58 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 269.60, NNZs: 656, Bias: -0.204873, T: 87120, Avg. loss: 0.188847\n",
      "Total training time: 0.63 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 269.82, NNZs: 642, Bias: -0.203654, T: 95040, Avg. loss: 0.182442\n",
      "Total training time: 0.68 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 270.02, NNZs: 627, Bias: -0.203762, T: 102960, Avg. loss: 0.177834\n",
      "Total training time: 0.73 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 270.17, NNZs: 615, Bias: -0.210273, T: 110880, Avg. loss: 0.176451\n",
      "Total training time: 0.78 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 270.31, NNZs: 607, Bias: -0.210156, T: 118800, Avg. loss: 0.167936\n",
      "Total training time: 0.83 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 270.44, NNZs: 597, Bias: -0.213331, T: 126720, Avg. loss: 0.163112\n",
      "Total training time: 0.88 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 270.55, NNZs: 580, Bias: -0.210328, T: 134640, Avg. loss: 0.159321\n",
      "Total training time: 0.94 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 270.64, NNZs: 570, Bias: -0.215226, T: 142560, Avg. loss: 0.161765\n",
      "Total training time: 0.99 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 270.73, NNZs: 563, Bias: -0.213829, T: 150480, Avg. loss: 0.157265\n",
      "Total training time: 1.04 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 270.82, NNZs: 556, Bias: -0.213941, T: 158400, Avg. loss: 0.156099\n",
      "Total training time: 1.09 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 270.90, NNZs: 544, Bias: -0.215756, T: 166320, Avg. loss: 0.154930\n",
      "Total training time: 1.14 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 270.97, NNZs: 535, Bias: -0.217019, T: 174240, Avg. loss: 0.154252\n",
      "Total training time: 1.19 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 271.04, NNZs: 525, Bias: -0.217571, T: 182160, Avg. loss: 0.150781\n",
      "Total training time: 1.24 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 271.10, NNZs: 520, Bias: -0.215979, T: 190080, Avg. loss: 0.147469\n",
      "Total training time: 1.29 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 271.16, NNZs: 519, Bias: -0.218514, T: 198000, Avg. loss: 0.148043\n",
      "Total training time: 1.34 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 271.22, NNZs: 512, Bias: -0.219062, T: 205920, Avg. loss: 0.144637\n",
      "Total training time: 1.39 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 271.27, NNZs: 513, Bias: -0.223327, T: 213840, Avg. loss: 0.145339\n",
      "Total training time: 1.43 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 271.32, NNZs: 508, Bias: -0.222380, T: 221760, Avg. loss: 0.140822\n",
      "Total training time: 1.48 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 271.37, NNZs: 504, Bias: -0.223267, T: 229680, Avg. loss: 0.139714\n",
      "Total training time: 1.53 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 271.41, NNZs: 503, Bias: -0.223287, T: 237600, Avg. loss: 0.141355\n",
      "Total training time: 1.58 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 271.44, NNZs: 502, Bias: -0.224482, T: 245520, Avg. loss: 0.140586\n",
      "Total training time: 1.63 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 271.48, NNZs: 500, Bias: -0.224873, T: 253440, Avg. loss: 0.138837\n",
      "Total training time: 1.67 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 271.51, NNZs: 501, Bias: -0.227955, T: 261360, Avg. loss: 0.137361\n",
      "Total training time: 1.72 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 271.55, NNZs: 496, Bias: -0.226474, T: 269280, Avg. loss: 0.138316\n",
      "Total training time: 1.77 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 271.58, NNZs: 491, Bias: -0.227562, T: 277200, Avg. loss: 0.134698\n",
      "Total training time: 1.82 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 271.62, NNZs: 487, Bias: -0.227924, T: 285120, Avg. loss: 0.135821\n",
      "Total training time: 1.87 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 271.65, NNZs: 481, Bias: -0.229642, T: 293040, Avg. loss: 0.134080\n",
      "Total training time: 1.92 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 271.68, NNZs: 478, Bias: -0.230648, T: 300960, Avg. loss: 0.135086\n",
      "Total training time: 1.97 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 271.71, NNZs: 476, Bias: -0.229346, T: 308880, Avg. loss: 0.136425\n",
      "Total training time: 2.02 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 271.75, NNZs: 474, Bias: -0.229066, T: 316800, Avg. loss: 0.132219\n",
      "Total training time: 2.06 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 271.78, NNZs: 472, Bias: -0.233089, T: 324720, Avg. loss: 0.130983\n",
      "Total training time: 2.11 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 271.81, NNZs: 470, Bias: -0.231261, T: 332640, Avg. loss: 0.132970\n",
      "Total training time: 2.16 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 271.83, NNZs: 470, Bias: -0.230965, T: 340560, Avg. loss: 0.132899\n",
      "Total training time: 2.21 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 271.85, NNZs: 469, Bias: -0.231547, T: 348480, Avg. loss: 0.132341\n",
      "Total training time: 2.26 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 271.88, NNZs: 466, Bias: -0.233234, T: 356400, Avg. loss: 0.130203\n",
      "Total training time: 2.31 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 271.91, NNZs: 463, Bias: -0.233249, T: 364320, Avg. loss: 0.130987\n",
      "Total training time: 2.36 seconds.\n",
      "Convergence after 46 epochs took 2.36 seconds\n",
      "-- Epoch 1\n",
      "Norm: 234.73, NNZs: 1896, Bias: -0.262348, T: 7920, Avg. loss: 4.574868\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 244.98, NNZs: 1326, Bias: -0.128470, T: 15840, Avg. loss: 0.979720\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 248.87, NNZs: 1093, Bias: -0.057181, T: 23760, Avg. loss: 0.555585\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 251.27, NNZs: 954, Bias: -0.064894, T: 31680, Avg. loss: 0.361015\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 252.46, NNZs: 870, Bias: -0.055195, T: 39600, Avg. loss: 0.337030\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 253.38, NNZs: 811, Bias: -0.064264, T: 47520, Avg. loss: 0.281513\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 254.02, NNZs: 759, Bias: -0.060107, T: 55440, Avg. loss: 0.255456\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 254.49, NNZs: 721, Bias: -0.066531, T: 63360, Avg. loss: 0.243174\n",
      "Total training time: 0.46 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 254.88, NNZs: 684, Bias: -0.062170, T: 71280, Avg. loss: 0.233239\n",
      "Total training time: 0.51 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 255.17, NNZs: 651, Bias: -0.067814, T: 79200, Avg. loss: 0.230638\n",
      "Total training time: 0.56 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 255.43, NNZs: 629, Bias: -0.071481, T: 87120, Avg. loss: 0.222158\n",
      "Total training time: 0.61 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 255.64, NNZs: 624, Bias: -0.074762, T: 95040, Avg. loss: 0.214180\n",
      "Total training time: 0.65 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 255.82, NNZs: 615, Bias: -0.074843, T: 102960, Avg. loss: 0.212671\n",
      "Total training time: 0.71 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 255.97, NNZs: 597, Bias: -0.081287, T: 110880, Avg. loss: 0.204389\n",
      "Total training time: 0.76 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 256.07, NNZs: 585, Bias: -0.082191, T: 118800, Avg. loss: 0.209195\n",
      "Total training time: 0.81 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 256.19, NNZs: 579, Bias: -0.082113, T: 126720, Avg. loss: 0.193228\n",
      "Total training time: 0.86 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 256.30, NNZs: 567, Bias: -0.079957, T: 134640, Avg. loss: 0.188243\n",
      "Total training time: 0.91 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 256.40, NNZs: 554, Bias: -0.083596, T: 142560, Avg. loss: 0.183380\n",
      "Total training time: 0.96 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 256.51, NNZs: 549, Bias: -0.087500, T: 150480, Avg. loss: 0.176761\n",
      "Total training time: 1.01 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 256.60, NNZs: 535, Bias: -0.085584, T: 158400, Avg. loss: 0.180336\n",
      "Total training time: 1.06 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 256.67, NNZs: 531, Bias: -0.085590, T: 166320, Avg. loss: 0.178054\n",
      "Total training time: 1.10 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 256.75, NNZs: 522, Bias: -0.088441, T: 174240, Avg. loss: 0.174372\n",
      "Total training time: 1.15 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 256.82, NNZs: 513, Bias: -0.088421, T: 182160, Avg. loss: 0.177152\n",
      "Total training time: 1.20 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 256.88, NNZs: 504, Bias: -0.090544, T: 190080, Avg. loss: 0.168966\n",
      "Total training time: 1.25 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 256.94, NNZs: 500, Bias: -0.090088, T: 198000, Avg. loss: 0.168929\n",
      "Total training time: 1.30 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 257.00, NNZs: 492, Bias: -0.092516, T: 205920, Avg. loss: 0.168282\n",
      "Total training time: 1.35 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 257.05, NNZs: 486, Bias: -0.092983, T: 213840, Avg. loss: 0.170402\n",
      "Total training time: 1.40 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 257.10, NNZs: 478, Bias: -0.095269, T: 221760, Avg. loss: 0.168617\n",
      "Total training time: 1.45 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 257.15, NNZs: 469, Bias: -0.095280, T: 229680, Avg. loss: 0.165154\n",
      "Total training time: 1.50 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 257.19, NNZs: 462, Bias: -0.095719, T: 237600, Avg. loss: 0.168779\n",
      "Total training time: 1.55 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 257.23, NNZs: 461, Bias: -0.095789, T: 245520, Avg. loss: 0.168649\n",
      "Total training time: 1.60 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 257.27, NNZs: 459, Bias: -0.097394, T: 253440, Avg. loss: 0.165889\n",
      "Total training time: 1.64 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 257.31, NNZs: 456, Bias: -0.099316, T: 261360, Avg. loss: 0.163867\n",
      "Total training time: 1.69 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 257.34, NNZs: 450, Bias: -0.101547, T: 269280, Avg. loss: 0.163727\n",
      "Total training time: 1.74 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 257.38, NNZs: 449, Bias: -0.101506, T: 277200, Avg. loss: 0.163164\n",
      "Total training time: 1.79 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 257.42, NNZs: 441, Bias: -0.103235, T: 285120, Avg. loss: 0.159561\n",
      "Total training time: 1.84 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 257.45, NNZs: 439, Bias: -0.101533, T: 293040, Avg. loss: 0.160821\n",
      "Total training time: 1.89 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 257.49, NNZs: 434, Bias: -0.102858, T: 300960, Avg. loss: 0.156707\n",
      "Total training time: 1.94 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 257.52, NNZs: 436, Bias: -0.102869, T: 308880, Avg. loss: 0.159980\n",
      "Total training time: 1.98 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 257.54, NNZs: 433, Bias: -0.104165, T: 316800, Avg. loss: 0.160467\n",
      "Total training time: 2.04 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 257.57, NNZs: 430, Bias: -0.105749, T: 324720, Avg. loss: 0.161833\n",
      "Total training time: 2.08 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 257.60, NNZs: 425, Bias: -0.106650, T: 332640, Avg. loss: 0.160381\n",
      "Total training time: 2.13 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 257.62, NNZs: 426, Bias: -0.105764, T: 340560, Avg. loss: 0.158605\n",
      "Total training time: 2.18 seconds.\n",
      "Convergence after 43 epochs took 2.18 seconds\n",
      "-- Epoch 1\n",
      "Norm: 238.41, NNZs: 1938, Bias: -0.094479, T: 7920, Avg. loss: 3.261016\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 248.08, NNZs: 1398, Bias: -0.132052, T: 15840, Avg. loss: 0.626326\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 250.69, NNZs: 1224, Bias: -0.136733, T: 23760, Avg. loss: 0.441458\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 251.85, NNZs: 1107, Bias: -0.150889, T: 31680, Avg. loss: 0.327185\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 252.47, NNZs: 1008, Bias: -0.141177, T: 39600, Avg. loss: 0.281094\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 252.86, NNZs: 947, Bias: -0.153474, T: 47520, Avg. loss: 0.238606\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 253.11, NNZs: 897, Bias: -0.154720, T: 55440, Avg. loss: 0.221372\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 253.29, NNZs: 857, Bias: -0.156655, T: 63360, Avg. loss: 0.208671\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 253.43, NNZs: 826, Bias: -0.149721, T: 71280, Avg. loss: 0.183400\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 253.54, NNZs: 799, Bias: -0.150926, T: 79200, Avg. loss: 0.173026\n",
      "Total training time: 0.56 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 253.68, NNZs: 782, Bias: -0.154425, T: 87120, Avg. loss: 0.162785\n",
      "Total training time: 0.61 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 253.74, NNZs: 764, Bias: -0.153349, T: 95040, Avg. loss: 0.159858\n",
      "Total training time: 0.66 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 253.80, NNZs: 740, Bias: -0.148155, T: 102960, Avg. loss: 0.157014\n",
      "Total training time: 0.71 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 253.84, NNZs: 723, Bias: -0.155615, T: 110880, Avg. loss: 0.150783\n",
      "Total training time: 0.76 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 253.89, NNZs: 708, Bias: -0.154736, T: 118800, Avg. loss: 0.146030\n",
      "Total training time: 0.82 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 253.94, NNZs: 692, Bias: -0.152167, T: 126720, Avg. loss: 0.139781\n",
      "Total training time: 0.87 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 253.98, NNZs: 682, Bias: -0.149945, T: 134640, Avg. loss: 0.138897\n",
      "Total training time: 0.92 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 254.02, NNZs: 669, Bias: -0.145116, T: 142560, Avg. loss: 0.134267\n",
      "Total training time: 0.97 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 254.06, NNZs: 656, Bias: -0.145841, T: 150480, Avg. loss: 0.132844\n",
      "Total training time: 1.02 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 254.11, NNZs: 648, Bias: -0.145912, T: 158400, Avg. loss: 0.127420\n",
      "Total training time: 1.08 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 254.14, NNZs: 641, Bias: -0.145883, T: 166320, Avg. loss: 0.126913\n",
      "Total training time: 1.13 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 254.17, NNZs: 638, Bias: -0.145333, T: 174240, Avg. loss: 0.124431\n",
      "Total training time: 1.18 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 254.20, NNZs: 631, Bias: -0.146406, T: 182160, Avg. loss: 0.124376\n",
      "Total training time: 1.23 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 254.24, NNZs: 623, Bias: -0.147986, T: 190080, Avg. loss: 0.119673\n",
      "Total training time: 1.28 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 254.27, NNZs: 613, Bias: -0.144428, T: 198000, Avg. loss: 0.121794\n",
      "Total training time: 1.33 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 254.29, NNZs: 606, Bias: -0.143942, T: 205920, Avg. loss: 0.119945\n",
      "Total training time: 1.38 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 254.31, NNZs: 600, Bias: -0.143078, T: 213840, Avg. loss: 0.118852\n",
      "Total training time: 1.43 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 254.34, NNZs: 594, Bias: -0.140833, T: 221760, Avg. loss: 0.114966\n",
      "Total training time: 1.48 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 254.37, NNZs: 588, Bias: -0.141291, T: 229680, Avg. loss: 0.115745\n",
      "Total training time: 1.53 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 254.39, NNZs: 582, Bias: -0.140006, T: 237600, Avg. loss: 0.115585\n",
      "Total training time: 1.59 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 254.42, NNZs: 573, Bias: -0.143325, T: 245520, Avg. loss: 0.115576\n",
      "Total training time: 1.64 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 254.44, NNZs: 571, Bias: -0.141743, T: 253440, Avg. loss: 0.114178\n",
      "Total training time: 1.69 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 254.46, NNZs: 566, Bias: -0.142089, T: 261360, Avg. loss: 0.110864\n",
      "Total training time: 1.74 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 254.48, NNZs: 562, Bias: -0.142428, T: 269280, Avg. loss: 0.113299\n",
      "Total training time: 1.79 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 254.51, NNZs: 561, Bias: -0.142096, T: 277200, Avg. loss: 0.109305\n",
      "Total training time: 1.84 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 254.53, NNZs: 560, Bias: -0.142096, T: 285120, Avg. loss: 0.110557\n",
      "Total training time: 1.89 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 254.55, NNZs: 555, Bias: -0.140029, T: 293040, Avg. loss: 0.107530\n",
      "Total training time: 1.94 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 254.58, NNZs: 550, Bias: -0.141729, T: 300960, Avg. loss: 0.107841\n",
      "Total training time: 1.99 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 254.60, NNZs: 548, Bias: -0.143337, T: 308880, Avg. loss: 0.109112\n",
      "Total training time: 2.04 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 254.62, NNZs: 545, Bias: -0.143344, T: 316800, Avg. loss: 0.109220\n",
      "Total training time: 2.08 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 254.64, NNZs: 542, Bias: -0.141772, T: 324720, Avg. loss: 0.107361\n",
      "Total training time: 2.13 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 254.66, NNZs: 539, Bias: -0.141788, T: 332640, Avg. loss: 0.104307\n",
      "Total training time: 2.18 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 254.68, NNZs: 537, Bias: -0.141497, T: 340560, Avg. loss: 0.107178\n",
      "Total training time: 2.23 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 254.70, NNZs: 536, Bias: -0.139765, T: 348480, Avg. loss: 0.104768\n",
      "Total training time: 2.28 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 254.72, NNZs: 532, Bias: -0.138917, T: 356400, Avg. loss: 0.104236\n",
      "Total training time: 2.33 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 254.74, NNZs: 528, Bias: -0.138653, T: 364320, Avg. loss: 0.105441\n",
      "Total training time: 2.38 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 254.76, NNZs: 524, Bias: -0.138916, T: 372240, Avg. loss: 0.104664\n",
      "Total training time: 2.43 seconds.\n",
      "Convergence after 47 epochs took 2.43 seconds\n",
      "-- Epoch 1\n",
      "Norm: 215.64, NNZs: 1557, Bias: -0.101146, T: 7920, Avg. loss: 3.097863\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 228.02, NNZs: 1104, Bias: -0.138136, T: 15840, Avg. loss: 0.659437\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 231.58, NNZs: 936, Bias: -0.135344, T: 23760, Avg. loss: 0.450557\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 233.28, NNZs: 824, Bias: -0.145462, T: 31680, Avg. loss: 0.337431\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 234.17, NNZs: 737, Bias: -0.136531, T: 39600, Avg. loss: 0.293192\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 234.79, NNZs: 690, Bias: -0.139736, T: 47520, Avg. loss: 0.271299\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 235.28, NNZs: 654, Bias: -0.142826, T: 55440, Avg. loss: 0.236381\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 235.61, NNZs: 630, Bias: -0.138292, T: 63360, Avg. loss: 0.226447\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 235.85, NNZs: 601, Bias: -0.132283, T: 71280, Avg. loss: 0.220969\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 236.01, NNZs: 584, Bias: -0.131142, T: 79200, Avg. loss: 0.210353\n",
      "Total training time: 0.55 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 236.13, NNZs: 575, Bias: -0.134630, T: 87120, Avg. loss: 0.201726\n",
      "Total training time: 0.60 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 236.24, NNZs: 561, Bias: -0.131293, T: 95040, Avg. loss: 0.194385\n",
      "Total training time: 0.65 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 236.33, NNZs: 552, Bias: -0.135193, T: 102960, Avg. loss: 0.185782\n",
      "Total training time: 0.70 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 236.44, NNZs: 547, Bias: -0.139598, T: 110880, Avg. loss: 0.179377\n",
      "Total training time: 0.75 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 236.53, NNZs: 533, Bias: -0.134342, T: 118800, Avg. loss: 0.173459\n",
      "Total training time: 0.80 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 236.61, NNZs: 523, Bias: -0.134409, T: 126720, Avg. loss: 0.172352\n",
      "Total training time: 0.85 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 236.67, NNZs: 519, Bias: -0.135843, T: 134640, Avg. loss: 0.173652\n",
      "Total training time: 0.90 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 236.74, NNZs: 510, Bias: -0.132270, T: 142560, Avg. loss: 0.172098\n",
      "Total training time: 0.95 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 236.80, NNZs: 498, Bias: -0.134993, T: 150480, Avg. loss: 0.166359\n",
      "Total training time: 1.00 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 236.85, NNZs: 488, Bias: -0.134376, T: 158400, Avg. loss: 0.163326\n",
      "Total training time: 1.05 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 236.91, NNZs: 486, Bias: -0.132590, T: 166320, Avg. loss: 0.161279\n",
      "Total training time: 1.10 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 236.96, NNZs: 481, Bias: -0.137200, T: 174240, Avg. loss: 0.157315\n",
      "Total training time: 1.15 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 237.00, NNZs: 474, Bias: -0.134947, T: 182160, Avg. loss: 0.159538\n",
      "Total training time: 1.20 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 237.05, NNZs: 468, Bias: -0.134416, T: 190080, Avg. loss: 0.155603\n",
      "Total training time: 1.25 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 237.09, NNZs: 466, Bias: -0.132976, T: 198000, Avg. loss: 0.154338\n",
      "Total training time: 1.30 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 237.12, NNZs: 464, Bias: -0.134982, T: 205920, Avg. loss: 0.153289\n",
      "Total training time: 1.35 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 237.16, NNZs: 459, Bias: -0.133603, T: 213840, Avg. loss: 0.148365\n",
      "Total training time: 1.41 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 237.19, NNZs: 454, Bias: -0.137717, T: 221760, Avg. loss: 0.149910\n",
      "Total training time: 1.46 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 237.22, NNZs: 453, Bias: -0.136400, T: 229680, Avg. loss: 0.148244\n",
      "Total training time: 1.51 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 237.25, NNZs: 449, Bias: -0.136822, T: 237600, Avg. loss: 0.147632\n",
      "Total training time: 1.55 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 237.28, NNZs: 441, Bias: -0.137613, T: 245520, Avg. loss: 0.145840\n",
      "Total training time: 1.60 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 237.31, NNZs: 441, Bias: -0.137601, T: 253440, Avg. loss: 0.146287\n",
      "Total training time: 1.65 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 237.34, NNZs: 437, Bias: -0.136444, T: 261360, Avg. loss: 0.143354\n",
      "Total training time: 1.70 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 237.37, NNZs: 436, Bias: -0.134598, T: 269280, Avg. loss: 0.144189\n",
      "Total training time: 1.75 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 237.40, NNZs: 431, Bias: -0.137493, T: 277200, Avg. loss: 0.143358\n",
      "Total training time: 1.80 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 237.42, NNZs: 430, Bias: -0.136764, T: 285120, Avg. loss: 0.144933\n",
      "Total training time: 1.85 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 237.45, NNZs: 427, Bias: -0.137105, T: 293040, Avg. loss: 0.141624\n",
      "Total training time: 1.90 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 237.48, NNZs: 428, Bias: -0.137115, T: 300960, Avg. loss: 0.141453\n",
      "Total training time: 1.95 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 237.50, NNZs: 425, Bias: -0.138414, T: 308880, Avg. loss: 0.142164\n",
      "Total training time: 1.99 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 237.52, NNZs: 423, Bias: -0.138733, T: 316800, Avg. loss: 0.141193\n",
      "Total training time: 2.04 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 237.54, NNZs: 423, Bias: -0.138143, T: 324720, Avg. loss: 0.140585\n",
      "Total training time: 2.09 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 237.56, NNZs: 422, Bias: -0.137852, T: 332640, Avg. loss: 0.138033\n",
      "Total training time: 2.14 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 237.59, NNZs: 423, Bias: -0.137286, T: 340560, Avg. loss: 0.139067\n",
      "Total training time: 2.19 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 237.61, NNZs: 424, Bias: -0.138162, T: 348480, Avg. loss: 0.139147\n",
      "Total training time: 2.24 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 237.63, NNZs: 416, Bias: -0.139018, T: 356400, Avg. loss: 0.137585\n",
      "Total training time: 2.29 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 237.65, NNZs: 414, Bias: -0.138750, T: 364320, Avg. loss: 0.137684\n",
      "Total training time: 2.34 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 237.67, NNZs: 413, Bias: -0.139024, T: 372240, Avg. loss: 0.137688\n",
      "Total training time: 2.39 seconds.\n",
      "Convergence after 47 epochs took 2.39 seconds\n",
      "-- Epoch 1\n",
      "Norm: 238.03, NNZs: 1603, Bias: -0.038632, T: 7920, Avg. loss: 3.516425\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 252.32, NNZs: 1035, Bias: -0.071888, T: 15840, Avg. loss: 0.594987\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 257.51, NNZs: 853, Bias: -0.041507, T: 23760, Avg. loss: 0.384923\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 259.93, NNZs: 759, Bias: -0.070581, T: 31680, Avg. loss: 0.288118\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 261.72, NNZs: 694, Bias: -0.057999, T: 39600, Avg. loss: 0.194885\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 262.99, NNZs: 655, Bias: -0.059578, T: 47520, Avg. loss: 0.172262\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 263.87, NNZs: 622, Bias: -0.051596, T: 55440, Avg. loss: 0.180369\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 264.65, NNZs: 586, Bias: -0.056509, T: 63360, Avg. loss: 0.149120\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 265.29, NNZs: 556, Bias: -0.050438, T: 71280, Avg. loss: 0.151464\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 265.84, NNZs: 544, Bias: -0.056825, T: 79200, Avg. loss: 0.138685\n",
      "Total training time: 0.55 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 266.32, NNZs: 533, Bias: -0.052880, T: 87120, Avg. loss: 0.140703\n",
      "Total training time: 0.60 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 266.68, NNZs: 515, Bias: -0.046268, T: 95040, Avg. loss: 0.152224\n",
      "Total training time: 0.65 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 267.05, NNZs: 504, Bias: -0.051415, T: 102960, Avg. loss: 0.132432\n",
      "Total training time: 0.71 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 267.37, NNZs: 492, Bias: -0.051360, T: 110880, Avg. loss: 0.135402\n",
      "Total training time: 0.76 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 267.62, NNZs: 483, Bias: -0.053735, T: 118800, Avg. loss: 0.144139\n",
      "Total training time: 0.81 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 267.88, NNZs: 477, Bias: -0.052837, T: 126720, Avg. loss: 0.131819\n",
      "Total training time: 0.86 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 268.12, NNZs: 468, Bias: -0.052774, T: 134640, Avg. loss: 0.127795\n",
      "Total training time: 0.91 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 268.34, NNZs: 455, Bias: -0.051309, T: 142560, Avg. loss: 0.129882\n",
      "Total training time: 0.96 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 268.54, NNZs: 449, Bias: -0.051980, T: 150480, Avg. loss: 0.129791\n",
      "Total training time: 1.01 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 268.73, NNZs: 446, Bias: -0.052601, T: 158400, Avg. loss: 0.126152\n",
      "Total training time: 1.06 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 268.90, NNZs: 447, Bias: -0.051358, T: 166320, Avg. loss: 0.127206\n",
      "Total training time: 1.11 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 269.04, NNZs: 441, Bias: -0.051880, T: 174240, Avg. loss: 0.134874\n",
      "Total training time: 1.16 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 269.18, NNZs: 442, Bias: -0.051817, T: 182160, Avg. loss: 0.135385\n",
      "Total training time: 1.21 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 269.31, NNZs: 433, Bias: -0.051780, T: 190080, Avg. loss: 0.134604\n",
      "Total training time: 1.26 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 269.44, NNZs: 427, Bias: -0.053294, T: 198000, Avg. loss: 0.130692\n",
      "Total training time: 1.31 seconds.\n",
      "Convergence after 25 epochs took 1.31 seconds\n",
      "-- Epoch 1\n",
      "Norm: 239.24, NNZs: 1505, Bias: -0.207921, T: 7920, Avg. loss: 4.068483\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 255.28, NNZs: 1058, Bias: -0.161665, T: 15840, Avg. loss: 0.667120\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 262.10, NNZs: 897, Bias: -0.140167, T: 23760, Avg. loss: 0.404713\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 265.73, NNZs: 812, Bias: -0.148917, T: 31680, Avg. loss: 0.288688\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 267.97, NNZs: 749, Bias: -0.149013, T: 39600, Avg. loss: 0.252858\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 269.60, NNZs: 715, Bias: -0.144860, T: 47520, Avg. loss: 0.228691\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 270.76, NNZs: 685, Bias: -0.146838, T: 55440, Avg. loss: 0.213483\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 271.68, NNZs: 665, Bias: -0.148475, T: 63360, Avg. loss: 0.208373\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 272.37, NNZs: 646, Bias: -0.144331, T: 71280, Avg. loss: 0.202611\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 273.00, NNZs: 638, Bias: -0.148304, T: 79200, Avg. loss: 0.182875\n",
      "Total training time: 0.56 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 273.50, NNZs: 628, Bias: -0.148221, T: 87120, Avg. loss: 0.189466\n",
      "Total training time: 0.61 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 273.89, NNZs: 610, Bias: -0.146369, T: 95040, Avg. loss: 0.196268\n",
      "Total training time: 0.66 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 274.25, NNZs: 600, Bias: -0.151490, T: 102960, Avg. loss: 0.183785\n",
      "Total training time: 0.71 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 274.56, NNZs: 589, Bias: -0.154173, T: 110880, Avg. loss: 0.180136\n",
      "Total training time: 0.76 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 274.83, NNZs: 588, Bias: -0.149937, T: 118800, Avg. loss: 0.182558\n",
      "Total training time: 0.81 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 275.06, NNZs: 579, Bias: -0.153989, T: 126720, Avg. loss: 0.174191\n",
      "Total training time: 0.86 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 275.27, NNZs: 578, Bias: -0.151707, T: 134640, Avg. loss: 0.178406\n",
      "Total training time: 0.92 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 275.47, NNZs: 571, Bias: -0.153905, T: 142560, Avg. loss: 0.173973\n",
      "Total training time: 0.97 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 275.65, NNZs: 566, Bias: -0.153920, T: 150480, Avg. loss: 0.169580\n",
      "Total training time: 1.02 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 275.81, NNZs: 558, Bias: -0.155872, T: 158400, Avg. loss: 0.168687\n",
      "Total training time: 1.07 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 275.96, NNZs: 552, Bias: -0.158316, T: 166320, Avg. loss: 0.172730\n",
      "Total training time: 1.12 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 276.09, NNZs: 543, Bias: -0.158285, T: 174240, Avg. loss: 0.171172\n",
      "Total training time: 1.17 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 276.23, NNZs: 538, Bias: -0.156583, T: 182160, Avg. loss: 0.161082\n",
      "Total training time: 1.23 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 276.36, NNZs: 538, Bias: -0.158683, T: 190080, Avg. loss: 0.163214\n",
      "Total training time: 1.28 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 276.47, NNZs: 535, Bias: -0.158695, T: 198000, Avg. loss: 0.164951\n",
      "Total training time: 1.33 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 276.58, NNZs: 537, Bias: -0.160647, T: 205920, Avg. loss: 0.161885\n",
      "Total training time: 1.38 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 276.69, NNZs: 538, Bias: -0.158739, T: 213840, Avg. loss: 0.159897\n",
      "Total training time: 1.43 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 276.79, NNZs: 532, Bias: -0.161455, T: 221760, Avg. loss: 0.156599\n",
      "Total training time: 1.48 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 276.88, NNZs: 534, Bias: -0.161006, T: 229680, Avg. loss: 0.159838\n",
      "Total training time: 1.53 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 276.97, NNZs: 530, Bias: -0.162722, T: 237600, Avg. loss: 0.160586\n",
      "Total training time: 1.58 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 277.05, NNZs: 527, Bias: -0.163121, T: 245520, Avg. loss: 0.157387\n",
      "Total training time: 1.63 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 277.12, NNZs: 526, Bias: -0.165088, T: 253440, Avg. loss: 0.157603\n",
      "Total training time: 1.68 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 277.20, NNZs: 522, Bias: -0.163129, T: 261360, Avg. loss: 0.157482\n",
      "Total training time: 1.74 seconds.\n",
      "Convergence after 33 epochs took 1.74 seconds\n",
      "-- Epoch 1\n",
      "Norm: 258.31, NNZs: 1880, Bias: -0.014295, T: 7920, Avg. loss: 3.441365\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 269.92, NNZs: 1290, Bias: 0.010759, T: 15840, Avg. loss: 0.559930\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 273.82, NNZs: 1073, Bias: 0.034817, T: 23760, Avg. loss: 0.331901\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 275.63, NNZs: 943, Bias: 0.027814, T: 31680, Avg. loss: 0.300424\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 276.59, NNZs: 848, Bias: 0.008930, T: 39600, Avg. loss: 0.242873\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 277.27, NNZs: 774, Bias: 0.014140, T: 47520, Avg. loss: 0.213217\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 277.83, NNZs: 727, Bias: 0.018039, T: 55440, Avg. loss: 0.196965\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 278.19, NNZs: 688, Bias: 0.018070, T: 63360, Avg. loss: 0.182101\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 278.48, NNZs: 663, Bias: 0.019293, T: 71280, Avg. loss: 0.160370\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 278.75, NNZs: 643, Bias: 0.023237, T: 79200, Avg. loss: 0.153771\n",
      "Total training time: 0.55 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 278.93, NNZs: 618, Bias: 0.025729, T: 87120, Avg. loss: 0.162533\n",
      "Total training time: 0.61 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 279.10, NNZs: 605, Bias: 0.026851, T: 95040, Avg. loss: 0.154424\n",
      "Total training time: 0.66 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 279.28, NNZs: 588, Bias: 0.028601, T: 102960, Avg. loss: 0.135272\n",
      "Total training time: 0.71 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 279.41, NNZs: 579, Bias: 0.024907, T: 110880, Avg. loss: 0.146280\n",
      "Total training time: 0.76 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 279.54, NNZs: 562, Bias: 0.025745, T: 118800, Avg. loss: 0.129554\n",
      "Total training time: 0.81 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 279.64, NNZs: 544, Bias: 0.029011, T: 126720, Avg. loss: 0.134483\n",
      "Total training time: 0.86 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 279.75, NNZs: 536, Bias: 0.029713, T: 134640, Avg. loss: 0.133682\n",
      "Total training time: 0.91 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 279.82, NNZs: 525, Bias: 0.029758, T: 142560, Avg. loss: 0.136416\n",
      "Total training time: 0.96 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 279.91, NNZs: 519, Bias: 0.029720, T: 150480, Avg. loss: 0.123948\n",
      "Total training time: 1.01 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 279.99, NNZs: 507, Bias: 0.029122, T: 158400, Avg. loss: 0.122270\n",
      "Total training time: 1.07 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 280.07, NNZs: 502, Bias: 0.031535, T: 166320, Avg. loss: 0.120259\n",
      "Total training time: 1.12 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 280.14, NNZs: 489, Bias: 0.030464, T: 174240, Avg. loss: 0.130778\n",
      "Total training time: 1.17 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 280.20, NNZs: 483, Bias: 0.032729, T: 182160, Avg. loss: 0.122615\n",
      "Total training time: 1.22 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 280.27, NNZs: 480, Bias: 0.032236, T: 190080, Avg. loss: 0.126701\n",
      "Total training time: 1.26 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 280.32, NNZs: 477, Bias: 0.031746, T: 198000, Avg. loss: 0.120140\n",
      "Total training time: 1.32 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 280.37, NNZs: 473, Bias: 0.033719, T: 205920, Avg. loss: 0.119426\n",
      "Total training time: 1.36 seconds.\n",
      "Convergence after 26 epochs took 1.36 seconds\n",
      "-- Epoch 1\n",
      "Norm: 248.49, NNZs: 1644, Bias: -0.081922, T: 7920, Avg. loss: 4.007254\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 265.14, NNZs: 1153, Bias: -0.016094, T: 15840, Avg. loss: 0.626822\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 272.33, NNZs: 979, Bias: -0.035332, T: 23760, Avg. loss: 0.286367\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 275.72, NNZs: 873, Bias: -0.043986, T: 31680, Avg. loss: 0.289407\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 278.04, NNZs: 815, Bias: -0.026842, T: 39600, Avg. loss: 0.220406\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 279.62, NNZs: 771, Bias: -0.016700, T: 47520, Avg. loss: 0.204301\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 280.65, NNZs: 732, Bias: -0.030076, T: 55440, Avg. loss: 0.227402\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 281.64, NNZs: 706, Bias: -0.031723, T: 63360, Avg. loss: 0.146387\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 282.46, NNZs: 691, Bias: -0.027427, T: 71280, Avg. loss: 0.134626\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 283.11, NNZs: 683, Bias: -0.026239, T: 79200, Avg. loss: 0.139105\n",
      "Total training time: 0.55 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 283.69, NNZs: 662, Bias: -0.027546, T: 87120, Avg. loss: 0.142695\n",
      "Total training time: 0.60 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 284.14, NNZs: 649, Bias: -0.024404, T: 95040, Avg. loss: 0.148482\n",
      "Total training time: 0.66 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 284.52, NNZs: 637, Bias: -0.026331, T: 102960, Avg. loss: 0.156894\n",
      "Total training time: 0.71 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 284.90, NNZs: 625, Bias: -0.028228, T: 110880, Avg. loss: 0.129331\n",
      "Total training time: 0.76 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 285.23, NNZs: 616, Bias: -0.024736, T: 118800, Avg. loss: 0.140985\n",
      "Total training time: 0.81 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 285.51, NNZs: 612, Bias: -0.027968, T: 126720, Avg. loss: 0.140392\n",
      "Total training time: 0.86 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 285.76, NNZs: 605, Bias: -0.025610, T: 134640, Avg. loss: 0.140041\n",
      "Total training time: 0.91 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 285.99, NNZs: 599, Bias: -0.026299, T: 142560, Avg. loss: 0.141784\n",
      "Total training time: 0.96 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 286.20, NNZs: 597, Bias: -0.023569, T: 150480, Avg. loss: 0.136990\n",
      "Total training time: 1.01 seconds.\n",
      "Convergence after 19 epochs took 1.01 seconds\n",
      "-- Epoch 1\n",
      "Norm: 244.41, NNZs: 1724, Bias: -0.174906, T: 7920, Avg. loss: 3.746398\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 261.37, NNZs: 1274, Bias: -0.147552, T: 15840, Avg. loss: 0.597830\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 267.07, NNZs: 1108, Bias: -0.131574, T: 23760, Avg. loss: 0.392470\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 269.97, NNZs: 1020, Bias: -0.123532, T: 31680, Avg. loss: 0.327525\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 271.74, NNZs: 942, Bias: -0.119960, T: 39600, Avg. loss: 0.266439\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 272.79, NNZs: 894, Bias: -0.113089, T: 47520, Avg. loss: 0.230952\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 273.63, NNZs: 863, Bias: -0.116726, T: 55440, Avg. loss: 0.208703\n",
      "Total training time: 0.41 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 274.31, NNZs: 838, Bias: -0.119255, T: 63360, Avg. loss: 0.190917\n",
      "Total training time: 0.46 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 274.80, NNZs: 824, Bias: -0.104701, T: 71280, Avg. loss: 0.193175\n",
      "Total training time: 0.51 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 275.21, NNZs: 798, Bias: -0.112588, T: 79200, Avg. loss: 0.176540\n",
      "Total training time: 0.57 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 275.53, NNZs: 783, Bias: -0.116936, T: 87120, Avg. loss: 0.178597\n",
      "Total training time: 0.62 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 275.81, NNZs: 768, Bias: -0.111444, T: 95040, Avg. loss: 0.180314\n",
      "Total training time: 0.67 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 276.05, NNZs: 760, Bias: -0.107481, T: 102960, Avg. loss: 0.166504\n",
      "Total training time: 0.72 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 276.27, NNZs: 756, Bias: -0.110336, T: 110880, Avg. loss: 0.161195\n",
      "Total training time: 0.77 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 276.48, NNZs: 741, Bias: -0.106945, T: 118800, Avg. loss: 0.153214\n",
      "Total training time: 0.82 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 276.67, NNZs: 724, Bias: -0.110946, T: 126720, Avg. loss: 0.147856\n",
      "Total training time: 0.87 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 276.82, NNZs: 714, Bias: -0.111673, T: 134640, Avg. loss: 0.151586\n",
      "Total training time: 0.92 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 276.98, NNZs: 709, Bias: -0.112281, T: 142560, Avg. loss: 0.145741\n",
      "Total training time: 0.98 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 277.12, NNZs: 704, Bias: -0.111601, T: 150480, Avg. loss: 0.151171\n",
      "Total training time: 1.03 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 277.25, NNZs: 702, Bias: -0.109726, T: 158400, Avg. loss: 0.147710\n",
      "Total training time: 1.08 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 277.37, NNZs: 697, Bias: -0.111000, T: 166320, Avg. loss: 0.145104\n",
      "Total training time: 1.13 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 277.49, NNZs: 691, Bias: -0.113268, T: 174240, Avg. loss: 0.143654\n",
      "Total training time: 1.18 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 277.59, NNZs: 686, Bias: -0.113229, T: 182160, Avg. loss: 0.144163\n",
      "Total training time: 1.24 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 277.69, NNZs: 681, Bias: -0.110058, T: 190080, Avg. loss: 0.145197\n",
      "Total training time: 1.28 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 277.78, NNZs: 674, Bias: -0.113116, T: 198000, Avg. loss: 0.138206\n",
      "Total training time: 1.33 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 277.87, NNZs: 672, Bias: -0.112146, T: 205920, Avg. loss: 0.140489\n",
      "Total training time: 1.39 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 277.95, NNZs: 661, Bias: -0.113982, T: 213840, Avg. loss: 0.137311\n",
      "Total training time: 1.44 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 278.03, NNZs: 658, Bias: -0.110815, T: 221760, Avg. loss: 0.139406\n",
      "Total training time: 1.49 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 278.10, NNZs: 653, Bias: -0.113905, T: 229680, Avg. loss: 0.133904\n",
      "Total training time: 1.54 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 278.18, NNZs: 644, Bias: -0.114776, T: 237600, Avg. loss: 0.138904\n",
      "Total training time: 1.59 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 278.25, NNZs: 642, Bias: -0.115649, T: 245520, Avg. loss: 0.138468\n",
      "Total training time: 1.64 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 278.31, NNZs: 641, Bias: -0.114448, T: 253440, Avg. loss: 0.135913\n",
      "Total training time: 1.70 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 278.38, NNZs: 636, Bias: -0.115229, T: 261360, Avg. loss: 0.132976\n",
      "Total training time: 1.75 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 278.44, NNZs: 634, Bias: -0.115597, T: 269280, Avg. loss: 0.130805\n",
      "Total training time: 1.80 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 278.51, NNZs: 633, Bias: -0.117756, T: 277200, Avg. loss: 0.130761\n",
      "Total training time: 1.86 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 278.57, NNZs: 629, Bias: -0.117400, T: 285120, Avg. loss: 0.133915\n",
      "Total training time: 1.91 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 278.62, NNZs: 626, Bias: -0.117741, T: 293040, Avg. loss: 0.130947\n",
      "Total training time: 1.96 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 278.68, NNZs: 624, Bias: -0.117084, T: 300960, Avg. loss: 0.133906\n",
      "Total training time: 2.02 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 278.73, NNZs: 624, Bias: -0.118397, T: 308880, Avg. loss: 0.131234\n",
      "Total training time: 2.06 seconds.\n",
      "Convergence after 39 epochs took 2.06 seconds\n",
      "-- Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  11 out of  11 | elapsed:   21.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 213.48, NNZs: 1493, Bias: 0.007783, T: 7920, Avg. loss: 3.390525\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 222.85, NNZs: 1082, Bias: -0.024662, T: 15840, Avg. loss: 0.830305\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 225.71, NNZs: 927, Bias: -0.026104, T: 23760, Avg. loss: 0.515127\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 227.07, NNZs: 819, Bias: -0.029611, T: 31680, Avg. loss: 0.394541\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 227.77, NNZs: 775, Bias: -0.034298, T: 39600, Avg. loss: 0.343294\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 228.22, NNZs: 739, Bias: -0.028907, T: 47520, Avg. loss: 0.303082\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 228.52, NNZs: 706, Bias: -0.025362, T: 55440, Avg. loss: 0.271249\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 228.74, NNZs: 679, Bias: -0.033267, T: 63360, Avg. loss: 0.249853\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 228.87, NNZs: 647, Bias: -0.029910, T: 71280, Avg. loss: 0.236342\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 229.00, NNZs: 631, Bias: -0.025969, T: 79200, Avg. loss: 0.226805\n",
      "Total training time: 0.55 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 229.08, NNZs: 611, Bias: -0.026064, T: 87120, Avg. loss: 0.220356\n",
      "Total training time: 0.60 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 229.15, NNZs: 597, Bias: -0.025084, T: 95040, Avg. loss: 0.210047\n",
      "Total training time: 0.65 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 229.22, NNZs: 582, Bias: -0.024889, T: 102960, Avg. loss: 0.205677\n",
      "Total training time: 0.70 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 229.28, NNZs: 572, Bias: -0.024722, T: 110880, Avg. loss: 0.198502\n",
      "Total training time: 0.75 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 229.33, NNZs: 561, Bias: -0.025557, T: 118800, Avg. loss: 0.193579\n",
      "Total training time: 0.80 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 229.38, NNZs: 555, Bias: -0.022303, T: 126720, Avg. loss: 0.187554\n",
      "Total training time: 0.85 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 229.43, NNZs: 545, Bias: -0.027617, T: 134640, Avg. loss: 0.181560\n",
      "Total training time: 0.90 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 229.45, NNZs: 534, Bias: -0.024711, T: 142560, Avg. loss: 0.180461\n",
      "Total training time: 0.95 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 229.49, NNZs: 528, Bias: -0.023354, T: 150480, Avg. loss: 0.175262\n",
      "Total training time: 1.00 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 229.51, NNZs: 525, Bias: -0.022807, T: 158400, Avg. loss: 0.177558\n",
      "Total training time: 1.05 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 229.53, NNZs: 519, Bias: -0.023997, T: 166320, Avg. loss: 0.172173\n",
      "Total training time: 1.10 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 229.55, NNZs: 513, Bias: -0.021722, T: 174240, Avg. loss: 0.167563\n",
      "Total training time: 1.15 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 229.57, NNZs: 509, Bias: -0.023992, T: 182160, Avg. loss: 0.166907\n",
      "Total training time: 1.20 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 229.60, NNZs: 505, Bias: -0.023435, T: 190080, Avg. loss: 0.166076\n",
      "Total training time: 1.25 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 229.62, NNZs: 502, Bias: -0.023441, T: 198000, Avg. loss: 0.161541\n",
      "Total training time: 1.30 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 229.64, NNZs: 500, Bias: -0.024898, T: 205920, Avg. loss: 0.162888\n",
      "Total training time: 1.35 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 229.66, NNZs: 495, Bias: -0.024400, T: 213840, Avg. loss: 0.160354\n",
      "Total training time: 1.40 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 229.69, NNZs: 490, Bias: -0.024443, T: 221760, Avg. loss: 0.158144\n",
      "Total training time: 1.45 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 229.70, NNZs: 485, Bias: -0.024862, T: 229680, Avg. loss: 0.158173\n",
      "Total training time: 1.50 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 229.72, NNZs: 482, Bias: -0.023968, T: 237600, Avg. loss: 0.156442\n",
      "Total training time: 1.55 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 229.73, NNZs: 479, Bias: -0.023124, T: 245520, Avg. loss: 0.156626\n",
      "Total training time: 1.60 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 229.75, NNZs: 475, Bias: -0.023506, T: 253440, Avg. loss: 0.153036\n",
      "Total training time: 1.66 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 229.77, NNZs: 471, Bias: -0.022350, T: 261360, Avg. loss: 0.152071\n",
      "Total training time: 1.70 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 229.79, NNZs: 467, Bias: -0.025329, T: 269280, Avg. loss: 0.151862\n",
      "Total training time: 1.75 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 229.81, NNZs: 464, Bias: -0.023133, T: 277200, Avg. loss: 0.152259\n",
      "Total training time: 1.80 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 229.82, NNZs: 457, Bias: -0.023846, T: 285120, Avg. loss: 0.150575\n",
      "Total training time: 1.85 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 229.84, NNZs: 456, Bias: -0.023505, T: 293040, Avg. loss: 0.150205\n",
      "Total training time: 1.90 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 229.85, NNZs: 455, Bias: -0.023500, T: 300960, Avg. loss: 0.150123\n",
      "Total training time: 1.95 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 229.87, NNZs: 453, Bias: -0.023489, T: 308880, Avg. loss: 0.148127\n",
      "Total training time: 2.00 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 229.88, NNZs: 454, Bias: -0.020961, T: 316800, Avg. loss: 0.145806\n",
      "Total training time: 2.05 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 229.90, NNZs: 452, Bias: -0.021928, T: 324720, Avg. loss: 0.145174\n",
      "Total training time: 2.10 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 229.92, NNZs: 453, Bias: -0.023128, T: 332640, Avg. loss: 0.144346\n",
      "Total training time: 2.14 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 229.94, NNZs: 451, Bias: -0.023112, T: 340560, Avg. loss: 0.146423\n",
      "Total training time: 2.19 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 229.96, NNZs: 449, Bias: -0.022829, T: 348480, Avg. loss: 0.143881\n",
      "Total training time: 2.24 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 229.97, NNZs: 447, Bias: -0.022831, T: 356400, Avg. loss: 0.144667\n",
      "Total training time: 2.29 seconds.\n",
      "Convergence after 45 epochs took 2.29 seconds\n",
      "-- Epoch 1\n",
      "Norm: 231.80, NNZs: 1735, Bias: -0.305552, T: 7920, Avg. loss: 4.220605\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 248.66, NNZs: 1299, Bias: -0.261238, T: 15840, Avg. loss: 0.694000\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 253.87, NNZs: 1125, Bias: -0.232118, T: 23760, Avg. loss: 0.380450\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 256.38, NNZs: 1008, Bias: -0.223806, T: 31680, Avg. loss: 0.282989\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 257.79, NNZs: 936, Bias: -0.207808, T: 39600, Avg. loss: 0.186593\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 258.71, NNZs: 878, Bias: -0.212590, T: 47520, Avg. loss: 0.173172\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 259.39, NNZs: 835, Bias: -0.213932, T: 55440, Avg. loss: 0.154989\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 259.91, NNZs: 797, Bias: -0.211897, T: 63360, Avg. loss: 0.155551\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 260.26, NNZs: 770, Bias: -0.207745, T: 71280, Avg. loss: 0.165896\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 260.56, NNZs: 751, Bias: -0.210211, T: 79200, Avg. loss: 0.150531\n",
      "Total training time: 0.55 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 260.82, NNZs: 721, Bias: -0.208926, T: 87120, Avg. loss: 0.151782\n",
      "Total training time: 0.60 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 261.05, NNZs: 700, Bias: -0.203502, T: 95040, Avg. loss: 0.142087\n",
      "Total training time: 0.65 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 261.26, NNZs: 690, Bias: -0.205412, T: 102960, Avg. loss: 0.135825\n",
      "Total training time: 0.70 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 261.42, NNZs: 670, Bias: -0.201776, T: 110880, Avg. loss: 0.138100\n",
      "Total training time: 0.75 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 261.57, NNZs: 652, Bias: -0.201923, T: 118800, Avg. loss: 0.131266\n",
      "Total training time: 0.80 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 261.71, NNZs: 640, Bias: -0.205198, T: 126720, Avg. loss: 0.132612\n",
      "Total training time: 0.85 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 261.83, NNZs: 630, Bias: -0.203740, T: 134640, Avg. loss: 0.128102\n",
      "Total training time: 0.90 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 261.94, NNZs: 618, Bias: -0.206583, T: 142560, Avg. loss: 0.127350\n",
      "Total training time: 0.95 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 262.03, NNZs: 609, Bias: -0.204565, T: 150480, Avg. loss: 0.125381\n",
      "Total training time: 1.00 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 262.13, NNZs: 597, Bias: -0.205785, T: 158400, Avg. loss: 0.121092\n",
      "Total training time: 1.05 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 262.20, NNZs: 594, Bias: -0.208797, T: 166320, Avg. loss: 0.127693\n",
      "Total training time: 1.10 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 262.28, NNZs: 589, Bias: -0.207011, T: 174240, Avg. loss: 0.124688\n",
      "Total training time: 1.15 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 262.35, NNZs: 584, Bias: -0.207504, T: 182160, Avg. loss: 0.120578\n",
      "Total training time: 1.20 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 262.42, NNZs: 578, Bias: -0.207473, T: 190080, Avg. loss: 0.120746\n",
      "Total training time: 1.25 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 262.48, NNZs: 575, Bias: -0.205426, T: 198000, Avg. loss: 0.119185\n",
      "Total training time: 1.30 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 262.54, NNZs: 572, Bias: -0.207439, T: 205920, Avg. loss: 0.119432\n",
      "Total training time: 1.35 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 262.60, NNZs: 565, Bias: -0.206977, T: 213840, Avg. loss: 0.121726\n",
      "Total training time: 1.40 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 262.65, NNZs: 558, Bias: -0.209216, T: 221760, Avg. loss: 0.115620\n",
      "Total training time: 1.45 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 262.70, NNZs: 556, Bias: -0.206552, T: 229680, Avg. loss: 0.115564\n",
      "Total training time: 1.50 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 262.75, NNZs: 555, Bias: -0.205737, T: 237600, Avg. loss: 0.116267\n",
      "Total training time: 1.55 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 262.80, NNZs: 549, Bias: -0.206610, T: 245520, Avg. loss: 0.114968\n",
      "Total training time: 1.60 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 262.84, NNZs: 545, Bias: -0.207827, T: 253440, Avg. loss: 0.114597\n",
      "Total training time: 1.65 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 262.89, NNZs: 541, Bias: -0.207828, T: 261360, Avg. loss: 0.115162\n",
      "Total training time: 1.70 seconds.\n",
      "Convergence after 33 epochs took 1.70 seconds\n",
      "-- Epoch 1\n",
      "Norm: 241.53, NNZs: 1528, Bias: -0.182882, T: 7920, Avg. loss: 3.492919\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 256.52, NNZs: 1112, Bias: -0.187093, T: 15840, Avg. loss: 0.752643\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 261.76, NNZs: 976, Bias: -0.190444, T: 23760, Avg. loss: 0.440054\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 264.14, NNZs: 879, Bias: -0.205397, T: 31680, Avg. loss: 0.333953\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 265.58, NNZs: 831, Bias: -0.204297, T: 39600, Avg. loss: 0.302215\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 266.56, NNZs: 779, Bias: -0.206629, T: 47520, Avg. loss: 0.253583\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 267.17, NNZs: 731, Bias: -0.202851, T: 55440, Avg. loss: 0.229204\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 267.67, NNZs: 708, Bias: -0.222262, T: 63360, Avg. loss: 0.208192\n",
      "Total training time: 0.44 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 268.04, NNZs: 684, Bias: -0.213135, T: 71280, Avg. loss: 0.203577\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 268.32, NNZs: 661, Bias: -0.213012, T: 79200, Avg. loss: 0.195603\n",
      "Total training time: 0.53 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 268.56, NNZs: 642, Bias: -0.218882, T: 87120, Avg. loss: 0.190020\n",
      "Total training time: 0.59 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 268.75, NNZs: 620, Bias: -0.221828, T: 95040, Avg. loss: 0.178219\n",
      "Total training time: 0.63 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 268.94, NNZs: 611, Bias: -0.216820, T: 102960, Avg. loss: 0.171121\n",
      "Total training time: 0.68 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 269.08, NNZs: 598, Bias: -0.218771, T: 110880, Avg. loss: 0.170951\n",
      "Total training time: 0.73 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 269.20, NNZs: 587, Bias: -0.220402, T: 118800, Avg. loss: 0.163381\n",
      "Total training time: 0.78 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 269.30, NNZs: 577, Bias: -0.223605, T: 126720, Avg. loss: 0.159232\n",
      "Total training time: 0.83 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 269.40, NNZs: 563, Bias: -0.224377, T: 134640, Avg. loss: 0.154686\n",
      "Total training time: 0.88 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 269.49, NNZs: 546, Bias: -0.225019, T: 142560, Avg. loss: 0.156608\n",
      "Total training time: 0.93 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 269.57, NNZs: 544, Bias: -0.226333, T: 150480, Avg. loss: 0.156118\n",
      "Total training time: 0.98 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 269.65, NNZs: 538, Bias: -0.225711, T: 158400, Avg. loss: 0.149390\n",
      "Total training time: 1.03 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 269.71, NNZs: 531, Bias: -0.225684, T: 166320, Avg. loss: 0.148052\n",
      "Total training time: 1.08 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 269.77, NNZs: 529, Bias: -0.229752, T: 174240, Avg. loss: 0.147134\n",
      "Total training time: 1.13 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 269.82, NNZs: 522, Bias: -0.230290, T: 182160, Avg. loss: 0.145039\n",
      "Total training time: 1.18 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 269.89, NNZs: 516, Bias: -0.226055, T: 190080, Avg. loss: 0.140871\n",
      "Total training time: 1.23 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 269.94, NNZs: 510, Bias: -0.228627, T: 198000, Avg. loss: 0.142677\n",
      "Total training time: 1.28 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 269.99, NNZs: 507, Bias: -0.230615, T: 205920, Avg. loss: 0.139546\n",
      "Total training time: 1.33 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 270.04, NNZs: 500, Bias: -0.230121, T: 213840, Avg. loss: 0.140843\n",
      "Total training time: 1.38 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 270.08, NNZs: 498, Bias: -0.230586, T: 221760, Avg. loss: 0.137951\n",
      "Total training time: 1.43 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 270.12, NNZs: 496, Bias: -0.233238, T: 229680, Avg. loss: 0.137594\n",
      "Total training time: 1.48 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 270.16, NNZs: 494, Bias: -0.233645, T: 237600, Avg. loss: 0.134123\n",
      "Total training time: 1.53 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 270.20, NNZs: 490, Bias: -0.234063, T: 245520, Avg. loss: 0.135871\n",
      "Total training time: 1.57 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 270.24, NNZs: 487, Bias: -0.236044, T: 253440, Avg. loss: 0.132972\n",
      "Total training time: 1.62 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 270.27, NNZs: 484, Bias: -0.237521, T: 261360, Avg. loss: 0.133892\n",
      "Total training time: 1.67 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 270.31, NNZs: 479, Bias: -0.234908, T: 269280, Avg. loss: 0.134032\n",
      "Total training time: 1.72 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 270.34, NNZs: 480, Bias: -0.235296, T: 277200, Avg. loss: 0.132758\n",
      "Total training time: 1.77 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 270.37, NNZs: 476, Bias: -0.236021, T: 285120, Avg. loss: 0.131227\n",
      "Total training time: 1.82 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 270.40, NNZs: 471, Bias: -0.237390, T: 293040, Avg. loss: 0.131773\n",
      "Total training time: 1.87 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 270.43, NNZs: 471, Bias: -0.237035, T: 300960, Avg. loss: 0.128502\n",
      "Total training time: 1.91 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 270.46, NNZs: 467, Bias: -0.238340, T: 308880, Avg. loss: 0.131207\n",
      "Total training time: 1.96 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 270.48, NNZs: 462, Bias: -0.237723, T: 316800, Avg. loss: 0.130074\n",
      "Total training time: 2.01 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 270.51, NNZs: 462, Bias: -0.240501, T: 324720, Avg. loss: 0.127191\n",
      "Total training time: 2.06 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 270.54, NNZs: 461, Bias: -0.240201, T: 332640, Avg. loss: 0.131268\n",
      "Total training time: 2.11 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 270.56, NNZs: 454, Bias: -0.237850, T: 340560, Avg. loss: 0.126950\n",
      "Total training time: 2.16 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 270.59, NNZs: 453, Bias: -0.238435, T: 348480, Avg. loss: 0.128764\n",
      "Total training time: 2.21 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 270.61, NNZs: 449, Bias: -0.240124, T: 356400, Avg. loss: 0.127892\n",
      "Total training time: 2.26 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 270.63, NNZs: 447, Bias: -0.239852, T: 364320, Avg. loss: 0.127688\n",
      "Total training time: 2.31 seconds.\n",
      "Convergence after 46 epochs took 2.31 seconds\n",
      "-- Epoch 1\n",
      "Norm: 228.87, NNZs: 1955, Bias: -0.250161, T: 7920, Avg. loss: 4.649602\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 242.13, NNZs: 1356, Bias: -0.098635, T: 15840, Avg. loss: 0.891697\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 246.82, NNZs: 1104, Bias: -0.065214, T: 23760, Avg. loss: 0.546374\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 249.51, NNZs: 953, Bias: -0.067639, T: 31680, Avg. loss: 0.348326\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 251.08, NNZs: 856, Bias: -0.061755, T: 39600, Avg. loss: 0.300722\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 252.07, NNZs: 804, Bias: -0.072331, T: 47520, Avg. loss: 0.265502\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 252.76, NNZs: 769, Bias: -0.073589, T: 55440, Avg. loss: 0.253962\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 253.31, NNZs: 725, Bias: -0.071781, T: 63360, Avg. loss: 0.234484\n",
      "Total training time: 0.44 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 253.77, NNZs: 690, Bias: -0.071797, T: 71280, Avg. loss: 0.219757\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 254.11, NNZs: 658, Bias: -0.078353, T: 79200, Avg. loss: 0.213138\n",
      "Total training time: 0.55 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 254.40, NNZs: 634, Bias: -0.082961, T: 87120, Avg. loss: 0.209674\n",
      "Total training time: 0.60 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 254.63, NNZs: 623, Bias: -0.084860, T: 95040, Avg. loss: 0.203732\n",
      "Total training time: 0.65 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 254.83, NNZs: 606, Bias: -0.084982, T: 102960, Avg. loss: 0.206606\n",
      "Total training time: 0.70 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 255.00, NNZs: 591, Bias: -0.089490, T: 110880, Avg. loss: 0.198246\n",
      "Total training time: 0.75 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 255.13, NNZs: 578, Bias: -0.090509, T: 118800, Avg. loss: 0.203428\n",
      "Total training time: 0.80 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 255.25, NNZs: 564, Bias: -0.092048, T: 126720, Avg. loss: 0.188745\n",
      "Total training time: 0.85 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 255.36, NNZs: 552, Bias: -0.091346, T: 134640, Avg. loss: 0.187463\n",
      "Total training time: 0.90 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 255.48, NNZs: 543, Bias: -0.096229, T: 142560, Avg. loss: 0.176299\n",
      "Total training time: 0.95 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 255.58, NNZs: 534, Bias: -0.097510, T: 150480, Avg. loss: 0.184689\n",
      "Total training time: 1.00 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 255.66, NNZs: 527, Bias: -0.096860, T: 158400, Avg. loss: 0.181315\n",
      "Total training time: 1.05 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 255.74, NNZs: 521, Bias: -0.098622, T: 166320, Avg. loss: 0.174338\n",
      "Total training time: 1.10 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 255.83, NNZs: 515, Bias: -0.099145, T: 174240, Avg. loss: 0.173303\n",
      "Total training time: 1.15 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 255.89, NNZs: 503, Bias: -0.099642, T: 182160, Avg. loss: 0.170633\n",
      "Total training time: 1.20 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 255.96, NNZs: 503, Bias: -0.099108, T: 190080, Avg. loss: 0.167218\n",
      "Total training time: 1.25 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 256.04, NNZs: 498, Bias: -0.102716, T: 198000, Avg. loss: 0.165714\n",
      "Total training time: 1.31 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 256.09, NNZs: 494, Bias: -0.106575, T: 205920, Avg. loss: 0.166444\n",
      "Total training time: 1.37 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 256.15, NNZs: 489, Bias: -0.104188, T: 213840, Avg. loss: 0.165213\n",
      "Total training time: 1.42 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 256.20, NNZs: 485, Bias: -0.108223, T: 221760, Avg. loss: 0.159124\n",
      "Total training time: 1.47 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 256.25, NNZs: 481, Bias: -0.106054, T: 229680, Avg. loss: 0.162878\n",
      "Total training time: 1.52 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 256.30, NNZs: 475, Bias: -0.106935, T: 237600, Avg. loss: 0.160882\n",
      "Total training time: 1.56 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 256.34, NNZs: 469, Bias: -0.108230, T: 245520, Avg. loss: 0.164901\n",
      "Total training time: 1.61 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 256.39, NNZs: 466, Bias: -0.109848, T: 253440, Avg. loss: 0.160089\n",
      "Total training time: 1.66 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 256.44, NNZs: 462, Bias: -0.110609, T: 261360, Avg. loss: 0.158850\n",
      "Total training time: 1.71 seconds.\n",
      "Convergence after 33 epochs took 1.71 seconds\n",
      "-- Epoch 1\n",
      "Norm: 230.94, NNZs: 1832, Bias: -0.107263, T: 7920, Avg. loss: 3.131159\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 241.04, NNZs: 1303, Bias: -0.137051, T: 15840, Avg. loss: 0.647190\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 244.18, NNZs: 1108, Bias: -0.161543, T: 23760, Avg. loss: 0.439362\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 245.53, NNZs: 999, Bias: -0.171271, T: 31680, Avg. loss: 0.328432\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 246.20, NNZs: 928, Bias: -0.179330, T: 39600, Avg. loss: 0.290883\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 246.60, NNZs: 878, Bias: -0.184878, T: 47520, Avg. loss: 0.246791\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 246.90, NNZs: 840, Bias: -0.188214, T: 55440, Avg. loss: 0.233007\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 247.10, NNZs: 806, Bias: -0.196186, T: 63360, Avg. loss: 0.214592\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 247.29, NNZs: 780, Bias: -0.193475, T: 71280, Avg. loss: 0.194454\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 247.40, NNZs: 756, Bias: -0.198399, T: 79200, Avg. loss: 0.184380\n",
      "Total training time: 0.55 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 247.50, NNZs: 735, Bias: -0.201925, T: 87120, Avg. loss: 0.174298\n",
      "Total training time: 0.60 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 247.58, NNZs: 709, Bias: -0.198622, T: 95040, Avg. loss: 0.163696\n",
      "Total training time: 0.65 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 247.65, NNZs: 695, Bias: -0.190514, T: 102960, Avg. loss: 0.161823\n",
      "Total training time: 0.70 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 247.72, NNZs: 681, Bias: -0.194212, T: 110880, Avg. loss: 0.157599\n",
      "Total training time: 0.76 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 247.78, NNZs: 665, Bias: -0.188189, T: 118800, Avg. loss: 0.153880\n",
      "Total training time: 0.81 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 247.85, NNZs: 654, Bias: -0.186649, T: 126720, Avg. loss: 0.149524\n",
      "Total training time: 0.86 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 247.90, NNZs: 641, Bias: -0.186025, T: 134640, Avg. loss: 0.142481\n",
      "Total training time: 0.91 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 247.95, NNZs: 636, Bias: -0.188246, T: 142560, Avg. loss: 0.140086\n",
      "Total training time: 0.96 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 248.01, NNZs: 625, Bias: -0.186892, T: 150480, Avg. loss: 0.134409\n",
      "Total training time: 1.01 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 248.06, NNZs: 614, Bias: -0.186993, T: 158400, Avg. loss: 0.136117\n",
      "Total training time: 1.05 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 248.10, NNZs: 607, Bias: -0.186968, T: 166320, Avg. loss: 0.131964\n",
      "Total training time: 1.10 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 248.14, NNZs: 600, Bias: -0.186967, T: 174240, Avg. loss: 0.129612\n",
      "Total training time: 1.15 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 248.19, NNZs: 596, Bias: -0.186911, T: 182160, Avg. loss: 0.127377\n",
      "Total training time: 1.20 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 248.23, NNZs: 586, Bias: -0.189005, T: 190080, Avg. loss: 0.125184\n",
      "Total training time: 1.25 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 248.27, NNZs: 579, Bias: -0.188016, T: 198000, Avg. loss: 0.125610\n",
      "Total training time: 1.30 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 248.31, NNZs: 574, Bias: -0.188015, T: 205920, Avg. loss: 0.123620\n",
      "Total training time: 1.35 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 248.34, NNZs: 567, Bias: -0.188118, T: 213840, Avg. loss: 0.122937\n",
      "Total training time: 1.40 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 248.38, NNZs: 559, Bias: -0.187218, T: 221760, Avg. loss: 0.120547\n",
      "Total training time: 1.45 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 248.42, NNZs: 557, Bias: -0.186363, T: 229680, Avg. loss: 0.119638\n",
      "Total training time: 1.50 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 248.46, NNZs: 555, Bias: -0.189350, T: 237600, Avg. loss: 0.118234\n",
      "Total training time: 1.55 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 248.49, NNZs: 551, Bias: -0.190202, T: 245520, Avg. loss: 0.117262\n",
      "Total training time: 1.60 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 248.52, NNZs: 548, Bias: -0.187810, T: 253440, Avg. loss: 0.116598\n",
      "Total training time: 1.65 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 248.54, NNZs: 541, Bias: -0.186615, T: 261360, Avg. loss: 0.114399\n",
      "Total training time: 1.70 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 248.57, NNZs: 538, Bias: -0.184730, T: 269280, Avg. loss: 0.116134\n",
      "Total training time: 1.75 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 248.60, NNZs: 536, Bias: -0.183288, T: 277200, Avg. loss: 0.113728\n",
      "Total training time: 1.80 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 248.63, NNZs: 533, Bias: -0.184384, T: 285120, Avg. loss: 0.115358\n",
      "Total training time: 1.85 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 248.66, NNZs: 532, Bias: -0.185059, T: 293040, Avg. loss: 0.110382\n",
      "Total training time: 1.90 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 248.69, NNZs: 528, Bias: -0.183731, T: 300960, Avg. loss: 0.111433\n",
      "Total training time: 1.94 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 248.72, NNZs: 526, Bias: -0.184049, T: 308880, Avg. loss: 0.110765\n",
      "Total training time: 1.99 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 248.75, NNZs: 526, Bias: -0.185010, T: 316800, Avg. loss: 0.110392\n",
      "Total training time: 2.04 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 248.78, NNZs: 526, Bias: -0.184066, T: 324720, Avg. loss: 0.110690\n",
      "Total training time: 2.09 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 248.81, NNZs: 521, Bias: -0.186497, T: 332640, Avg. loss: 0.110400\n",
      "Total training time: 2.14 seconds.\n",
      "Convergence after 42 epochs took 2.14 seconds\n",
      "-- Epoch 1\n",
      "Norm: 227.11, NNZs: 1566, Bias: -0.083548, T: 7920, Avg. loss: 3.105714\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 237.71, NNZs: 1108, Bias: -0.117056, T: 15840, Avg. loss: 0.621209\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 241.08, NNZs: 933, Bias: -0.125344, T: 23760, Avg. loss: 0.389912\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 242.50, NNZs: 823, Bias: -0.141823, T: 31680, Avg. loss: 0.326873\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 243.32, NNZs: 758, Bias: -0.130351, T: 39600, Avg. loss: 0.279223\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 243.88, NNZs: 721, Bias: -0.130979, T: 47520, Avg. loss: 0.245333\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 244.26, NNZs: 687, Bias: -0.130514, T: 55440, Avg. loss: 0.240478\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 244.52, NNZs: 650, Bias: -0.133176, T: 63360, Avg. loss: 0.224638\n",
      "Total training time: 0.44 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 244.73, NNZs: 620, Bias: -0.131427, T: 71280, Avg. loss: 0.203879\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 244.86, NNZs: 601, Bias: -0.125187, T: 79200, Avg. loss: 0.197086\n",
      "Total training time: 0.54 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 244.98, NNZs: 583, Bias: -0.127454, T: 87120, Avg. loss: 0.193202\n",
      "Total training time: 0.59 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 245.08, NNZs: 569, Bias: -0.124203, T: 95040, Avg. loss: 0.187664\n",
      "Total training time: 0.65 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 245.17, NNZs: 553, Bias: -0.122292, T: 102960, Avg. loss: 0.179288\n",
      "Total training time: 0.70 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 245.25, NNZs: 540, Bias: -0.124313, T: 110880, Avg. loss: 0.177931\n",
      "Total training time: 0.75 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 245.32, NNZs: 532, Bias: -0.126045, T: 118800, Avg. loss: 0.171878\n",
      "Total training time: 0.80 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 245.39, NNZs: 525, Bias: -0.127577, T: 126720, Avg. loss: 0.165663\n",
      "Total training time: 0.85 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 245.44, NNZs: 518, Bias: -0.127578, T: 134640, Avg. loss: 0.166419\n",
      "Total training time: 0.90 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 245.50, NNZs: 508, Bias: -0.126113, T: 142560, Avg. loss: 0.165780\n",
      "Total training time: 0.94 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 245.54, NNZs: 495, Bias: -0.124831, T: 150480, Avg. loss: 0.162615\n",
      "Total training time: 0.99 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 245.59, NNZs: 489, Bias: -0.126033, T: 158400, Avg. loss: 0.159210\n",
      "Total training time: 1.04 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 245.62, NNZs: 482, Bias: -0.126630, T: 166320, Avg. loss: 0.159227\n",
      "Total training time: 1.09 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 245.66, NNZs: 480, Bias: -0.128299, T: 174240, Avg. loss: 0.151654\n",
      "Total training time: 1.14 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 245.69, NNZs: 472, Bias: -0.128233, T: 182160, Avg. loss: 0.156011\n",
      "Total training time: 1.19 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 245.73, NNZs: 470, Bias: -0.126650, T: 190080, Avg. loss: 0.152489\n",
      "Total training time: 1.24 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 245.76, NNZs: 469, Bias: -0.126197, T: 198000, Avg. loss: 0.149945\n",
      "Total training time: 1.29 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 245.79, NNZs: 466, Bias: -0.125797, T: 205920, Avg. loss: 0.148612\n",
      "Total training time: 1.33 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 245.81, NNZs: 460, Bias: -0.125368, T: 213840, Avg. loss: 0.149355\n",
      "Total training time: 1.38 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 245.84, NNZs: 459, Bias: -0.127221, T: 221760, Avg. loss: 0.146360\n",
      "Total training time: 1.43 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 245.86, NNZs: 457, Bias: -0.128113, T: 229680, Avg. loss: 0.144543\n",
      "Total training time: 1.48 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 245.88, NNZs: 454, Bias: -0.128964, T: 237600, Avg. loss: 0.145126\n",
      "Total training time: 1.53 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 245.91, NNZs: 451, Bias: -0.127327, T: 245520, Avg. loss: 0.142434\n",
      "Total training time: 1.58 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 245.93, NNZs: 448, Bias: -0.126935, T: 253440, Avg. loss: 0.144500\n",
      "Total training time: 1.63 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 245.95, NNZs: 444, Bias: -0.128092, T: 261360, Avg. loss: 0.142871\n",
      "Total training time: 1.68 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 245.98, NNZs: 443, Bias: -0.127024, T: 269280, Avg. loss: 0.140206\n",
      "Total training time: 1.73 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 246.00, NNZs: 441, Bias: -0.128101, T: 277200, Avg. loss: 0.141059\n",
      "Total training time: 1.78 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 246.02, NNZs: 437, Bias: -0.128801, T: 285120, Avg. loss: 0.138607\n",
      "Total training time: 1.82 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 246.03, NNZs: 436, Bias: -0.129829, T: 293040, Avg. loss: 0.140757\n",
      "Total training time: 1.87 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 246.05, NNZs: 434, Bias: -0.129181, T: 300960, Avg. loss: 0.138822\n",
      "Total training time: 1.92 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 246.06, NNZs: 430, Bias: -0.127231, T: 308880, Avg. loss: 0.136181\n",
      "Total training time: 1.97 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 246.08, NNZs: 428, Bias: -0.129147, T: 316800, Avg. loss: 0.138607\n",
      "Total training time: 2.02 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 246.10, NNZs: 427, Bias: -0.129147, T: 324720, Avg. loss: 0.139200\n",
      "Total training time: 2.07 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 246.11, NNZs: 427, Bias: -0.128260, T: 332640, Avg. loss: 0.136665\n",
      "Total training time: 2.11 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 246.13, NNZs: 426, Bias: -0.127685, T: 340560, Avg. loss: 0.134322\n",
      "Total training time: 2.16 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 246.15, NNZs: 424, Bias: -0.129430, T: 348480, Avg. loss: 0.135979\n",
      "Total training time: 2.21 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 246.17, NNZs: 421, Bias: -0.128594, T: 356400, Avg. loss: 0.134680\n",
      "Total training time: 2.26 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 246.18, NNZs: 421, Bias: -0.129411, T: 364320, Avg. loss: 0.136381\n",
      "Total training time: 2.31 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 246.19, NNZs: 422, Bias: -0.129679, T: 372240, Avg. loss: 0.134864\n",
      "Total training time: 2.36 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 246.21, NNZs: 422, Bias: -0.129415, T: 380160, Avg. loss: 0.133651\n",
      "Total training time: 2.41 seconds.\n",
      "Convergence after 48 epochs took 2.41 seconds\n",
      "-- Epoch 1\n",
      "Norm: 241.30, NNZs: 1502, Bias: -0.034388, T: 7920, Avg. loss: 3.379661\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 256.47, NNZs: 980, Bias: -0.067611, T: 15840, Avg. loss: 0.544249\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 262.42, NNZs: 815, Bias: -0.035070, T: 23760, Avg. loss: 0.293067\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 265.22, NNZs: 741, Bias: -0.064328, T: 31680, Avg. loss: 0.281180\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 267.24, NNZs: 690, Bias: -0.052801, T: 39600, Avg. loss: 0.186328\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 268.54, NNZs: 643, Bias: -0.054957, T: 47520, Avg. loss: 0.203782\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 269.59, NNZs: 625, Bias: -0.058915, T: 55440, Avg. loss: 0.171547\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 270.39, NNZs: 597, Bias: -0.060406, T: 63360, Avg. loss: 0.174828\n",
      "Total training time: 0.44 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 271.00, NNZs: 584, Bias: -0.058832, T: 71280, Avg. loss: 0.172005\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 271.55, NNZs: 568, Bias: -0.057805, T: 79200, Avg. loss: 0.152834\n",
      "Total training time: 0.54 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 271.96, NNZs: 554, Bias: -0.062510, T: 87120, Avg. loss: 0.175850\n",
      "Total training time: 0.59 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 272.35, NNZs: 541, Bias: -0.062551, T: 95040, Avg. loss: 0.156997\n",
      "Total training time: 0.65 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 272.74, NNZs: 528, Bias: -0.066766, T: 102960, Avg. loss: 0.130582\n",
      "Total training time: 0.70 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 273.09, NNZs: 518, Bias: -0.068692, T: 110880, Avg. loss: 0.131088\n",
      "Total training time: 0.75 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 273.41, NNZs: 511, Bias: -0.072121, T: 118800, Avg. loss: 0.130830\n",
      "Total training time: 0.80 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 273.69, NNZs: 505, Bias: -0.073625, T: 126720, Avg. loss: 0.130098\n",
      "Total training time: 0.85 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 273.95, NNZs: 500, Bias: -0.072796, T: 134640, Avg. loss: 0.129585\n",
      "Total training time: 0.90 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 274.18, NNZs: 495, Bias: -0.074884, T: 142560, Avg. loss: 0.128656\n",
      "Total training time: 0.96 seconds.\n",
      "Convergence after 18 epochs took 0.96 seconds\n",
      "-- Epoch 1\n",
      "Norm: 233.68, NNZs: 1425, Bias: -0.195662, T: 7920, Avg. loss: 3.857418\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 250.07, NNZs: 1010, Bias: -0.186281, T: 15840, Avg. loss: 0.680980\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 256.94, NNZs: 841, Bias: -0.152686, T: 23760, Avg. loss: 0.426094\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 260.82, NNZs: 760, Bias: -0.150487, T: 31680, Avg. loss: 0.295306\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 263.49, NNZs: 697, Bias: -0.155655, T: 39600, Avg. loss: 0.223917\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 265.37, NNZs: 653, Bias: -0.150801, T: 47520, Avg. loss: 0.208193\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 266.63, NNZs: 633, Bias: -0.154867, T: 55440, Avg. loss: 0.221806\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 267.67, NNZs: 613, Bias: -0.161073, T: 63360, Avg. loss: 0.198069\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 268.44, NNZs: 595, Bias: -0.153888, T: 71280, Avg. loss: 0.211290\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 269.10, NNZs: 576, Bias: -0.157851, T: 79200, Avg. loss: 0.182119\n",
      "Total training time: 0.55 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 269.64, NNZs: 558, Bias: -0.152166, T: 87120, Avg. loss: 0.189038\n",
      "Total training time: 0.60 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 270.12, NNZs: 548, Bias: -0.156741, T: 95040, Avg. loss: 0.172045\n",
      "Total training time: 0.65 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 270.58, NNZs: 535, Bias: -0.160515, T: 102960, Avg. loss: 0.163152\n",
      "Total training time: 0.70 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 270.95, NNZs: 533, Bias: -0.163978, T: 110880, Avg. loss: 0.173879\n",
      "Total training time: 0.75 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 271.24, NNZs: 527, Bias: -0.161328, T: 118800, Avg. loss: 0.178932\n",
      "Total training time: 0.81 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 271.54, NNZs: 526, Bias: -0.160595, T: 126720, Avg. loss: 0.169565\n",
      "Total training time: 0.86 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 271.78, NNZs: 519, Bias: -0.162217, T: 134640, Avg. loss: 0.173569\n",
      "Total training time: 0.92 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 272.01, NNZs: 517, Bias: -0.162973, T: 142560, Avg. loss: 0.168273\n",
      "Total training time: 0.97 seconds.\n",
      "Convergence after 18 epochs took 0.97 seconds\n",
      "-- Epoch 1\n",
      "Norm: 248.81, NNZs: 1706, Bias: -0.006494, T: 7920, Avg. loss: 3.193129\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 261.72, NNZs: 1205, Bias: -0.011399, T: 15840, Avg. loss: 0.614723\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 265.73, NNZs: 982, Bias: 0.009865, T: 23760, Avg. loss: 0.279994\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 267.54, NNZs: 873, Bias: 0.013123, T: 31680, Avg. loss: 0.299706\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 268.52, NNZs: 780, Bias: 0.012745, T: 39600, Avg. loss: 0.244781\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 269.16, NNZs: 736, Bias: 0.011032, T: 47520, Avg. loss: 0.219896\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 269.64, NNZs: 699, Bias: 0.009544, T: 55440, Avg. loss: 0.205319\n",
      "Total training time: 0.42 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 269.98, NNZs: 661, Bias: 0.016330, T: 63360, Avg. loss: 0.186000\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 270.25, NNZs: 642, Bias: 0.017670, T: 71280, Avg. loss: 0.177431\n",
      "Total training time: 0.52 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 270.47, NNZs: 620, Bias: 0.018960, T: 79200, Avg. loss: 0.159538\n",
      "Total training time: 0.57 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 270.64, NNZs: 602, Bias: 0.025033, T: 87120, Avg. loss: 0.169749\n",
      "Total training time: 0.62 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 270.78, NNZs: 582, Bias: 0.019769, T: 95040, Avg. loss: 0.158029\n",
      "Total training time: 0.67 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 270.91, NNZs: 565, Bias: 0.021780, T: 102960, Avg. loss: 0.147856\n",
      "Total training time: 0.72 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 271.02, NNZs: 554, Bias: 0.024636, T: 110880, Avg. loss: 0.151127\n",
      "Total training time: 0.77 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 271.10, NNZs: 543, Bias: 0.023714, T: 118800, Avg. loss: 0.149030\n",
      "Total training time: 0.83 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 271.18, NNZs: 529, Bias: 0.026154, T: 126720, Avg. loss: 0.142480\n",
      "Total training time: 0.88 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 271.27, NNZs: 523, Bias: 0.023805, T: 134640, Avg. loss: 0.137525\n",
      "Total training time: 0.93 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 271.33, NNZs: 514, Bias: 0.026598, T: 142560, Avg. loss: 0.133407\n",
      "Total training time: 0.98 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 271.39, NNZs: 504, Bias: 0.027903, T: 150480, Avg. loss: 0.132447\n",
      "Total training time: 1.03 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 271.45, NNZs: 495, Bias: 0.027879, T: 158400, Avg. loss: 0.133351\n",
      "Total training time: 1.08 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 271.50, NNZs: 490, Bias: 0.028478, T: 166320, Avg. loss: 0.132004\n",
      "Total training time: 1.13 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 271.55, NNZs: 485, Bias: 0.026781, T: 174240, Avg. loss: 0.132611\n",
      "Total training time: 1.18 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 271.60, NNZs: 483, Bias: 0.026825, T: 182160, Avg. loss: 0.127632\n",
      "Total training time: 1.23 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 271.64, NNZs: 476, Bias: 0.027930, T: 190080, Avg. loss: 0.129450\n",
      "Total training time: 1.28 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 271.69, NNZs: 468, Bias: 0.027406, T: 198000, Avg. loss: 0.121033\n",
      "Total training time: 1.34 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 271.73, NNZs: 465, Bias: 0.029435, T: 205920, Avg. loss: 0.126409\n",
      "Total training time: 1.41 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 271.76, NNZs: 461, Bias: 0.029442, T: 213840, Avg. loss: 0.125109\n",
      "Total training time: 1.47 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 271.80, NNZs: 457, Bias: 0.029452, T: 221760, Avg. loss: 0.120501\n",
      "Total training time: 1.53 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 271.83, NNZs: 455, Bias: 0.028624, T: 229680, Avg. loss: 0.122261\n",
      "Total training time: 1.58 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 271.87, NNZs: 448, Bias: 0.031162, T: 237600, Avg. loss: 0.117884\n",
      "Total training time: 1.63 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 271.90, NNZs: 444, Bias: 0.032362, T: 245520, Avg. loss: 0.120777\n",
      "Total training time: 1.68 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 271.93, NNZs: 441, Bias: 0.032344, T: 253440, Avg. loss: 0.118614\n",
      "Total training time: 1.73 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 271.97, NNZs: 439, Bias: 0.031563, T: 261360, Avg. loss: 0.117035\n",
      "Total training time: 1.78 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 272.00, NNZs: 437, Bias: 0.031219, T: 269280, Avg. loss: 0.118547\n",
      "Total training time: 1.83 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 272.02, NNZs: 432, Bias: 0.031598, T: 277200, Avg. loss: 0.118519\n",
      "Total training time: 1.88 seconds.\n",
      "Convergence after 35 epochs took 1.88 seconds\n",
      "-- Epoch 1\n",
      "Norm: 239.12, NNZs: 1625, Bias: -0.022462, T: 7920, Avg. loss: 3.807368\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 258.17, NNZs: 1129, Bias: -0.019304, T: 15840, Avg. loss: 0.579208\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 265.29, NNZs: 953, Bias: -0.003885, T: 23760, Avg. loss: 0.314000\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 268.47, NNZs: 876, Bias: -0.013878, T: 31680, Avg. loss: 0.332460\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 270.57, NNZs: 812, Bias: -0.000854, T: 39600, Avg. loss: 0.254217\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 272.10, NNZs: 770, Bias: -0.001879, T: 47520, Avg. loss: 0.214151\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 273.25, NNZs: 738, Bias: -0.007625, T: 55440, Avg. loss: 0.197413\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 274.13, NNZs: 708, Bias: -0.002566, T: 63360, Avg. loss: 0.175754\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 274.83, NNZs: 694, Bias: -0.002521, T: 71280, Avg. loss: 0.191595\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 275.42, NNZs: 671, Bias: -0.001133, T: 79200, Avg. loss: 0.166310\n",
      "Total training time: 0.56 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 275.97, NNZs: 652, Bias: 0.004365, T: 87120, Avg. loss: 0.147892\n",
      "Total training time: 0.61 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 276.39, NNZs: 643, Bias: 0.004062, T: 95040, Avg. loss: 0.165590\n",
      "Total training time: 0.67 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 276.76, NNZs: 634, Bias: -0.002913, T: 102960, Avg. loss: 0.158454\n",
      "Total training time: 0.73 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 277.10, NNZs: 636, Bias: 0.002795, T: 110880, Avg. loss: 0.160449\n",
      "Total training time: 0.79 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 277.40, NNZs: 633, Bias: 0.001894, T: 118800, Avg. loss: 0.160316\n",
      "Total training time: 0.84 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 277.65, NNZs: 623, Bias: 0.000389, T: 126720, Avg. loss: 0.156477\n",
      "Total training time: 0.90 seconds.\n",
      "Convergence after 16 epochs took 0.90 seconds\n",
      "-- Epoch 1\n",
      "Norm: 241.95, NNZs: 1583, Bias: -0.164088, T: 7920, Avg. loss: 3.522103\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 260.07, NNZs: 1215, Bias: -0.139430, T: 15840, Avg. loss: 0.625312\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 265.94, NNZs: 1047, Bias: -0.129901, T: 23760, Avg. loss: 0.408025\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 268.83, NNZs: 970, Bias: -0.125967, T: 31680, Avg. loss: 0.306320\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 270.50, NNZs: 906, Bias: -0.128264, T: 39600, Avg. loss: 0.275212\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 271.61, NNZs: 862, Bias: -0.125111, T: 47520, Avg. loss: 0.253121\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 272.38, NNZs: 820, Bias: -0.118987, T: 55440, Avg. loss: 0.220682\n",
      "Total training time: 0.46 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 272.99, NNZs: 794, Bias: -0.117484, T: 63360, Avg. loss: 0.217545\n",
      "Total training time: 0.52 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 273.48, NNZs: 774, Bias: -0.114608, T: 71280, Avg. loss: 0.199437\n",
      "Total training time: 0.58 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 273.90, NNZs: 752, Bias: -0.115944, T: 79200, Avg. loss: 0.188204\n",
      "Total training time: 0.64 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 274.22, NNZs: 737, Bias: -0.119409, T: 87120, Avg. loss: 0.188214\n",
      "Total training time: 0.69 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 274.46, NNZs: 721, Bias: -0.121290, T: 95040, Avg. loss: 0.182192\n",
      "Total training time: 0.76 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 274.70, NNZs: 708, Bias: -0.115193, T: 102960, Avg. loss: 0.178224\n",
      "Total training time: 0.82 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 274.90, NNZs: 699, Bias: -0.115236, T: 110880, Avg. loss: 0.175244\n",
      "Total training time: 0.89 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 275.08, NNZs: 695, Bias: -0.113556, T: 118800, Avg. loss: 0.172456\n",
      "Total training time: 0.95 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 275.25, NNZs: 682, Bias: -0.115821, T: 126720, Avg. loss: 0.158737\n",
      "Total training time: 1.01 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 275.40, NNZs: 670, Bias: -0.119530, T: 134640, Avg. loss: 0.162165\n",
      "Total training time: 1.07 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 275.53, NNZs: 668, Bias: -0.116618, T: 142560, Avg. loss: 0.161019\n",
      "Total training time: 1.13 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 275.65, NNZs: 655, Bias: -0.116552, T: 150480, Avg. loss: 0.159219\n",
      "Total training time: 1.19 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 275.76, NNZs: 652, Bias: -0.115919, T: 158400, Avg. loss: 0.159033\n",
      "Total training time: 1.25 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 275.86, NNZs: 649, Bias: -0.113545, T: 166320, Avg. loss: 0.160955\n",
      "Total training time: 1.32 seconds.\n",
      "Convergence after 21 epochs took 1.32 seconds\n",
      "-- Epoch 1\n",
      "Norm: 219.10, NNZs: 1540, Bias: -0.002281, T: 7920, Avg. loss: 3.387212\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  11 out of  11 | elapsed:   18.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 229.89, NNZs: 1097, Bias: -0.033806, T: 15840, Avg. loss: 0.797270\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 232.98, NNZs: 935, Bias: -0.030145, T: 23760, Avg. loss: 0.504139\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 234.41, NNZs: 844, Bias: -0.022376, T: 31680, Avg. loss: 0.413479\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 235.22, NNZs: 789, Bias: -0.024675, T: 39600, Avg. loss: 0.342814\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 235.73, NNZs: 735, Bias: -0.028886, T: 47520, Avg. loss: 0.303108\n",
      "Total training time: 0.41 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 236.03, NNZs: 691, Bias: -0.021526, T: 55440, Avg. loss: 0.283431\n",
      "Total training time: 0.48 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 236.24, NNZs: 670, Bias: -0.031323, T: 63360, Avg. loss: 0.264254\n",
      "Total training time: 0.55 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 236.41, NNZs: 655, Bias: -0.023926, T: 71280, Avg. loss: 0.241671\n",
      "Total training time: 0.61 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 236.54, NNZs: 632, Bias: -0.022898, T: 79200, Avg. loss: 0.234219\n",
      "Total training time: 0.67 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 236.65, NNZs: 615, Bias: -0.030037, T: 87120, Avg. loss: 0.219910\n",
      "Total training time: 0.74 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 236.75, NNZs: 596, Bias: -0.025789, T: 95040, Avg. loss: 0.208349\n",
      "Total training time: 0.81 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 236.80, NNZs: 585, Bias: -0.022804, T: 102960, Avg. loss: 0.211293\n",
      "Total training time: 0.87 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 236.85, NNZs: 568, Bias: -0.022180, T: 110880, Avg. loss: 0.207372\n",
      "Total training time: 0.93 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 236.89, NNZs: 564, Bias: -0.025807, T: 118800, Avg. loss: 0.196836\n",
      "Total training time: 0.98 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 236.94, NNZs: 561, Bias: -0.026546, T: 126720, Avg. loss: 0.192772\n",
      "Total training time: 1.04 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 236.96, NNZs: 557, Bias: -0.028789, T: 134640, Avg. loss: 0.190861\n",
      "Total training time: 1.10 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 236.99, NNZs: 553, Bias: -0.026644, T: 142560, Avg. loss: 0.186503\n",
      "Total training time: 1.16 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 237.02, NNZs: 545, Bias: -0.023931, T: 150480, Avg. loss: 0.180625\n",
      "Total training time: 1.21 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 237.04, NNZs: 536, Bias: -0.024623, T: 158400, Avg. loss: 0.181515\n",
      "Total training time: 1.27 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 237.06, NNZs: 531, Bias: -0.026460, T: 166320, Avg. loss: 0.182000\n",
      "Total training time: 1.32 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 237.08, NNZs: 525, Bias: -0.025934, T: 174240, Avg. loss: 0.173468\n",
      "Total training time: 1.37 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 237.09, NNZs: 523, Bias: -0.024915, T: 182160, Avg. loss: 0.173470\n",
      "Total training time: 1.42 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 237.12, NNZs: 516, Bias: -0.026499, T: 190080, Avg. loss: 0.170941\n",
      "Total training time: 1.48 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 237.13, NNZs: 515, Bias: -0.025996, T: 198000, Avg. loss: 0.169614\n",
      "Total training time: 1.52 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 237.15, NNZs: 511, Bias: -0.026999, T: 205920, Avg. loss: 0.167673\n",
      "Total training time: 1.58 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 237.17, NNZs: 510, Bias: -0.026047, T: 213840, Avg. loss: 0.165373\n",
      "Total training time: 1.63 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 237.19, NNZs: 510, Bias: -0.027870, T: 221760, Avg. loss: 0.163919\n",
      "Total training time: 1.68 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 237.20, NNZs: 507, Bias: -0.025255, T: 229680, Avg. loss: 0.163905\n",
      "Total training time: 1.73 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 237.21, NNZs: 501, Bias: -0.025689, T: 237600, Avg. loss: 0.164424\n",
      "Total training time: 1.78 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 237.23, NNZs: 496, Bias: -0.026482, T: 245520, Avg. loss: 0.159785\n",
      "Total training time: 1.83 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 237.24, NNZs: 495, Bias: -0.026046, T: 253440, Avg. loss: 0.159197\n",
      "Total training time: 1.89 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 237.26, NNZs: 489, Bias: -0.026039, T: 261360, Avg. loss: 0.158708\n",
      "Total training time: 1.94 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 237.27, NNZs: 486, Bias: -0.026749, T: 269280, Avg. loss: 0.155659\n",
      "Total training time: 1.99 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 237.28, NNZs: 484, Bias: -0.025283, T: 277200, Avg. loss: 0.155946\n",
      "Total training time: 2.04 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 237.30, NNZs: 484, Bias: -0.026326, T: 285120, Avg. loss: 0.155833\n",
      "Total training time: 2.09 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 237.31, NNZs: 482, Bias: -0.025293, T: 293040, Avg. loss: 0.154034\n",
      "Total training time: 2.14 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 237.33, NNZs: 481, Bias: -0.025622, T: 300960, Avg. loss: 0.153770\n",
      "Total training time: 2.19 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 237.35, NNZs: 479, Bias: -0.025279, T: 308880, Avg. loss: 0.150331\n",
      "Total training time: 2.24 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 237.36, NNZs: 475, Bias: -0.024014, T: 316800, Avg. loss: 0.151695\n",
      "Total training time: 2.29 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 237.38, NNZs: 473, Bias: -0.024662, T: 324720, Avg. loss: 0.150233\n",
      "Total training time: 2.34 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 237.39, NNZs: 473, Bias: -0.024380, T: 332640, Avg. loss: 0.149905\n",
      "Total training time: 2.41 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 237.40, NNZs: 473, Bias: -0.025255, T: 340560, Avg. loss: 0.151484\n",
      "Total training time: 2.46 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 237.42, NNZs: 466, Bias: -0.024400, T: 348480, Avg. loss: 0.149192\n",
      "Total training time: 2.52 seconds.\n",
      "Convergence after 44 epochs took 2.52 seconds\n",
      "-- Epoch 1\n",
      "Norm: 232.91, NNZs: 1804, Bias: -0.308009, T: 7920, Avg. loss: 4.124864\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 249.68, NNZs: 1326, Bias: -0.259605, T: 15840, Avg. loss: 0.627250\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 255.01, NNZs: 1122, Bias: -0.217296, T: 23760, Avg. loss: 0.328612\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 257.70, NNZs: 980, Bias: -0.212350, T: 31680, Avg. loss: 0.218736\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 259.44, NNZs: 880, Bias: -0.210718, T: 39600, Avg. loss: 0.167288\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 260.56, NNZs: 808, Bias: -0.213270, T: 47520, Avg. loss: 0.152138\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 261.45, NNZs: 761, Bias: -0.220022, T: 55440, Avg. loss: 0.137362\n",
      "Total training time: 0.43 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 262.13, NNZs: 724, Bias: -0.214179, T: 63360, Avg. loss: 0.132086\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 262.67, NNZs: 691, Bias: -0.205315, T: 71280, Avg. loss: 0.125336\n",
      "Total training time: 0.54 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 263.14, NNZs: 672, Bias: -0.213064, T: 79200, Avg. loss: 0.120775\n",
      "Total training time: 0.60 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 263.53, NNZs: 647, Bias: -0.204635, T: 87120, Avg. loss: 0.118150\n",
      "Total training time: 0.66 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 263.87, NNZs: 635, Bias: -0.207897, T: 95040, Avg. loss: 0.117026\n",
      "Total training time: 0.71 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 264.16, NNZs: 618, Bias: -0.203940, T: 102960, Avg. loss: 0.114325\n",
      "Total training time: 0.77 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 264.42, NNZs: 603, Bias: -0.202304, T: 110880, Avg. loss: 0.113403\n",
      "Total training time: 0.83 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 264.66, NNZs: 584, Bias: -0.205847, T: 118800, Avg. loss: 0.110970\n",
      "Total training time: 0.90 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 264.88, NNZs: 570, Bias: -0.204256, T: 126720, Avg. loss: 0.109507\n",
      "Total training time: 0.96 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 265.07, NNZs: 562, Bias: -0.203575, T: 134640, Avg. loss: 0.108409\n",
      "Total training time: 1.02 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 265.26, NNZs: 554, Bias: -0.211318, T: 142560, Avg. loss: 0.107770\n",
      "Total training time: 1.09 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 265.43, NNZs: 551, Bias: -0.206419, T: 150480, Avg. loss: 0.107420\n",
      "Total training time: 1.15 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 265.58, NNZs: 546, Bias: -0.204525, T: 158400, Avg. loss: 0.107318\n",
      "Total training time: 1.21 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 265.73, NNZs: 541, Bias: -0.208214, T: 166320, Avg. loss: 0.107479\n",
      "Total training time: 1.26 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 265.86, NNZs: 534, Bias: -0.209331, T: 174240, Avg. loss: 0.110064\n",
      "Total training time: 1.32 seconds.\n",
      "Convergence after 22 epochs took 1.33 seconds\n",
      "-- Epoch 1\n",
      "Norm: 235.71, NNZs: 1576, Bias: -0.146710, T: 7920, Avg. loss: 3.559114\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 253.05, NNZs: 1177, Bias: -0.158050, T: 15840, Avg. loss: 0.723291\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 258.15, NNZs: 1007, Bias: -0.179131, T: 23760, Avg. loss: 0.458305\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 260.89, NNZs: 904, Bias: -0.199059, T: 31680, Avg. loss: 0.334296\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 262.42, NNZs: 843, Bias: -0.175738, T: 39600, Avg. loss: 0.299532\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 263.44, NNZs: 799, Bias: -0.184913, T: 47520, Avg. loss: 0.275490\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 264.20, NNZs: 756, Bias: -0.184535, T: 55440, Avg. loss: 0.242150\n",
      "Total training time: 0.43 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 264.74, NNZs: 727, Bias: -0.190715, T: 63360, Avg. loss: 0.222844\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 265.11, NNZs: 708, Bias: -0.193213, T: 71280, Avg. loss: 0.215936\n",
      "Total training time: 0.54 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 265.44, NNZs: 689, Bias: -0.189227, T: 79200, Avg. loss: 0.199255\n",
      "Total training time: 0.60 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 265.68, NNZs: 668, Bias: -0.193067, T: 87120, Avg. loss: 0.198743\n",
      "Total training time: 0.65 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 265.92, NNZs: 658, Bias: -0.198340, T: 95040, Avg. loss: 0.185694\n",
      "Total training time: 0.71 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 266.09, NNZs: 646, Bias: -0.197394, T: 102960, Avg. loss: 0.181717\n",
      "Total training time: 0.77 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 266.24, NNZs: 624, Bias: -0.201024, T: 110880, Avg. loss: 0.179853\n",
      "Total training time: 0.84 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 266.39, NNZs: 606, Bias: -0.200045, T: 118800, Avg. loss: 0.167400\n",
      "Total training time: 0.90 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 266.52, NNZs: 596, Bias: -0.201591, T: 126720, Avg. loss: 0.168252\n",
      "Total training time: 0.95 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 266.64, NNZs: 587, Bias: -0.204575, T: 134640, Avg. loss: 0.162463\n",
      "Total training time: 1.00 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 266.73, NNZs: 576, Bias: -0.203782, T: 142560, Avg. loss: 0.157023\n",
      "Total training time: 1.06 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 266.82, NNZs: 571, Bias: -0.205161, T: 150480, Avg. loss: 0.158383\n",
      "Total training time: 1.11 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 266.90, NNZs: 561, Bias: -0.205758, T: 158400, Avg. loss: 0.156114\n",
      "Total training time: 1.16 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 266.98, NNZs: 555, Bias: -0.205718, T: 166320, Avg. loss: 0.153741\n",
      "Total training time: 1.22 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 267.05, NNZs: 553, Bias: -0.204606, T: 174240, Avg. loss: 0.153609\n",
      "Total training time: 1.27 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 267.11, NNZs: 548, Bias: -0.205800, T: 182160, Avg. loss: 0.151188\n",
      "Total training time: 1.32 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 267.17, NNZs: 542, Bias: -0.205861, T: 190080, Avg. loss: 0.147942\n",
      "Total training time: 1.38 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 267.23, NNZs: 540, Bias: -0.209961, T: 198000, Avg. loss: 0.148733\n",
      "Total training time: 1.44 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 267.29, NNZs: 537, Bias: -0.207562, T: 205920, Avg. loss: 0.141563\n",
      "Total training time: 1.49 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 267.34, NNZs: 533, Bias: -0.209956, T: 213840, Avg. loss: 0.146578\n",
      "Total training time: 1.55 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 267.39, NNZs: 529, Bias: -0.211785, T: 221760, Avg. loss: 0.144787\n",
      "Total training time: 1.60 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 267.44, NNZs: 519, Bias: -0.211775, T: 229680, Avg. loss: 0.140468\n",
      "Total training time: 1.66 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 267.48, NNZs: 514, Bias: -0.209650, T: 237600, Avg. loss: 0.141081\n",
      "Total training time: 1.71 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 267.53, NNZs: 510, Bias: -0.212939, T: 245520, Avg. loss: 0.143472\n",
      "Total training time: 1.77 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 267.57, NNZs: 510, Bias: -0.213322, T: 253440, Avg. loss: 0.139004\n",
      "Total training time: 1.83 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 267.61, NNZs: 510, Bias: -0.216763, T: 261360, Avg. loss: 0.138333\n",
      "Total training time: 1.88 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 267.65, NNZs: 504, Bias: -0.214149, T: 269280, Avg. loss: 0.136615\n",
      "Total training time: 1.94 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 267.69, NNZs: 502, Bias: -0.215969, T: 277200, Avg. loss: 0.139340\n",
      "Total training time: 2.00 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 267.72, NNZs: 500, Bias: -0.215279, T: 285120, Avg. loss: 0.135837\n",
      "Total training time: 2.05 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 267.75, NNZs: 498, Bias: -0.215977, T: 293040, Avg. loss: 0.137466\n",
      "Total training time: 2.10 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 267.79, NNZs: 497, Bias: -0.216637, T: 300960, Avg. loss: 0.135090\n",
      "Total training time: 2.16 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 267.82, NNZs: 495, Bias: -0.217620, T: 308880, Avg. loss: 0.134533\n",
      "Total training time: 2.21 seconds.\n",
      "Convergence after 39 epochs took 2.21 seconds\n",
      "-- Epoch 1\n",
      "Norm: 232.99, NNZs: 1953, Bias: -0.256210, T: 7920, Avg. loss: 4.709921\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 245.84, NNZs: 1378, Bias: -0.081559, T: 15840, Avg. loss: 0.982966\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 250.70, NNZs: 1096, Bias: -0.056652, T: 23760, Avg. loss: 0.498439\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 253.36, NNZs: 963, Bias: -0.064195, T: 31680, Avg. loss: 0.331137\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 255.21, NNZs: 866, Bias: -0.066429, T: 39600, Avg. loss: 0.280883\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 256.39, NNZs: 798, Bias: -0.070189, T: 47520, Avg. loss: 0.250476\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 257.18, NNZs: 750, Bias: -0.081413, T: 55440, Avg. loss: 0.264365\n",
      "Total training time: 0.43 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 257.74, NNZs: 721, Bias: -0.072853, T: 63360, Avg. loss: 0.244337\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 258.21, NNZs: 679, Bias: -0.079811, T: 71280, Avg. loss: 0.218285\n",
      "Total training time: 0.54 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 258.53, NNZs: 657, Bias: -0.080263, T: 79200, Avg. loss: 0.230941\n",
      "Total training time: 0.60 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 258.84, NNZs: 632, Bias: -0.081394, T: 87120, Avg. loss: 0.208897\n",
      "Total training time: 0.66 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 259.10, NNZs: 614, Bias: -0.080565, T: 95040, Avg. loss: 0.207454\n",
      "Total training time: 0.71 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 259.31, NNZs: 592, Bias: -0.085864, T: 102960, Avg. loss: 0.209962\n",
      "Total training time: 0.77 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 259.50, NNZs: 580, Bias: -0.090499, T: 110880, Avg. loss: 0.202575\n",
      "Total training time: 0.82 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 259.66, NNZs: 575, Bias: -0.093977, T: 118800, Avg. loss: 0.196185\n",
      "Total training time: 0.88 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 259.81, NNZs: 563, Bias: -0.092355, T: 126720, Avg. loss: 0.193592\n",
      "Total training time: 0.93 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 259.95, NNZs: 555, Bias: -0.094630, T: 134640, Avg. loss: 0.187805\n",
      "Total training time: 0.98 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 260.08, NNZs: 550, Bias: -0.098876, T: 142560, Avg. loss: 0.184859\n",
      "Total training time: 1.04 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 260.18, NNZs: 542, Bias: -0.100819, T: 150480, Avg. loss: 0.180328\n",
      "Total training time: 1.10 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 260.28, NNZs: 539, Bias: -0.101413, T: 158400, Avg. loss: 0.183126\n",
      "Total training time: 1.16 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 260.36, NNZs: 531, Bias: -0.105033, T: 166320, Avg. loss: 0.181952\n",
      "Total training time: 1.21 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 260.44, NNZs: 528, Bias: -0.100865, T: 174240, Avg. loss: 0.177758\n",
      "Total training time: 1.27 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 260.52, NNZs: 522, Bias: -0.103041, T: 182160, Avg. loss: 0.170508\n",
      "Total training time: 1.32 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 260.59, NNZs: 515, Bias: -0.103621, T: 190080, Avg. loss: 0.171208\n",
      "Total training time: 1.38 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 260.64, NNZs: 509, Bias: -0.106272, T: 198000, Avg. loss: 0.174106\n",
      "Total training time: 1.43 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 260.72, NNZs: 505, Bias: -0.107208, T: 205920, Avg. loss: 0.170926\n",
      "Total training time: 1.49 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 260.78, NNZs: 499, Bias: -0.109098, T: 213840, Avg. loss: 0.167874\n",
      "Total training time: 1.54 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 260.84, NNZs: 494, Bias: -0.109505, T: 221760, Avg. loss: 0.165581\n",
      "Total training time: 1.60 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 260.88, NNZs: 490, Bias: -0.110436, T: 229680, Avg. loss: 0.169395\n",
      "Total training time: 1.65 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 260.93, NNZs: 486, Bias: -0.110024, T: 237600, Avg. loss: 0.166781\n",
      "Total training time: 1.71 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 260.98, NNZs: 480, Bias: -0.110866, T: 245520, Avg. loss: 0.164588\n",
      "Total training time: 1.76 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 261.02, NNZs: 476, Bias: -0.112097, T: 253440, Avg. loss: 0.165825\n",
      "Total training time: 1.82 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 261.08, NNZs: 473, Bias: -0.114041, T: 261360, Avg. loss: 0.162046\n",
      "Total training time: 1.88 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 261.12, NNZs: 471, Bias: -0.116650, T: 269280, Avg. loss: 0.160663\n",
      "Total training time: 1.94 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 261.16, NNZs: 468, Bias: -0.116249, T: 277200, Avg. loss: 0.164902\n",
      "Total training time: 2.00 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 261.19, NNZs: 469, Bias: -0.118348, T: 285120, Avg. loss: 0.164435\n",
      "Total training time: 2.06 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 261.23, NNZs: 465, Bias: -0.117639, T: 293040, Avg. loss: 0.161757\n",
      "Total training time: 2.11 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 261.26, NNZs: 464, Bias: -0.118627, T: 300960, Avg. loss: 0.163267\n",
      "Total training time: 2.17 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 261.29, NNZs: 464, Bias: -0.118941, T: 308880, Avg. loss: 0.161724\n",
      "Total training time: 2.22 seconds.\n",
      "Convergence after 39 epochs took 2.22 seconds\n",
      "-- Epoch 1\n",
      "Norm: 235.68, NNZs: 1962, Bias: -0.082014, T: 7920, Avg. loss: 3.237441\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 246.08, NNZs: 1403, Bias: -0.126636, T: 15840, Avg. loss: 0.607919\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 249.39, NNZs: 1192, Bias: -0.144693, T: 23760, Avg. loss: 0.426568\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 250.81, NNZs: 1088, Bias: -0.180682, T: 31680, Avg. loss: 0.330195\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 251.65, NNZs: 1004, Bias: -0.180272, T: 39600, Avg. loss: 0.278944\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 252.11, NNZs: 942, Bias: -0.182047, T: 47520, Avg. loss: 0.252462\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 252.43, NNZs: 910, Bias: -0.179837, T: 55440, Avg. loss: 0.235077\n",
      "Total training time: 0.43 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 252.64, NNZs: 877, Bias: -0.183083, T: 63360, Avg. loss: 0.209247\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 252.79, NNZs: 836, Bias: -0.177552, T: 71280, Avg. loss: 0.191214\n",
      "Total training time: 0.55 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 252.91, NNZs: 804, Bias: -0.179970, T: 79200, Avg. loss: 0.182538\n",
      "Total training time: 0.60 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 253.06, NNZs: 784, Bias: -0.184299, T: 87120, Avg. loss: 0.173863\n",
      "Total training time: 0.66 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 253.16, NNZs: 747, Bias: -0.181013, T: 95040, Avg. loss: 0.163696\n",
      "Total training time: 0.72 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 253.24, NNZs: 729, Bias: -0.180924, T: 102960, Avg. loss: 0.159362\n",
      "Total training time: 0.78 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 253.33, NNZs: 709, Bias: -0.184569, T: 110880, Avg. loss: 0.154104\n",
      "Total training time: 0.84 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 253.40, NNZs: 701, Bias: -0.182757, T: 118800, Avg. loss: 0.151509\n",
      "Total training time: 0.89 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 253.47, NNZs: 685, Bias: -0.179535, T: 126720, Avg. loss: 0.145708\n",
      "Total training time: 0.94 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 253.53, NNZs: 669, Bias: -0.178165, T: 134640, Avg. loss: 0.138959\n",
      "Total training time: 1.00 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 253.59, NNZs: 663, Bias: -0.179043, T: 142560, Avg. loss: 0.134888\n",
      "Total training time: 1.06 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 253.66, NNZs: 654, Bias: -0.175058, T: 150480, Avg. loss: 0.131767\n",
      "Total training time: 1.12 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 253.73, NNZs: 648, Bias: -0.175196, T: 158400, Avg. loss: 0.129608\n",
      "Total training time: 1.18 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 253.78, NNZs: 638, Bias: -0.179476, T: 166320, Avg. loss: 0.130252\n",
      "Total training time: 1.23 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 253.83, NNZs: 627, Bias: -0.178901, T: 174240, Avg. loss: 0.126873\n",
      "Total training time: 1.29 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 253.87, NNZs: 621, Bias: -0.181079, T: 182160, Avg. loss: 0.125303\n",
      "Total training time: 1.35 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 253.92, NNZs: 611, Bias: -0.178933, T: 190080, Avg. loss: 0.122466\n",
      "Total training time: 1.40 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 253.96, NNZs: 605, Bias: -0.179439, T: 198000, Avg. loss: 0.122740\n",
      "Total training time: 1.46 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 254.01, NNZs: 597, Bias: -0.179969, T: 205920, Avg. loss: 0.120071\n",
      "Total training time: 1.51 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 254.05, NNZs: 592, Bias: -0.180460, T: 213840, Avg. loss: 0.120487\n",
      "Total training time: 1.56 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 254.09, NNZs: 589, Bias: -0.181867, T: 221760, Avg. loss: 0.118528\n",
      "Total training time: 1.62 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 254.13, NNZs: 587, Bias: -0.180553, T: 229680, Avg. loss: 0.115320\n",
      "Total training time: 1.68 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 254.17, NNZs: 584, Bias: -0.180504, T: 237600, Avg. loss: 0.117505\n",
      "Total training time: 1.74 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 254.20, NNZs: 576, Bias: -0.181775, T: 245520, Avg. loss: 0.113692\n",
      "Total training time: 1.79 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 254.24, NNZs: 574, Bias: -0.181387, T: 253440, Avg. loss: 0.112643\n",
      "Total training time: 1.85 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 254.27, NNZs: 564, Bias: -0.179828, T: 261360, Avg. loss: 0.114342\n",
      "Total training time: 1.90 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 254.30, NNZs: 561, Bias: -0.179107, T: 269280, Avg. loss: 0.113138\n",
      "Total training time: 1.96 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 254.34, NNZs: 558, Bias: -0.178038, T: 277200, Avg. loss: 0.112975\n",
      "Total training time: 2.02 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 254.37, NNZs: 553, Bias: -0.179119, T: 285120, Avg. loss: 0.110212\n",
      "Total training time: 2.07 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 254.40, NNZs: 551, Bias: -0.180466, T: 293040, Avg. loss: 0.111711\n",
      "Total training time: 2.12 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 254.43, NNZs: 548, Bias: -0.179801, T: 300960, Avg. loss: 0.110408\n",
      "Total training time: 2.18 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 254.46, NNZs: 549, Bias: -0.181751, T: 308880, Avg. loss: 0.109301\n",
      "Total training time: 2.24 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 254.50, NNZs: 543, Bias: -0.183328, T: 316800, Avg. loss: 0.107227\n",
      "Total training time: 2.29 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 254.53, NNZs: 542, Bias: -0.182994, T: 324720, Avg. loss: 0.109122\n",
      "Total training time: 2.35 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 254.56, NNZs: 540, Bias: -0.184533, T: 332640, Avg. loss: 0.108583\n",
      "Total training time: 2.40 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 254.58, NNZs: 539, Bias: -0.185728, T: 340560, Avg. loss: 0.109186\n",
      "Total training time: 2.46 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 254.61, NNZs: 537, Bias: -0.184864, T: 348480, Avg. loss: 0.105597\n",
      "Total training time: 2.51 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 254.64, NNZs: 537, Bias: -0.184308, T: 356400, Avg. loss: 0.106908\n",
      "Total training time: 2.56 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 254.66, NNZs: 531, Bias: -0.184337, T: 364320, Avg. loss: 0.106869\n",
      "Total training time: 2.62 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 254.69, NNZs: 532, Bias: -0.185422, T: 372240, Avg. loss: 0.107279\n",
      "Total training time: 2.67 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 254.71, NNZs: 529, Bias: -0.185432, T: 380160, Avg. loss: 0.106439\n",
      "Total training time: 2.72 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 254.74, NNZs: 523, Bias: -0.185182, T: 388080, Avg. loss: 0.106193\n",
      "Total training time: 2.77 seconds.\n",
      "Convergence after 49 epochs took 2.77 seconds\n",
      "-- Epoch 1\n",
      "Norm: 216.83, NNZs: 1564, Bias: 0.016439, T: 7920, Avg. loss: 2.991515\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 227.69, NNZs: 1083, Bias: -0.080505, T: 15840, Avg. loss: 0.664505\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 231.51, NNZs: 914, Bias: -0.087691, T: 23760, Avg. loss: 0.421340\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 233.02, NNZs: 825, Bias: -0.097374, T: 31680, Avg. loss: 0.338613\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 233.91, NNZs: 736, Bias: -0.089966, T: 39600, Avg. loss: 0.289205\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 234.51, NNZs: 683, Bias: -0.092337, T: 47520, Avg. loss: 0.253324\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 234.94, NNZs: 651, Bias: -0.090667, T: 55440, Avg. loss: 0.237334\n",
      "Total training time: 0.44 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 235.24, NNZs: 617, Bias: -0.097172, T: 63360, Avg. loss: 0.224095\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 235.48, NNZs: 593, Bias: -0.098117, T: 71280, Avg. loss: 0.210663\n",
      "Total training time: 0.55 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 235.68, NNZs: 573, Bias: -0.090457, T: 79200, Avg. loss: 0.200734\n",
      "Total training time: 0.60 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 235.82, NNZs: 555, Bias: -0.091813, T: 87120, Avg. loss: 0.193983\n",
      "Total training time: 0.65 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 235.96, NNZs: 538, Bias: -0.091941, T: 95040, Avg. loss: 0.190814\n",
      "Total training time: 0.70 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 236.10, NNZs: 527, Bias: -0.088173, T: 102960, Avg. loss: 0.177494\n",
      "Total training time: 0.76 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 236.22, NNZs: 516, Bias: -0.089322, T: 110880, Avg. loss: 0.175378\n",
      "Total training time: 0.81 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 236.30, NNZs: 504, Bias: -0.094495, T: 118800, Avg. loss: 0.173094\n",
      "Total training time: 0.87 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 236.38, NNZs: 496, Bias: -0.092868, T: 126720, Avg. loss: 0.166194\n",
      "Total training time: 0.93 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 236.45, NNZs: 491, Bias: -0.096554, T: 134640, Avg. loss: 0.172487\n",
      "Total training time: 1.00 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 236.52, NNZs: 485, Bias: -0.093825, T: 142560, Avg. loss: 0.164901\n",
      "Total training time: 1.06 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 236.57, NNZs: 474, Bias: -0.093213, T: 150480, Avg. loss: 0.162371\n",
      "Total training time: 1.12 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 236.63, NNZs: 470, Bias: -0.096973, T: 158400, Avg. loss: 0.159033\n",
      "Total training time: 1.19 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 236.68, NNZs: 470, Bias: -0.095672, T: 166320, Avg. loss: 0.158573\n",
      "Total training time: 1.25 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 236.73, NNZs: 467, Bias: -0.095028, T: 174240, Avg. loss: 0.156200\n",
      "Total training time: 1.31 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 236.77, NNZs: 466, Bias: -0.097205, T: 182160, Avg. loss: 0.153443\n",
      "Total training time: 1.37 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 236.81, NNZs: 460, Bias: -0.096120, T: 190080, Avg. loss: 0.152537\n",
      "Total training time: 1.43 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 236.85, NNZs: 455, Bias: -0.095581, T: 198000, Avg. loss: 0.147434\n",
      "Total training time: 1.48 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 236.90, NNZs: 449, Bias: -0.093200, T: 205920, Avg. loss: 0.147596\n",
      "Total training time: 1.55 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 236.94, NNZs: 446, Bias: -0.097431, T: 213840, Avg. loss: 0.148780\n",
      "Total training time: 1.62 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 236.98, NNZs: 443, Bias: -0.096045, T: 221760, Avg. loss: 0.141413\n",
      "Total training time: 1.69 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 237.01, NNZs: 439, Bias: -0.097350, T: 229680, Avg. loss: 0.146219\n",
      "Total training time: 1.75 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 237.04, NNZs: 440, Bias: -0.096087, T: 237600, Avg. loss: 0.146331\n",
      "Total training time: 1.82 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 237.07, NNZs: 436, Bias: -0.097730, T: 245520, Avg. loss: 0.141096\n",
      "Total training time: 1.88 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 237.10, NNZs: 434, Bias: -0.096524, T: 253440, Avg. loss: 0.143735\n",
      "Total training time: 1.94 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 237.13, NNZs: 433, Bias: -0.096917, T: 261360, Avg. loss: 0.141867\n",
      "Total training time: 2.01 seconds.\n",
      "Convergence after 33 epochs took 2.01 seconds\n",
      "-- Epoch 1\n",
      "Norm: 230.61, NNZs: 1489, Bias: -0.057808, T: 7920, Avg. loss: 3.358436\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 244.60, NNZs: 1009, Bias: -0.051626, T: 15840, Avg. loss: 0.642803\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 250.67, NNZs: 860, Bias: -0.023948, T: 23760, Avg. loss: 0.303529\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 253.25, NNZs: 772, Bias: -0.011178, T: 31680, Avg. loss: 0.292767\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 255.03, NNZs: 703, Bias: -0.018252, T: 39600, Avg. loss: 0.245545\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 256.15, NNZs: 659, Bias: -0.021921, T: 47520, Avg. loss: 0.259016\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 257.01, NNZs: 622, Bias: -0.020221, T: 55440, Avg. loss: 0.210874\n",
      "Total training time: 0.46 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 257.67, NNZs: 594, Bias: -0.016786, T: 63360, Avg. loss: 0.208881\n",
      "Total training time: 0.53 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 258.21, NNZs: 579, Bias: -0.019833, T: 71280, Avg. loss: 0.191358\n",
      "Total training time: 0.59 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 258.63, NNZs: 560, Bias: -0.019780, T: 79200, Avg. loss: 0.189821\n",
      "Total training time: 0.66 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 258.97, NNZs: 552, Bias: -0.020711, T: 87120, Avg. loss: 0.193161\n",
      "Total training time: 0.73 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 259.28, NNZs: 537, Bias: -0.021580, T: 95040, Avg. loss: 0.185160\n",
      "Total training time: 0.79 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 259.57, NNZs: 525, Bias: -0.016665, T: 102960, Avg. loss: 0.170140\n",
      "Total training time: 0.85 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 259.80, NNZs: 513, Bias: -0.018556, T: 110880, Avg. loss: 0.171653\n",
      "Total training time: 0.91 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 259.99, NNZs: 502, Bias: -0.016759, T: 118800, Avg. loss: 0.173116\n",
      "Total training time: 0.97 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 260.16, NNZs: 494, Bias: -0.017555, T: 126720, Avg. loss: 0.164567\n",
      "Total training time: 1.02 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 260.33, NNZs: 491, Bias: -0.019771, T: 134640, Avg. loss: 0.161508\n",
      "Total training time: 1.08 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 260.49, NNZs: 488, Bias: -0.018358, T: 142560, Avg. loss: 0.156843\n",
      "Total training time: 1.13 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 260.63, NNZs: 481, Bias: -0.019015, T: 150480, Avg. loss: 0.157708\n",
      "Total training time: 1.18 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 260.76, NNZs: 481, Bias: -0.018424, T: 158400, Avg. loss: 0.155972\n",
      "Total training time: 1.24 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 260.88, NNZs: 475, Bias: -0.020230, T: 166320, Avg. loss: 0.154504\n",
      "Total training time: 1.31 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 261.00, NNZs: 469, Bias: -0.020198, T: 174240, Avg. loss: 0.153380\n",
      "Total training time: 1.38 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 261.10, NNZs: 465, Bias: -0.020108, T: 182160, Avg. loss: 0.155937\n",
      "Total training time: 1.44 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 261.20, NNZs: 469, Bias: -0.018004, T: 190080, Avg. loss: 0.148261\n",
      "Total training time: 1.50 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 261.29, NNZs: 460, Bias: -0.021089, T: 198000, Avg. loss: 0.148920\n",
      "Total training time: 1.56 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 261.38, NNZs: 455, Bias: -0.020097, T: 205920, Avg. loss: 0.145841\n",
      "Total training time: 1.61 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 261.47, NNZs: 452, Bias: -0.021551, T: 213840, Avg. loss: 0.141463\n",
      "Total training time: 1.66 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 261.54, NNZs: 449, Bias: -0.021031, T: 221760, Avg. loss: 0.149978\n",
      "Total training time: 1.72 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 261.63, NNZs: 444, Bias: -0.021054, T: 229680, Avg. loss: 0.137340\n",
      "Total training time: 1.77 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 261.70, NNZs: 445, Bias: -0.022346, T: 237600, Avg. loss: 0.140134\n",
      "Total training time: 1.82 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 261.78, NNZs: 442, Bias: -0.019106, T: 245520, Avg. loss: 0.138973\n",
      "Total training time: 1.88 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 261.85, NNZs: 443, Bias: -0.019943, T: 253440, Avg. loss: 0.143562\n",
      "Total training time: 1.93 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 261.91, NNZs: 441, Bias: -0.021123, T: 261360, Avg. loss: 0.143383\n",
      "Total training time: 1.99 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 261.97, NNZs: 440, Bias: -0.022625, T: 269280, Avg. loss: 0.142888\n",
      "Total training time: 2.04 seconds.\n",
      "Convergence after 34 epochs took 2.04 seconds\n",
      "-- Epoch 1\n",
      "Norm: 229.33, NNZs: 1292, Bias: -0.173106, T: 7920, Avg. loss: 3.610474\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 248.61, NNZs: 945, Bias: -0.152812, T: 15840, Avg. loss: 0.737598\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 257.07, NNZs: 817, Bias: -0.142959, T: 23760, Avg. loss: 0.378772\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 261.46, NNZs: 735, Bias: -0.140008, T: 31680, Avg. loss: 0.323887\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 264.35, NNZs: 681, Bias: -0.140001, T: 39600, Avg. loss: 0.238250\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 266.51, NNZs: 639, Bias: -0.133328, T: 47520, Avg. loss: 0.197467\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 268.02, NNZs: 627, Bias: -0.139070, T: 55440, Avg. loss: 0.202969\n",
      "Total training time: 0.43 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 269.13, NNZs: 604, Bias: -0.143868, T: 63360, Avg. loss: 0.214474\n",
      "Total training time: 0.48 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 270.01, NNZs: 594, Bias: -0.139369, T: 71280, Avg. loss: 0.202638\n",
      "Total training time: 0.54 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 270.75, NNZs: 587, Bias: -0.142036, T: 79200, Avg. loss: 0.195674\n",
      "Total training time: 0.59 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 271.38, NNZs: 583, Bias: -0.140839, T: 87120, Avg. loss: 0.186114\n",
      "Total training time: 0.64 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 271.88, NNZs: 573, Bias: -0.142098, T: 95040, Avg. loss: 0.191612\n",
      "Total training time: 0.70 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 272.34, NNZs: 566, Bias: -0.148927, T: 102960, Avg. loss: 0.176518\n",
      "Total training time: 0.75 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 272.73, NNZs: 563, Bias: -0.146036, T: 110880, Avg. loss: 0.186957\n",
      "Total training time: 0.80 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 273.08, NNZs: 560, Bias: -0.147779, T: 118800, Avg. loss: 0.179529\n",
      "Total training time: 0.86 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 273.38, NNZs: 556, Bias: -0.149314, T: 126720, Avg. loss: 0.172242\n",
      "Total training time: 0.91 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 273.63, NNZs: 555, Bias: -0.150053, T: 134640, Avg. loss: 0.185300\n",
      "Total training time: 0.97 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 273.87, NNZs: 550, Bias: -0.149978, T: 142560, Avg. loss: 0.174673\n",
      "Total training time: 1.03 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 274.10, NNZs: 546, Bias: -0.147280, T: 150480, Avg. loss: 0.169106\n",
      "Total training time: 1.09 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 274.29, NNZs: 547, Bias: -0.149321, T: 158400, Avg. loss: 0.175612\n",
      "Total training time: 1.16 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 274.48, NNZs: 543, Bias: -0.150017, T: 166320, Avg. loss: 0.172450\n",
      "Total training time: 1.22 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 274.65, NNZs: 540, Bias: -0.152873, T: 174240, Avg. loss: 0.164280\n",
      "Total training time: 1.29 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 274.81, NNZs: 535, Bias: -0.153372, T: 182160, Avg. loss: 0.168599\n",
      "Total training time: 1.35 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 274.96, NNZs: 533, Bias: -0.153838, T: 190080, Avg. loss: 0.168308\n",
      "Total training time: 1.41 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 275.09, NNZs: 532, Bias: -0.155283, T: 198000, Avg. loss: 0.166211\n",
      "Total training time: 1.46 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 275.21, NNZs: 531, Bias: -0.152318, T: 205920, Avg. loss: 0.171299\n",
      "Total training time: 1.51 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 275.33, NNZs: 529, Bias: -0.152819, T: 213840, Avg. loss: 0.164014\n",
      "Total training time: 1.57 seconds.\n",
      "Convergence after 27 epochs took 1.57 seconds\n",
      "-- Epoch 1\n",
      "Norm: 256.77, NNZs: 1914, Bias: -0.019617, T: 7920, Avg. loss: 3.352289\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 268.43, NNZs: 1302, Bias: -0.004761, T: 15840, Avg. loss: 0.537256\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 271.98, NNZs: 1087, Bias: 0.015399, T: 23760, Avg. loss: 0.365943\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 273.66, NNZs: 958, Bias: 0.004257, T: 31680, Avg. loss: 0.299542\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 274.60, NNZs: 872, Bias: 0.012305, T: 39600, Avg. loss: 0.255229\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 275.09, NNZs: 822, Bias: 0.016659, T: 47520, Avg. loss: 0.241512\n",
      "Total training time: 0.41 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 275.47, NNZs: 767, Bias: 0.009335, T: 55440, Avg. loss: 0.216739\n",
      "Total training time: 0.46 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 275.78, NNZs: 731, Bias: 0.019380, T: 63360, Avg. loss: 0.200336\n",
      "Total training time: 0.52 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 275.98, NNZs: 702, Bias: 0.020801, T: 71280, Avg. loss: 0.188453\n",
      "Total training time: 0.58 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 276.13, NNZs: 677, Bias: 0.019637, T: 79200, Avg. loss: 0.183407\n",
      "Total training time: 0.65 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 276.26, NNZs: 656, Bias: 0.025394, T: 87120, Avg. loss: 0.163444\n",
      "Total training time: 0.71 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 276.38, NNZs: 640, Bias: 0.021181, T: 95040, Avg. loss: 0.161687\n",
      "Total training time: 0.77 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 276.46, NNZs: 620, Bias: 0.024255, T: 102960, Avg. loss: 0.156422\n",
      "Total training time: 0.83 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 276.52, NNZs: 606, Bias: 0.027955, T: 110880, Avg. loss: 0.156332\n",
      "Total training time: 0.88 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 276.59, NNZs: 590, Bias: 0.028811, T: 118800, Avg. loss: 0.153731\n",
      "Total training time: 0.94 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 276.65, NNZs: 579, Bias: 0.029558, T: 126720, Avg. loss: 0.148073\n",
      "Total training time: 1.00 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 276.72, NNZs: 572, Bias: 0.031748, T: 134640, Avg. loss: 0.145871\n",
      "Total training time: 1.06 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 276.76, NNZs: 569, Bias: 0.032434, T: 142560, Avg. loss: 0.145542\n",
      "Total training time: 1.12 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 276.81, NNZs: 559, Bias: 0.035066, T: 150480, Avg. loss: 0.138021\n",
      "Total training time: 1.18 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 276.84, NNZs: 554, Bias: 0.035077, T: 158400, Avg. loss: 0.141074\n",
      "Total training time: 1.23 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 276.88, NNZs: 543, Bias: 0.033217, T: 166320, Avg. loss: 0.136896\n",
      "Total training time: 1.29 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 276.92, NNZs: 535, Bias: 0.034304, T: 174240, Avg. loss: 0.131152\n",
      "Total training time: 1.34 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 276.95, NNZs: 530, Bias: 0.034871, T: 182160, Avg. loss: 0.135246\n",
      "Total training time: 1.39 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 276.99, NNZs: 519, Bias: 0.035921, T: 190080, Avg. loss: 0.137655\n",
      "Total training time: 1.45 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 277.02, NNZs: 513, Bias: 0.036403, T: 198000, Avg. loss: 0.130423\n",
      "Total training time: 1.50 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 277.04, NNZs: 512, Bias: 0.034970, T: 205920, Avg. loss: 0.129871\n",
      "Total training time: 1.56 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 277.06, NNZs: 509, Bias: 0.036896, T: 213840, Avg. loss: 0.131172\n",
      "Total training time: 1.61 seconds.\n",
      "Convergence after 27 epochs took 1.61 seconds\n",
      "-- Epoch 1\n",
      "Norm: 239.08, NNZs: 1450, Bias: -0.065253, T: 7920, Avg. loss: 3.609359\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 258.59, NNZs: 1032, Bias: -0.021231, T: 15840, Avg. loss: 0.584294\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 266.26, NNZs: 900, Bias: -0.019371, T: 23760, Avg. loss: 0.301375\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 270.17, NNZs: 830, Bias: -0.016959, T: 31680, Avg. loss: 0.275610\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 272.69, NNZs: 769, Bias: -0.023066, T: 39600, Avg. loss: 0.222081\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 274.35, NNZs: 737, Bias: -0.012105, T: 47520, Avg. loss: 0.213820\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 275.48, NNZs: 715, Bias: -0.024954, T: 55440, Avg. loss: 0.237025\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 276.46, NNZs: 700, Bias: -0.024778, T: 63360, Avg. loss: 0.175690\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 277.19, NNZs: 693, Bias: -0.023029, T: 71280, Avg. loss: 0.189697\n",
      "Total training time: 0.51 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 277.81, NNZs: 677, Bias: -0.025464, T: 79200, Avg. loss: 0.171375\n",
      "Total training time: 0.58 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 278.37, NNZs: 667, Bias: -0.018499, T: 87120, Avg. loss: 0.151284\n",
      "Total training time: 0.64 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 278.79, NNZs: 659, Bias: -0.022720, T: 95040, Avg. loss: 0.180638\n",
      "Total training time: 0.70 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 279.20, NNZs: 650, Bias: -0.023796, T: 102960, Avg. loss: 0.149218\n",
      "Total training time: 0.76 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 279.55, NNZs: 635, Bias: -0.025495, T: 110880, Avg. loss: 0.156647\n",
      "Total training time: 0.81 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 279.88, NNZs: 632, Bias: -0.023804, T: 118800, Avg. loss: 0.143850\n",
      "Total training time: 0.86 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 280.16, NNZs: 625, Bias: -0.024580, T: 126720, Avg. loss: 0.150495\n",
      "Total training time: 0.91 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 280.41, NNZs: 609, Bias: -0.022276, T: 134640, Avg. loss: 0.148990\n",
      "Total training time: 0.97 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 280.62, NNZs: 602, Bias: -0.020970, T: 142560, Avg. loss: 0.144372\n",
      "Total training time: 1.02 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 280.83, NNZs: 598, Bias: -0.025743, T: 150480, Avg. loss: 0.145490\n",
      "Total training time: 1.07 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 281.05, NNZs: 594, Bias: -0.027073, T: 158400, Avg. loss: 0.125116\n",
      "Total training time: 1.13 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 281.22, NNZs: 590, Bias: -0.023362, T: 166320, Avg. loss: 0.145043\n",
      "Total training time: 1.18 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 281.38, NNZs: 582, Bias: -0.024028, T: 174240, Avg. loss: 0.141051\n",
      "Total training time: 1.23 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 281.55, NNZs: 581, Bias: -0.026344, T: 182160, Avg. loss: 0.133078\n",
      "Total training time: 1.28 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 281.70, NNZs: 580, Bias: -0.025807, T: 190080, Avg. loss: 0.138406\n",
      "Total training time: 1.33 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 281.84, NNZs: 576, Bias: -0.025829, T: 198000, Avg. loss: 0.135047\n",
      "Total training time: 1.38 seconds.\n",
      "Convergence after 25 epochs took 1.38 seconds\n",
      "-- Epoch 1\n",
      "Norm: 253.11, NNZs: 1871, Bias: -0.177688, T: 7920, Avg. loss: 3.786942\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 268.38, NNZs: 1343, Bias: -0.137395, T: 15840, Avg. loss: 0.565216\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 273.57, NNZs: 1121, Bias: -0.130994, T: 23760, Avg. loss: 0.355835\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 276.15, NNZs: 1024, Bias: -0.123555, T: 31680, Avg. loss: 0.300995\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 277.73, NNZs: 961, Bias: -0.109712, T: 39600, Avg. loss: 0.249564\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 278.76, NNZs: 899, Bias: -0.107533, T: 47520, Avg. loss: 0.209885\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 279.52, NNZs: 855, Bias: -0.105646, T: 55440, Avg. loss: 0.203904\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 280.11, NNZs: 821, Bias: -0.107251, T: 63360, Avg. loss: 0.194893\n",
      "Total training time: 0.51 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 280.57, NNZs: 797, Bias: -0.099836, T: 71280, Avg. loss: 0.185514\n",
      "Total training time: 0.57 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 280.94, NNZs: 779, Bias: -0.103813, T: 79200, Avg. loss: 0.186016\n",
      "Total training time: 0.62 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 281.23, NNZs: 758, Bias: -0.097803, T: 87120, Avg. loss: 0.176212\n",
      "Total training time: 0.68 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 281.51, NNZs: 739, Bias: -0.099820, T: 95040, Avg. loss: 0.161548\n",
      "Total training time: 0.73 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 281.74, NNZs: 725, Bias: -0.095693, T: 102960, Avg. loss: 0.162777\n",
      "Total training time: 0.78 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 281.96, NNZs: 717, Bias: -0.092066, T: 110880, Avg. loss: 0.152088\n",
      "Total training time: 0.83 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 282.14, NNZs: 705, Bias: -0.093930, T: 118800, Avg. loss: 0.154180\n",
      "Total training time: 0.89 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 282.31, NNZs: 687, Bias: -0.097038, T: 126720, Avg. loss: 0.144942\n",
      "Total training time: 0.94 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 282.45, NNZs: 682, Bias: -0.093919, T: 134640, Avg. loss: 0.154333\n",
      "Total training time: 0.99 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 282.60, NNZs: 675, Bias: -0.094558, T: 142560, Avg. loss: 0.144835\n",
      "Total training time: 1.04 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 282.73, NNZs: 667, Bias: -0.095125, T: 150480, Avg. loss: 0.146589\n",
      "Total training time: 1.09 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 282.84, NNZs: 662, Bias: -0.091963, T: 158400, Avg. loss: 0.149195\n",
      "Total training time: 1.14 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 282.95, NNZs: 656, Bias: -0.091407, T: 166320, Avg. loss: 0.144931\n",
      "Total training time: 1.19 seconds.\n",
      "Convergence after 21 epochs took 1.19 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  11 out of  11 | elapsed:   20.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 212.53, NNZs: 1451, Bias: -0.014363, T: 7920, Avg. loss: 3.233850\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 224.24, NNZs: 1060, Bias: -0.043802, T: 15840, Avg. loss: 0.808680\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 227.52, NNZs: 910, Bias: -0.040681, T: 23760, Avg. loss: 0.502669\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 228.73, NNZs: 804, Bias: -0.036658, T: 31680, Avg. loss: 0.411821\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 229.35, NNZs: 734, Bias: -0.042481, T: 39600, Avg. loss: 0.362285\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 229.83, NNZs: 702, Bias: -0.041831, T: 47520, Avg. loss: 0.305211\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 230.19, NNZs: 668, Bias: -0.038704, T: 55440, Avg. loss: 0.283837\n",
      "Total training time: 0.44 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 230.41, NNZs: 643, Bias: -0.051763, T: 63360, Avg. loss: 0.257350\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 230.57, NNZs: 623, Bias: -0.049530, T: 71280, Avg. loss: 0.244389\n",
      "Total training time: 0.56 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 230.67, NNZs: 597, Bias: -0.044132, T: 79200, Avg. loss: 0.236656\n",
      "Total training time: 0.62 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 230.74, NNZs: 582, Bias: -0.041732, T: 87120, Avg. loss: 0.226655\n",
      "Total training time: 0.68 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 230.83, NNZs: 568, Bias: -0.042873, T: 95040, Avg. loss: 0.212740\n",
      "Total training time: 0.74 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 230.88, NNZs: 557, Bias: -0.040690, T: 102960, Avg. loss: 0.216552\n",
      "Total training time: 0.80 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 230.91, NNZs: 547, Bias: -0.039846, T: 110880, Avg. loss: 0.204583\n",
      "Total training time: 0.87 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 230.95, NNZs: 533, Bias: -0.042575, T: 118800, Avg. loss: 0.200282\n",
      "Total training time: 0.93 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 230.97, NNZs: 528, Bias: -0.042471, T: 126720, Avg. loss: 0.195491\n",
      "Total training time: 1.00 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 231.01, NNZs: 514, Bias: -0.040979, T: 134640, Avg. loss: 0.192983\n",
      "Total training time: 1.06 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 231.03, NNZs: 507, Bias: -0.037471, T: 142560, Avg. loss: 0.189010\n",
      "Total training time: 1.12 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 231.06, NNZs: 498, Bias: -0.039559, T: 150480, Avg. loss: 0.183965\n",
      "Total training time: 1.18 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 231.09, NNZs: 497, Bias: -0.040847, T: 158400, Avg. loss: 0.181520\n",
      "Total training time: 1.25 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 231.11, NNZs: 494, Bias: -0.041981, T: 166320, Avg. loss: 0.178625\n",
      "Total training time: 1.31 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 231.13, NNZs: 490, Bias: -0.039028, T: 174240, Avg. loss: 0.172727\n",
      "Total training time: 1.37 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 231.14, NNZs: 484, Bias: -0.039100, T: 182160, Avg. loss: 0.174624\n",
      "Total training time: 1.43 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 231.15, NNZs: 479, Bias: -0.039065, T: 190080, Avg. loss: 0.173906\n",
      "Total training time: 1.50 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 231.16, NNZs: 477, Bias: -0.041135, T: 198000, Avg. loss: 0.169998\n",
      "Total training time: 1.56 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 231.17, NNZs: 473, Bias: -0.039594, T: 205920, Avg. loss: 0.169805\n",
      "Total training time: 1.62 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 231.19, NNZs: 472, Bias: -0.038639, T: 213840, Avg. loss: 0.166427\n",
      "Total training time: 1.68 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 231.21, NNZs: 466, Bias: -0.038225, T: 221760, Avg. loss: 0.163525\n",
      "Total training time: 1.74 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 231.22, NNZs: 465, Bias: -0.035641, T: 229680, Avg. loss: 0.165086\n",
      "Total training time: 1.79 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 231.23, NNZs: 465, Bias: -0.039021, T: 237600, Avg. loss: 0.164043\n",
      "Total training time: 1.85 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 231.24, NNZs: 462, Bias: -0.038584, T: 245520, Avg. loss: 0.160501\n",
      "Total training time: 1.90 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 231.26, NNZs: 460, Bias: -0.036597, T: 253440, Avg. loss: 0.160890\n",
      "Total training time: 1.95 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 231.27, NNZs: 457, Bias: -0.038485, T: 261360, Avg. loss: 0.160663\n",
      "Total training time: 2.01 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 231.28, NNZs: 457, Bias: -0.037376, T: 269280, Avg. loss: 0.161317\n",
      "Total training time: 2.07 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 231.29, NNZs: 452, Bias: -0.037358, T: 277200, Avg. loss: 0.159062\n",
      "Total training time: 2.13 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 231.30, NNZs: 451, Bias: -0.037350, T: 285120, Avg. loss: 0.155900\n",
      "Total training time: 2.19 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 231.31, NNZs: 452, Bias: -0.037002, T: 293040, Avg. loss: 0.155397\n",
      "Total training time: 2.25 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 231.32, NNZs: 450, Bias: -0.036327, T: 300960, Avg. loss: 0.156942\n",
      "Total training time: 2.30 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 231.33, NNZs: 446, Bias: -0.037602, T: 308880, Avg. loss: 0.153717\n",
      "Total training time: 2.37 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 231.34, NNZs: 441, Bias: -0.035696, T: 316800, Avg. loss: 0.154211\n",
      "Total training time: 2.44 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 231.35, NNZs: 443, Bias: -0.036315, T: 324720, Avg. loss: 0.152759\n",
      "Total training time: 2.50 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 231.36, NNZs: 438, Bias: -0.034812, T: 332640, Avg. loss: 0.152779\n",
      "Total training time: 2.56 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 231.37, NNZs: 439, Bias: -0.036270, T: 340560, Avg. loss: 0.152089\n",
      "Total training time: 2.62 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 231.39, NNZs: 438, Bias: -0.036567, T: 348480, Avg. loss: 0.152250\n",
      "Total training time: 2.68 seconds.\n",
      "Convergence after 44 epochs took 2.68 seconds\n",
      "-- Epoch 1\n",
      "Norm: 237.01, NNZs: 1942, Bias: -0.344521, T: 7920, Avg. loss: 4.508682\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 253.28, NNZs: 1455, Bias: -0.274991, T: 15840, Avg. loss: 0.730477\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 258.39, NNZs: 1241, Bias: -0.239826, T: 23760, Avg. loss: 0.379035\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 260.70, NNZs: 1081, Bias: -0.226327, T: 31680, Avg. loss: 0.262293\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 262.04, NNZs: 990, Bias: -0.230339, T: 39600, Avg. loss: 0.242305\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 262.99, NNZs: 906, Bias: -0.229763, T: 47520, Avg. loss: 0.197081\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 263.62, NNZs: 857, Bias: -0.225469, T: 55440, Avg. loss: 0.189452\n",
      "Total training time: 0.41 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 264.12, NNZs: 823, Bias: -0.220128, T: 63360, Avg. loss: 0.169464\n",
      "Total training time: 0.46 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 264.52, NNZs: 788, Bias: -0.215532, T: 71280, Avg. loss: 0.146769\n",
      "Total training time: 0.51 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 264.85, NNZs: 752, Bias: -0.218197, T: 79200, Avg. loss: 0.151769\n",
      "Total training time: 0.57 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 265.10, NNZs: 734, Bias: -0.217120, T: 87120, Avg. loss: 0.155208\n",
      "Total training time: 0.62 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 265.33, NNZs: 711, Bias: -0.216030, T: 95040, Avg. loss: 0.146327\n",
      "Total training time: 0.68 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 265.52, NNZs: 688, Bias: -0.213900, T: 102960, Avg. loss: 0.133429\n",
      "Total training time: 0.73 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 265.69, NNZs: 674, Bias: -0.214772, T: 110880, Avg. loss: 0.135501\n",
      "Total training time: 0.78 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 265.84, NNZs: 657, Bias: -0.214721, T: 118800, Avg. loss: 0.132634\n",
      "Total training time: 0.84 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 265.98, NNZs: 643, Bias: -0.212292, T: 126720, Avg. loss: 0.130940\n",
      "Total training time: 0.90 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 266.11, NNZs: 629, Bias: -0.211546, T: 134640, Avg. loss: 0.128786\n",
      "Total training time: 0.97 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 266.23, NNZs: 613, Bias: -0.212981, T: 142560, Avg. loss: 0.127873\n",
      "Total training time: 1.03 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 266.33, NNZs: 601, Bias: -0.212386, T: 150480, Avg. loss: 0.128627\n",
      "Total training time: 1.09 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 266.43, NNZs: 596, Bias: -0.211781, T: 158400, Avg. loss: 0.128197\n",
      "Total training time: 1.14 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 266.52, NNZs: 592, Bias: -0.213116, T: 166320, Avg. loss: 0.129557\n",
      "Total training time: 1.19 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 266.60, NNZs: 585, Bias: -0.214838, T: 174240, Avg. loss: 0.124427\n",
      "Total training time: 1.24 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 266.68, NNZs: 580, Bias: -0.213113, T: 182160, Avg. loss: 0.119719\n",
      "Total training time: 1.29 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 266.74, NNZs: 580, Bias: -0.216291, T: 190080, Avg. loss: 0.125762\n",
      "Total training time: 1.34 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 266.80, NNZs: 576, Bias: -0.213725, T: 198000, Avg. loss: 0.126245\n",
      "Total training time: 1.39 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 266.86, NNZs: 568, Bias: -0.213233, T: 205920, Avg. loss: 0.121540\n",
      "Total training time: 1.45 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 266.93, NNZs: 562, Bias: -0.214144, T: 213840, Avg. loss: 0.119407\n",
      "Total training time: 1.50 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 266.98, NNZs: 559, Bias: -0.215480, T: 221760, Avg. loss: 0.122196\n",
      "Total training time: 1.55 seconds.\n",
      "Convergence after 28 epochs took 1.55 seconds\n",
      "-- Epoch 1\n",
      "Norm: 236.75, NNZs: 1460, Bias: -0.146680, T: 7920, Avg. loss: 3.475813\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 255.39, NNZs: 1083, Bias: -0.178034, T: 15840, Avg. loss: 0.626019\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 260.96, NNZs: 931, Bias: -0.178447, T: 23760, Avg. loss: 0.415973\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 263.86, NNZs: 858, Bias: -0.194763, T: 31680, Avg. loss: 0.349219\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 265.51, NNZs: 795, Bias: -0.188343, T: 39600, Avg. loss: 0.272132\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 266.64, NNZs: 748, Bias: -0.192605, T: 47520, Avg. loss: 0.257362\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 267.39, NNZs: 710, Bias: -0.194246, T: 55440, Avg. loss: 0.229190\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 267.95, NNZs: 682, Bias: -0.197405, T: 63360, Avg. loss: 0.215088\n",
      "Total training time: 0.44 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 268.40, NNZs: 656, Bias: -0.201809, T: 71280, Avg. loss: 0.203087\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 268.75, NNZs: 640, Bias: -0.201776, T: 79200, Avg. loss: 0.191401\n",
      "Total training time: 0.55 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 269.06, NNZs: 626, Bias: -0.208060, T: 87120, Avg. loss: 0.180729\n",
      "Total training time: 0.60 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 269.31, NNZs: 612, Bias: -0.209961, T: 95040, Avg. loss: 0.178708\n",
      "Total training time: 0.66 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 269.53, NNZs: 603, Bias: -0.209944, T: 102960, Avg. loss: 0.172038\n",
      "Total training time: 0.73 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 269.71, NNZs: 582, Bias: -0.210890, T: 110880, Avg. loss: 0.171707\n",
      "Total training time: 0.79 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 269.86, NNZs: 572, Bias: -0.211753, T: 118800, Avg. loss: 0.165354\n",
      "Total training time: 0.85 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 269.99, NNZs: 562, Bias: -0.216586, T: 126720, Avg. loss: 0.161634\n",
      "Total training time: 0.91 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 270.12, NNZs: 552, Bias: -0.215020, T: 134640, Avg. loss: 0.154986\n",
      "Total training time: 0.96 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 270.22, NNZs: 543, Bias: -0.217130, T: 142560, Avg. loss: 0.156251\n",
      "Total training time: 1.01 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 270.31, NNZs: 537, Bias: -0.217105, T: 150480, Avg. loss: 0.155265\n",
      "Total training time: 1.06 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 270.40, NNZs: 532, Bias: -0.220918, T: 158400, Avg. loss: 0.152172\n",
      "Total training time: 1.11 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 270.49, NNZs: 524, Bias: -0.220901, T: 166320, Avg. loss: 0.150971\n",
      "Total training time: 1.16 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 270.56, NNZs: 522, Bias: -0.220872, T: 174240, Avg. loss: 0.148344\n",
      "Total training time: 1.21 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 270.63, NNZs: 517, Bias: -0.223114, T: 182160, Avg. loss: 0.147559\n",
      "Total training time: 1.26 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 270.70, NNZs: 514, Bias: -0.223181, T: 190080, Avg. loss: 0.142424\n",
      "Total training time: 1.31 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 270.76, NNZs: 510, Bias: -0.226216, T: 198000, Avg. loss: 0.144561\n",
      "Total training time: 1.36 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 270.83, NNZs: 506, Bias: -0.225756, T: 205920, Avg. loss: 0.142179\n",
      "Total training time: 1.41 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 270.88, NNZs: 503, Bias: -0.228091, T: 213840, Avg. loss: 0.140698\n",
      "Total training time: 1.46 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 270.93, NNZs: 497, Bias: -0.228982, T: 221760, Avg. loss: 0.142787\n",
      "Total training time: 1.50 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 270.99, NNZs: 495, Bias: -0.230301, T: 229680, Avg. loss: 0.139989\n",
      "Total training time: 1.55 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 271.04, NNZs: 490, Bias: -0.229466, T: 237600, Avg. loss: 0.137077\n",
      "Total training time: 1.60 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 271.08, NNZs: 487, Bias: -0.231956, T: 245520, Avg. loss: 0.138304\n",
      "Total training time: 1.65 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 271.12, NNZs: 482, Bias: -0.231957, T: 253440, Avg. loss: 0.136285\n",
      "Total training time: 1.70 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 271.17, NNZs: 481, Bias: -0.232700, T: 261360, Avg. loss: 0.138797\n",
      "Total training time: 1.76 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 271.21, NNZs: 478, Bias: -0.232720, T: 269280, Avg. loss: 0.135636\n",
      "Total training time: 1.82 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 271.25, NNZs: 477, Bias: -0.234170, T: 277200, Avg. loss: 0.136665\n",
      "Total training time: 1.89 seconds.\n",
      "Convergence after 35 epochs took 1.89 seconds\n",
      "-- Epoch 1\n",
      "Norm: 236.07, NNZs: 2017, Bias: -0.209630, T: 7920, Avg. loss: 4.601918\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 246.90, NNZs: 1393, Bias: -0.082796, T: 15840, Avg. loss: 0.996636\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 251.46, NNZs: 1121, Bias: -0.075773, T: 23760, Avg. loss: 0.536164\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 253.77, NNZs: 968, Bias: -0.047758, T: 31680, Avg. loss: 0.392129\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 255.31, NNZs: 888, Bias: -0.059202, T: 39600, Avg. loss: 0.317798\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 256.31, NNZs: 811, Bias: -0.065178, T: 47520, Avg. loss: 0.274082\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 256.97, NNZs: 765, Bias: -0.063344, T: 55440, Avg. loss: 0.269874\n",
      "Total training time: 0.44 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 257.47, NNZs: 740, Bias: -0.063373, T: 63360, Avg. loss: 0.251746\n",
      "Total training time: 0.51 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 257.91, NNZs: 718, Bias: -0.063371, T: 71280, Avg. loss: 0.225902\n",
      "Total training time: 0.56 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 258.22, NNZs: 683, Bias: -0.063800, T: 79200, Avg. loss: 0.229434\n",
      "Total training time: 0.62 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 258.52, NNZs: 656, Bias: -0.065994, T: 87120, Avg. loss: 0.196891\n",
      "Total training time: 0.67 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 258.78, NNZs: 636, Bias: -0.063990, T: 95040, Avg. loss: 0.197076\n",
      "Total training time: 0.73 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 259.00, NNZs: 617, Bias: -0.071016, T: 102960, Avg. loss: 0.191965\n",
      "Total training time: 0.79 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 259.18, NNZs: 603, Bias: -0.071067, T: 110880, Avg. loss: 0.192392\n",
      "Total training time: 0.84 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 259.36, NNZs: 588, Bias: -0.073627, T: 118800, Avg. loss: 0.184645\n",
      "Total training time: 0.90 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 259.49, NNZs: 583, Bias: -0.076887, T: 126720, Avg. loss: 0.195199\n",
      "Total training time: 0.95 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 259.62, NNZs: 575, Bias: -0.079875, T: 134640, Avg. loss: 0.187270\n",
      "Total training time: 1.00 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 259.73, NNZs: 569, Bias: -0.078463, T: 142560, Avg. loss: 0.185385\n",
      "Total training time: 1.06 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 259.84, NNZs: 563, Bias: -0.077756, T: 150480, Avg. loss: 0.177970\n",
      "Total training time: 1.11 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 259.94, NNZs: 558, Bias: -0.078447, T: 158400, Avg. loss: 0.175172\n",
      "Total training time: 1.16 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 260.03, NNZs: 549, Bias: -0.081473, T: 166320, Avg. loss: 0.176115\n",
      "Total training time: 1.22 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 260.12, NNZs: 542, Bias: -0.081423, T: 174240, Avg. loss: 0.167440\n",
      "Total training time: 1.27 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 260.20, NNZs: 532, Bias: -0.084137, T: 182160, Avg. loss: 0.164818\n",
      "Total training time: 1.33 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 260.28, NNZs: 524, Bias: -0.083067, T: 190080, Avg. loss: 0.171631\n",
      "Total training time: 1.38 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 260.35, NNZs: 518, Bias: -0.084128, T: 198000, Avg. loss: 0.166623\n",
      "Total training time: 1.43 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 260.43, NNZs: 514, Bias: -0.086030, T: 205920, Avg. loss: 0.163298\n",
      "Total training time: 1.48 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 260.50, NNZs: 510, Bias: -0.086543, T: 213840, Avg. loss: 0.168753\n",
      "Total training time: 1.53 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 260.56, NNZs: 507, Bias: -0.088814, T: 221760, Avg. loss: 0.166200\n",
      "Total training time: 1.58 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 260.62, NNZs: 501, Bias: -0.087952, T: 229680, Avg. loss: 0.166144\n",
      "Total training time: 1.63 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 260.67, NNZs: 496, Bias: -0.088813, T: 237600, Avg. loss: 0.165481\n",
      "Total training time: 1.70 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 260.72, NNZs: 493, Bias: -0.089287, T: 245520, Avg. loss: 0.167007\n",
      "Total training time: 1.76 seconds.\n",
      "Convergence after 31 epochs took 1.76 seconds\n",
      "-- Epoch 1\n",
      "Norm: 233.15, NNZs: 1816, Bias: -0.117061, T: 7920, Avg. loss: 3.152822\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 243.53, NNZs: 1361, Bias: -0.133616, T: 15840, Avg. loss: 0.669477\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 246.22, NNZs: 1155, Bias: -0.155991, T: 23760, Avg. loss: 0.419573\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 247.48, NNZs: 1053, Bias: -0.184481, T: 31680, Avg. loss: 0.334610\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 248.09, NNZs: 990, Bias: -0.179940, T: 39600, Avg. loss: 0.286414\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 248.51, NNZs: 945, Bias: -0.184102, T: 47520, Avg. loss: 0.248596\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 248.83, NNZs: 887, Bias: -0.175095, T: 55440, Avg. loss: 0.222365\n",
      "Total training time: 0.43 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 249.03, NNZs: 849, Bias: -0.171213, T: 63360, Avg. loss: 0.199094\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 249.18, NNZs: 821, Bias: -0.169858, T: 71280, Avg. loss: 0.190976\n",
      "Total training time: 0.54 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 249.30, NNZs: 793, Bias: -0.170762, T: 79200, Avg. loss: 0.182999\n",
      "Total training time: 0.60 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 249.40, NNZs: 767, Bias: -0.162305, T: 87120, Avg. loss: 0.174256\n",
      "Total training time: 0.66 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 249.50, NNZs: 746, Bias: -0.162270, T: 95040, Avg. loss: 0.161168\n",
      "Total training time: 0.71 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 249.56, NNZs: 729, Bias: -0.158287, T: 102960, Avg. loss: 0.158722\n",
      "Total training time: 0.77 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 249.63, NNZs: 715, Bias: -0.160122, T: 110880, Avg. loss: 0.155872\n",
      "Total training time: 0.82 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 249.68, NNZs: 701, Bias: -0.157544, T: 118800, Avg. loss: 0.148531\n",
      "Total training time: 0.87 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 249.73, NNZs: 690, Bias: -0.155125, T: 126720, Avg. loss: 0.146245\n",
      "Total training time: 0.93 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 249.78, NNZs: 675, Bias: -0.156854, T: 134640, Avg. loss: 0.137968\n",
      "Total training time: 0.98 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 249.83, NNZs: 660, Bias: -0.152673, T: 142560, Avg. loss: 0.136791\n",
      "Total training time: 1.04 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 249.87, NNZs: 648, Bias: -0.148583, T: 150480, Avg. loss: 0.134892\n",
      "Total training time: 1.10 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 249.92, NNZs: 642, Bias: -0.146726, T: 158400, Avg. loss: 0.133214\n",
      "Total training time: 1.15 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 249.95, NNZs: 631, Bias: -0.145615, T: 166320, Avg. loss: 0.130459\n",
      "Total training time: 1.20 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 249.99, NNZs: 625, Bias: -0.145641, T: 174240, Avg. loss: 0.128050\n",
      "Total training time: 1.26 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 250.03, NNZs: 621, Bias: -0.146200, T: 182160, Avg. loss: 0.127737\n",
      "Total training time: 1.31 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 250.06, NNZs: 618, Bias: -0.146793, T: 190080, Avg. loss: 0.123541\n",
      "Total training time: 1.37 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 250.10, NNZs: 611, Bias: -0.146766, T: 198000, Avg. loss: 0.123002\n",
      "Total training time: 1.42 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 250.13, NNZs: 608, Bias: -0.143339, T: 205920, Avg. loss: 0.122888\n",
      "Total training time: 1.48 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 250.17, NNZs: 601, Bias: -0.142939, T: 213840, Avg. loss: 0.121283\n",
      "Total training time: 1.54 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 250.21, NNZs: 591, Bias: -0.141116, T: 221760, Avg. loss: 0.117745\n",
      "Total training time: 1.60 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 250.24, NNZs: 584, Bias: -0.140647, T: 229680, Avg. loss: 0.116111\n",
      "Total training time: 1.66 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 250.27, NNZs: 580, Bias: -0.136777, T: 237600, Avg. loss: 0.116817\n",
      "Total training time: 1.71 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 250.31, NNZs: 576, Bias: -0.136384, T: 245520, Avg. loss: 0.114243\n",
      "Total training time: 1.77 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 250.34, NNZs: 572, Bias: -0.133586, T: 253440, Avg. loss: 0.116002\n",
      "Total training time: 1.83 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 250.36, NNZs: 563, Bias: -0.134679, T: 261360, Avg. loss: 0.114264\n",
      "Total training time: 1.89 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 250.39, NNZs: 563, Bias: -0.132451, T: 269280, Avg. loss: 0.113882\n",
      "Total training time: 1.94 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 250.42, NNZs: 560, Bias: -0.132093, T: 277200, Avg. loss: 0.111228\n",
      "Total training time: 1.99 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 250.45, NNZs: 560, Bias: -0.131029, T: 285120, Avg. loss: 0.111393\n",
      "Total training time: 2.05 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 250.48, NNZs: 557, Bias: -0.131344, T: 293040, Avg. loss: 0.110570\n",
      "Total training time: 2.10 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 250.50, NNZs: 553, Bias: -0.131020, T: 300960, Avg. loss: 0.111700\n",
      "Total training time: 2.16 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 250.53, NNZs: 552, Bias: -0.129720, T: 308880, Avg. loss: 0.112035\n",
      "Total training time: 2.21 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 250.56, NNZs: 551, Bias: -0.130045, T: 316800, Avg. loss: 0.105355\n",
      "Total training time: 2.27 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 250.59, NNZs: 547, Bias: -0.129748, T: 324720, Avg. loss: 0.108787\n",
      "Total training time: 2.32 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 250.61, NNZs: 546, Bias: -0.130045, T: 332640, Avg. loss: 0.107594\n",
      "Total training time: 2.38 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 250.63, NNZs: 541, Bias: -0.128886, T: 340560, Avg. loss: 0.107308\n",
      "Total training time: 2.43 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 250.66, NNZs: 539, Bias: -0.127730, T: 348480, Avg. loss: 0.105763\n",
      "Total training time: 2.48 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 250.69, NNZs: 535, Bias: -0.127169, T: 356400, Avg. loss: 0.106605\n",
      "Total training time: 2.54 seconds.\n",
      "Convergence after 45 epochs took 2.54 seconds\n",
      "-- Epoch 1\n",
      "Norm: 214.53, NNZs: 1560, Bias: -0.028628, T: 7920, Avg. loss: 3.008815\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 225.23, NNZs: 1098, Bias: -0.128155, T: 15840, Avg. loss: 0.660884\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 228.98, NNZs: 902, Bias: -0.139806, T: 23760, Avg. loss: 0.390212\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 230.66, NNZs: 794, Bias: -0.144722, T: 31680, Avg. loss: 0.346708\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 231.74, NNZs: 733, Bias: -0.134107, T: 39600, Avg. loss: 0.289024\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 232.46, NNZs: 697, Bias: -0.137456, T: 47520, Avg. loss: 0.254614\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 232.92, NNZs: 672, Bias: -0.137892, T: 55440, Avg. loss: 0.234888\n",
      "Total training time: 0.43 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 233.28, NNZs: 650, Bias: -0.142881, T: 63360, Avg. loss: 0.223744\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 233.52, NNZs: 622, Bias: -0.142892, T: 71280, Avg. loss: 0.213061\n",
      "Total training time: 0.55 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 233.71, NNZs: 598, Bias: -0.139428, T: 79200, Avg. loss: 0.199297\n",
      "Total training time: 0.61 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 233.88, NNZs: 578, Bias: -0.144399, T: 87120, Avg. loss: 0.196074\n",
      "Total training time: 0.66 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 234.03, NNZs: 560, Bias: -0.144369, T: 95040, Avg. loss: 0.184194\n",
      "Total training time: 0.72 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 234.16, NNZs: 544, Bias: -0.139611, T: 102960, Avg. loss: 0.176365\n",
      "Total training time: 0.78 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 234.27, NNZs: 528, Bias: -0.141620, T: 110880, Avg. loss: 0.171318\n",
      "Total training time: 0.82 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 234.34, NNZs: 520, Bias: -0.144273, T: 118800, Avg. loss: 0.174171\n",
      "Total training time: 0.88 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 234.42, NNZs: 508, Bias: -0.141852, T: 126720, Avg. loss: 0.167700\n",
      "Total training time: 0.93 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 234.48, NNZs: 500, Bias: -0.151632, T: 134640, Avg. loss: 0.168786\n",
      "Total training time: 1.00 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 234.55, NNZs: 497, Bias: -0.142442, T: 142560, Avg. loss: 0.164366\n",
      "Total training time: 1.05 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 234.61, NNZs: 488, Bias: -0.144474, T: 150480, Avg. loss: 0.160929\n",
      "Total training time: 1.10 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 234.66, NNZs: 478, Bias: -0.148854, T: 158400, Avg. loss: 0.157163\n",
      "Total training time: 1.16 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 234.73, NNZs: 477, Bias: -0.148781, T: 166320, Avg. loss: 0.154699\n",
      "Total training time: 1.21 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 234.78, NNZs: 474, Bias: -0.147529, T: 174240, Avg. loss: 0.154382\n",
      "Total training time: 1.27 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 234.83, NNZs: 467, Bias: -0.147006, T: 182160, Avg. loss: 0.152473\n",
      "Total training time: 1.33 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 234.87, NNZs: 462, Bias: -0.145447, T: 190080, Avg. loss: 0.152685\n",
      "Total training time: 1.38 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 234.91, NNZs: 456, Bias: -0.147517, T: 198000, Avg. loss: 0.150637\n",
      "Total training time: 1.44 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 234.95, NNZs: 454, Bias: -0.145145, T: 205920, Avg. loss: 0.144942\n",
      "Total training time: 1.50 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 234.98, NNZs: 452, Bias: -0.147982, T: 213840, Avg. loss: 0.151543\n",
      "Total training time: 1.56 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 235.02, NNZs: 444, Bias: -0.147508, T: 221760, Avg. loss: 0.146063\n",
      "Total training time: 1.61 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 235.05, NNZs: 443, Bias: -0.148366, T: 229680, Avg. loss: 0.145462\n",
      "Total training time: 1.66 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 235.09, NNZs: 441, Bias: -0.149644, T: 237600, Avg. loss: 0.146008\n",
      "Total training time: 1.72 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 235.13, NNZs: 438, Bias: -0.149667, T: 245520, Avg. loss: 0.143267\n",
      "Total training time: 1.77 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 235.16, NNZs: 437, Bias: -0.150056, T: 253440, Avg. loss: 0.141120\n",
      "Total training time: 1.82 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 235.19, NNZs: 433, Bias: -0.150773, T: 261360, Avg. loss: 0.142005\n",
      "Total training time: 1.88 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 235.22, NNZs: 432, Bias: -0.148550, T: 269280, Avg. loss: 0.143629\n",
      "Total training time: 1.94 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 235.25, NNZs: 429, Bias: -0.150719, T: 277200, Avg. loss: 0.139883\n",
      "Total training time: 1.99 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 235.27, NNZs: 428, Bias: -0.150008, T: 285120, Avg. loss: 0.139419\n",
      "Total training time: 2.04 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 235.30, NNZs: 427, Bias: -0.150697, T: 293040, Avg. loss: 0.140129\n",
      "Total training time: 2.09 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 235.32, NNZs: 426, Bias: -0.152003, T: 300960, Avg. loss: 0.139979\n",
      "Total training time: 2.14 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 235.35, NNZs: 423, Bias: -0.150690, T: 308880, Avg. loss: 0.140545\n",
      "Total training time: 2.20 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 235.37, NNZs: 420, Bias: -0.150666, T: 316800, Avg. loss: 0.135316\n",
      "Total training time: 2.27 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 235.40, NNZs: 419, Bias: -0.151856, T: 324720, Avg. loss: 0.138521\n",
      "Total training time: 2.32 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 235.42, NNZs: 417, Bias: -0.150967, T: 332640, Avg. loss: 0.137144\n",
      "Total training time: 2.37 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 235.45, NNZs: 417, Bias: -0.148915, T: 340560, Avg. loss: 0.133774\n",
      "Total training time: 2.42 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 235.47, NNZs: 414, Bias: -0.150665, T: 348480, Avg. loss: 0.136834\n",
      "Total training time: 2.48 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 235.49, NNZs: 413, Bias: -0.150394, T: 356400, Avg. loss: 0.134800\n",
      "Total training time: 2.53 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 235.51, NNZs: 411, Bias: -0.151240, T: 364320, Avg. loss: 0.134353\n",
      "Total training time: 2.58 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 235.53, NNZs: 409, Bias: -0.151775, T: 372240, Avg. loss: 0.137182\n",
      "Total training time: 2.63 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 235.55, NNZs: 407, Bias: -0.151508, T: 380160, Avg. loss: 0.134971\n",
      "Total training time: 2.68 seconds.\n",
      "Convergence after 48 epochs took 2.68 seconds\n",
      "-- Epoch 1\n",
      "Norm: 229.43, NNZs: 1406, Bias: -0.095927, T: 7920, Avg. loss: 3.316079\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 244.56, NNZs: 943, Bias: -0.092301, T: 15840, Avg. loss: 0.577815\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 251.28, NNZs: 767, Bias: -0.067405, T: 23760, Avg. loss: 0.281329\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 254.56, NNZs: 664, Bias: -0.071500, T: 31680, Avg. loss: 0.229558\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 256.82, NNZs: 617, Bias: -0.066833, T: 39600, Avg. loss: 0.190613\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 258.17, NNZs: 582, Bias: -0.066778, T: 47520, Avg. loss: 0.214259\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 259.25, NNZs: 548, Bias: -0.067219, T: 55440, Avg. loss: 0.176167\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 260.17, NNZs: 528, Bias: -0.066071, T: 63360, Avg. loss: 0.164489\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 260.82, NNZs: 512, Bias: -0.070531, T: 71280, Avg. loss: 0.180574\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 261.40, NNZs: 503, Bias: -0.070577, T: 79200, Avg. loss: 0.162341\n",
      "Total training time: 0.56 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 261.86, NNZs: 487, Bias: -0.073926, T: 87120, Avg. loss: 0.173629\n",
      "Total training time: 0.61 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 262.29, NNZs: 477, Bias: -0.072723, T: 95040, Avg. loss: 0.157403\n",
      "Total training time: 0.66 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 262.70, NNZs: 468, Bias: -0.075817, T: 102960, Avg. loss: 0.137329\n",
      "Total training time: 0.72 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 263.06, NNZs: 458, Bias: -0.073996, T: 110880, Avg. loss: 0.140503\n",
      "Total training time: 0.78 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 263.37, NNZs: 452, Bias: -0.077457, T: 118800, Avg. loss: 0.143342\n",
      "Total training time: 0.83 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 263.67, NNZs: 442, Bias: -0.079852, T: 126720, Avg. loss: 0.135420\n",
      "Total training time: 0.89 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 263.91, NNZs: 438, Bias: -0.080447, T: 134640, Avg. loss: 0.142897\n",
      "Total training time: 0.94 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 264.15, NNZs: 429, Bias: -0.081793, T: 142560, Avg. loss: 0.133556\n",
      "Total training time: 0.99 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 264.36, NNZs: 422, Bias: -0.079665, T: 150480, Avg. loss: 0.141456\n",
      "Total training time: 1.05 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 264.56, NNZs: 423, Bias: -0.081621, T: 158400, Avg. loss: 0.133379\n",
      "Total training time: 1.10 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 264.75, NNZs: 419, Bias: -0.083429, T: 166320, Avg. loss: 0.135926\n",
      "Total training time: 1.16 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 264.92, NNZs: 417, Bias: -0.084546, T: 174240, Avg. loss: 0.134528\n",
      "Total training time: 1.23 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 265.08, NNZs: 415, Bias: -0.087242, T: 182160, Avg. loss: 0.132191\n",
      "Total training time: 1.29 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 265.23, NNZs: 412, Bias: -0.086617, T: 190080, Avg. loss: 0.131680\n",
      "Total training time: 1.35 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 265.36, NNZs: 408, Bias: -0.083054, T: 198000, Avg. loss: 0.132189\n",
      "Total training time: 1.40 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 265.50, NNZs: 405, Bias: -0.084121, T: 205920, Avg. loss: 0.131668\n",
      "Total training time: 1.46 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 265.62, NNZs: 404, Bias: -0.088871, T: 213840, Avg. loss: 0.130835\n",
      "Total training time: 1.53 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 265.74, NNZs: 399, Bias: -0.087025, T: 221760, Avg. loss: 0.130925\n",
      "Total training time: 1.59 seconds.\n",
      "Convergence after 28 epochs took 1.59 seconds\n",
      "-- Epoch 1\n",
      "Norm: 232.40, NNZs: 1404, Bias: -0.216451, T: 7920, Avg. loss: 3.588399\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 249.88, NNZs: 973, Bias: -0.214772, T: 15840, Avg. loss: 0.669274\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 257.66, NNZs: 833, Bias: -0.194222, T: 23760, Avg. loss: 0.376986\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 261.62, NNZs: 773, Bias: -0.196110, T: 31680, Avg. loss: 0.329038\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 264.28, NNZs: 713, Bias: -0.201786, T: 39600, Avg. loss: 0.240372\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 266.04, NNZs: 673, Bias: -0.203883, T: 47520, Avg. loss: 0.237068\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 267.37, NNZs: 630, Bias: -0.200019, T: 55440, Avg. loss: 0.212315\n",
      "Total training time: 0.44 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 268.33, NNZs: 613, Bias: -0.203625, T: 63360, Avg. loss: 0.222490\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 269.14, NNZs: 592, Bias: -0.200788, T: 71280, Avg. loss: 0.205660\n",
      "Total training time: 0.56 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 269.83, NNZs: 572, Bias: -0.203455, T: 79200, Avg. loss: 0.186802\n",
      "Total training time: 0.62 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 270.37, NNZs: 574, Bias: -0.203643, T: 87120, Avg. loss: 0.201790\n",
      "Total training time: 0.67 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 270.83, NNZs: 568, Bias: -0.204903, T: 95040, Avg. loss: 0.188515\n",
      "Total training time: 0.73 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 271.23, NNZs: 564, Bias: -0.210815, T: 102960, Avg. loss: 0.182207\n",
      "Total training time: 0.78 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 271.59, NNZs: 560, Bias: -0.210627, T: 110880, Avg. loss: 0.181512\n",
      "Total training time: 0.84 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 271.88, NNZs: 555, Bias: -0.210603, T: 118800, Avg. loss: 0.188077\n",
      "Total training time: 0.89 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 272.15, NNZs: 548, Bias: -0.209003, T: 126720, Avg. loss: 0.176393\n",
      "Total training time: 0.95 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 272.39, NNZs: 543, Bias: -0.210605, T: 134640, Avg. loss: 0.175019\n",
      "Total training time: 1.00 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 272.63, NNZs: 541, Bias: -0.210547, T: 142560, Avg. loss: 0.165891\n",
      "Total training time: 1.06 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 272.82, NNZs: 537, Bias: -0.211283, T: 150480, Avg. loss: 0.169495\n",
      "Total training time: 1.11 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 273.01, NNZs: 532, Bias: -0.213223, T: 158400, Avg. loss: 0.164884\n",
      "Total training time: 1.17 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 273.17, NNZs: 528, Bias: -0.214475, T: 166320, Avg. loss: 0.169472\n",
      "Total training time: 1.22 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 273.33, NNZs: 526, Bias: -0.215639, T: 174240, Avg. loss: 0.168544\n",
      "Total training time: 1.28 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 273.47, NNZs: 522, Bias: -0.215146, T: 182160, Avg. loss: 0.168155\n",
      "Total training time: 1.33 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 273.62, NNZs: 521, Bias: -0.216161, T: 190080, Avg. loss: 0.156774\n",
      "Total training time: 1.38 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 273.75, NNZs: 518, Bias: -0.218666, T: 198000, Avg. loss: 0.160869\n",
      "Total training time: 1.43 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 273.86, NNZs: 517, Bias: -0.218114, T: 205920, Avg. loss: 0.161401\n",
      "Total training time: 1.48 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 273.97, NNZs: 514, Bias: -0.217636, T: 213840, Avg. loss: 0.162070\n",
      "Total training time: 1.53 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 274.08, NNZs: 512, Bias: -0.215878, T: 221760, Avg. loss: 0.162618\n",
      "Total training time: 1.58 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 274.18, NNZs: 512, Bias: -0.217689, T: 229680, Avg. loss: 0.158010\n",
      "Total training time: 1.63 seconds.\n",
      "Convergence after 29 epochs took 1.63 seconds\n",
      "-- Epoch 1\n",
      "Norm: 248.71, NNZs: 1905, Bias: -0.031023, T: 7920, Avg. loss: 3.234556\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 261.98, NNZs: 1254, Bias: -0.004411, T: 15840, Avg. loss: 0.518189\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 265.65, NNZs: 1031, Bias: 0.021380, T: 23760, Avg. loss: 0.332996\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 267.60, NNZs: 890, Bias: 0.012346, T: 31680, Avg. loss: 0.282801\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 268.73, NNZs: 829, Bias: 0.017460, T: 39600, Avg. loss: 0.251149\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 269.42, NNZs: 767, Bias: 0.013427, T: 47520, Avg. loss: 0.228690\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 269.87, NNZs: 720, Bias: 0.015652, T: 55440, Avg. loss: 0.200612\n",
      "Total training time: 0.41 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 270.25, NNZs: 694, Bias: 0.019188, T: 63360, Avg. loss: 0.189798\n",
      "Total training time: 0.46 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 270.50, NNZs: 674, Bias: 0.019471, T: 71280, Avg. loss: 0.181666\n",
      "Total training time: 0.51 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 270.72, NNZs: 653, Bias: 0.023485, T: 79200, Avg. loss: 0.170920\n",
      "Total training time: 0.56 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 270.91, NNZs: 628, Bias: 0.026835, T: 87120, Avg. loss: 0.160787\n",
      "Total training time: 0.62 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 271.04, NNZs: 613, Bias: 0.020261, T: 95040, Avg. loss: 0.154340\n",
      "Total training time: 0.67 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 271.20, NNZs: 591, Bias: 0.021222, T: 102960, Avg. loss: 0.141200\n",
      "Total training time: 0.72 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 271.31, NNZs: 574, Bias: 0.025849, T: 110880, Avg. loss: 0.146019\n",
      "Total training time: 0.77 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 271.43, NNZs: 558, Bias: 0.025772, T: 118800, Avg. loss: 0.138453\n",
      "Total training time: 0.82 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 271.53, NNZs: 540, Bias: 0.028945, T: 126720, Avg. loss: 0.137912\n",
      "Total training time: 0.87 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 271.62, NNZs: 532, Bias: 0.028861, T: 134640, Avg. loss: 0.135398\n",
      "Total training time: 0.92 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 271.70, NNZs: 525, Bias: 0.030830, T: 142560, Avg. loss: 0.130415\n",
      "Total training time: 0.97 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 271.77, NNZs: 519, Bias: 0.031464, T: 150480, Avg. loss: 0.132359\n",
      "Total training time: 1.02 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 271.85, NNZs: 515, Bias: 0.027603, T: 158400, Avg. loss: 0.128788\n",
      "Total training time: 1.07 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 271.92, NNZs: 508, Bias: 0.030136, T: 166320, Avg. loss: 0.127347\n",
      "Total training time: 1.12 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 271.99, NNZs: 498, Bias: 0.030138, T: 174240, Avg. loss: 0.124161\n",
      "Total training time: 1.17 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 272.04, NNZs: 492, Bias: 0.031186, T: 182160, Avg. loss: 0.123336\n",
      "Total training time: 1.22 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 272.10, NNZs: 486, Bias: 0.031701, T: 190080, Avg. loss: 0.122581\n",
      "Total training time: 1.27 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 272.15, NNZs: 474, Bias: 0.031197, T: 198000, Avg. loss: 0.121323\n",
      "Total training time: 1.32 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 272.20, NNZs: 468, Bias: 0.028307, T: 205920, Avg. loss: 0.121030\n",
      "Total training time: 1.37 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 272.25, NNZs: 462, Bias: 0.030686, T: 213840, Avg. loss: 0.115126\n",
      "Total training time: 1.42 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 272.29, NNZs: 460, Bias: 0.031176, T: 221760, Avg. loss: 0.121195\n",
      "Total training time: 1.47 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 272.33, NNZs: 456, Bias: 0.032923, T: 229680, Avg. loss: 0.118814\n",
      "Total training time: 1.52 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 272.37, NNZs: 453, Bias: 0.033272, T: 237600, Avg. loss: 0.115321\n",
      "Total training time: 1.57 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 272.41, NNZs: 451, Bias: 0.032836, T: 245520, Avg. loss: 0.118489\n",
      "Total training time: 1.62 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 272.45, NNZs: 449, Bias: 0.032022, T: 253440, Avg. loss: 0.116238\n",
      "Total training time: 1.67 seconds.\n",
      "Convergence after 32 epochs took 1.67 seconds\n",
      "-- Epoch 1\n",
      "Norm: 241.57, NNZs: 1470, Bias: -0.068772, T: 7920, Avg. loss: 3.629751\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 260.54, NNZs: 1082, Bias: -0.038165, T: 15840, Avg. loss: 0.600591\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 267.23, NNZs: 949, Bias: -0.017892, T: 23760, Avg. loss: 0.421843\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 270.29, NNZs: 878, Bias: -0.022176, T: 31680, Avg. loss: 0.358338\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 272.27, NNZs: 832, Bias: -0.023746, T: 39600, Avg. loss: 0.293840\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 273.66, NNZs: 797, Bias: -0.017731, T: 47520, Avg. loss: 0.254020\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 274.63, NNZs: 778, Bias: -0.019802, T: 55440, Avg. loss: 0.247920\n",
      "Total training time: 0.44 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 275.39, NNZs: 758, Bias: -0.018632, T: 63360, Avg. loss: 0.220922\n",
      "Total training time: 0.51 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 275.94, NNZs: 736, Bias: -0.020327, T: 71280, Avg. loss: 0.213438\n",
      "Total training time: 0.56 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 276.45, NNZs: 725, Bias: -0.026749, T: 79200, Avg. loss: 0.203958\n",
      "Total training time: 0.61 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 276.83, NNZs: 715, Bias: -0.024034, T: 87120, Avg. loss: 0.188710\n",
      "Total training time: 0.67 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 277.18, NNZs: 702, Bias: -0.021663, T: 95040, Avg. loss: 0.187833\n",
      "Total training time: 0.72 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 277.49, NNZs: 694, Bias: -0.019000, T: 102960, Avg. loss: 0.163650\n",
      "Total training time: 0.77 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 277.76, NNZs: 685, Bias: -0.023561, T: 110880, Avg. loss: 0.165191\n",
      "Total training time: 0.82 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 278.00, NNZs: 675, Bias: -0.023477, T: 118800, Avg. loss: 0.159120\n",
      "Total training time: 0.88 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 278.22, NNZs: 665, Bias: -0.019389, T: 126720, Avg. loss: 0.156545\n",
      "Total training time: 0.93 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 278.40, NNZs: 658, Bias: -0.019288, T: 134640, Avg. loss: 0.161927\n",
      "Total training time: 0.99 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 278.59, NNZs: 652, Bias: -0.017328, T: 142560, Avg. loss: 0.151247\n",
      "Total training time: 1.04 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 278.73, NNZs: 645, Bias: -0.019373, T: 150480, Avg. loss: 0.156757\n",
      "Total training time: 1.11 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 278.88, NNZs: 639, Bias: -0.017472, T: 158400, Avg. loss: 0.154165\n",
      "Total training time: 1.17 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 279.03, NNZs: 635, Bias: -0.016887, T: 166320, Avg. loss: 0.151229\n",
      "Total training time: 1.22 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 279.15, NNZs: 631, Bias: -0.014643, T: 174240, Avg. loss: 0.150610\n",
      "Total training time: 1.28 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 279.28, NNZs: 629, Bias: -0.015296, T: 182160, Avg. loss: 0.148663\n",
      "Total training time: 1.34 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 279.39, NNZs: 619, Bias: -0.015350, T: 190080, Avg. loss: 0.146347\n",
      "Total training time: 1.41 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 279.50, NNZs: 619, Bias: -0.015926, T: 198000, Avg. loss: 0.140950\n",
      "Total training time: 1.47 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 279.61, NNZs: 617, Bias: -0.016441, T: 205920, Avg. loss: 0.140937\n",
      "Total training time: 1.53 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 279.71, NNZs: 614, Bias: -0.015496, T: 213840, Avg. loss: 0.142103\n",
      "Total training time: 1.59 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 279.80, NNZs: 610, Bias: -0.015973, T: 221760, Avg. loss: 0.138613\n",
      "Total training time: 1.65 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 279.89, NNZs: 602, Bias: -0.015946, T: 229680, Avg. loss: 0.140711\n",
      "Total training time: 1.70 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 279.97, NNZs: 603, Bias: -0.013432, T: 237600, Avg. loss: 0.135561\n",
      "Total training time: 1.75 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 280.06, NNZs: 600, Bias: -0.015954, T: 245520, Avg. loss: 0.134970\n",
      "Total training time: 1.80 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 280.13, NNZs: 599, Bias: -0.014744, T: 253440, Avg. loss: 0.139848\n",
      "Total training time: 1.86 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 280.20, NNZs: 597, Bias: -0.015127, T: 261360, Avg. loss: 0.139152\n",
      "Total training time: 1.91 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 280.27, NNZs: 594, Bias: -0.017745, T: 269280, Avg. loss: 0.133736\n",
      "Total training time: 1.96 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 280.34, NNZs: 589, Bias: -0.014478, T: 277200, Avg. loss: 0.135706\n",
      "Total training time: 2.02 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 280.41, NNZs: 584, Bias: -0.015557, T: 285120, Avg. loss: 0.135654\n",
      "Total training time: 2.08 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 280.47, NNZs: 580, Bias: -0.014873, T: 293040, Avg. loss: 0.134600\n",
      "Total training time: 2.15 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 280.53, NNZs: 573, Bias: -0.015189, T: 300960, Avg. loss: 0.134346\n",
      "Total training time: 2.22 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 280.59, NNZs: 575, Bias: -0.014541, T: 308880, Avg. loss: 0.132588\n",
      "Total training time: 2.28 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 280.64, NNZs: 574, Bias: -0.013595, T: 316800, Avg. loss: 0.134033\n",
      "Total training time: 2.34 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 280.70, NNZs: 573, Bias: -0.013920, T: 324720, Avg. loss: 0.132181\n",
      "Total training time: 2.40 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 280.75, NNZs: 574, Bias: -0.014825, T: 332640, Avg. loss: 0.130305\n",
      "Total training time: 2.45 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 280.80, NNZs: 571, Bias: -0.013356, T: 340560, Avg. loss: 0.132873\n",
      "Total training time: 2.51 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 280.85, NNZs: 569, Bias: -0.015084, T: 348480, Avg. loss: 0.131745\n",
      "Total training time: 2.56 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 280.90, NNZs: 567, Bias: -0.014527, T: 356400, Avg. loss: 0.130586\n",
      "Total training time: 2.62 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 280.94, NNZs: 565, Bias: -0.013152, T: 364320, Avg. loss: 0.131156\n",
      "Total training time: 2.67 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 280.99, NNZs: 567, Bias: -0.013429, T: 372240, Avg. loss: 0.129681\n",
      "Total training time: 2.73 seconds.\n",
      "Convergence after 47 epochs took 2.73 seconds\n",
      "-- Epoch 1\n",
      "Norm: 252.60, NNZs: 1955, Bias: -0.200026, T: 7920, Avg. loss: 3.882052\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 268.22, NNZs: 1401, Bias: -0.166005, T: 15840, Avg. loss: 0.661560\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 273.32, NNZs: 1201, Bias: -0.151773, T: 23760, Avg. loss: 0.365353\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 275.85, NNZs: 1084, Bias: -0.151664, T: 31680, Avg. loss: 0.302199\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 277.35, NNZs: 1011, Bias: -0.140413, T: 39600, Avg. loss: 0.254323\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 278.40, NNZs: 955, Bias: -0.135930, T: 47520, Avg. loss: 0.222056\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 279.09, NNZs: 917, Bias: -0.132155, T: 55440, Avg. loss: 0.207755\n",
      "Total training time: 0.46 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 279.63, NNZs: 893, Bias: -0.132115, T: 63360, Avg. loss: 0.199158\n",
      "Total training time: 0.51 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 280.08, NNZs: 865, Bias: -0.126462, T: 71280, Avg. loss: 0.186130\n",
      "Total training time: 0.56 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 280.44, NNZs: 836, Bias: -0.123086, T: 79200, Avg. loss: 0.185699\n",
      "Total training time: 0.61 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 280.77, NNZs: 817, Bias: -0.123232, T: 87120, Avg. loss: 0.167772\n",
      "Total training time: 0.67 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 281.03, NNZs: 803, Bias: -0.123242, T: 95040, Avg. loss: 0.160734\n",
      "Total training time: 0.73 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 281.27, NNZs: 786, Bias: -0.124037, T: 102960, Avg. loss: 0.157415\n",
      "Total training time: 0.78 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 281.47, NNZs: 767, Bias: -0.124719, T: 110880, Avg. loss: 0.154340\n",
      "Total training time: 0.85 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 281.66, NNZs: 759, Bias: -0.121285, T: 118800, Avg. loss: 0.157186\n",
      "Total training time: 0.90 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 281.84, NNZs: 744, Bias: -0.121933, T: 126720, Avg. loss: 0.146979\n",
      "Total training time: 0.96 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 281.98, NNZs: 733, Bias: -0.119618, T: 134640, Avg. loss: 0.153435\n",
      "Total training time: 1.01 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 282.11, NNZs: 724, Bias: -0.117469, T: 142560, Avg. loss: 0.144929\n",
      "Total training time: 1.07 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 282.24, NNZs: 722, Bias: -0.118102, T: 150480, Avg. loss: 0.142449\n",
      "Total training time: 1.13 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 282.36, NNZs: 710, Bias: -0.118065, T: 158400, Avg. loss: 0.142137\n",
      "Total training time: 1.18 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 282.46, NNZs: 702, Bias: -0.115650, T: 166320, Avg. loss: 0.145615\n",
      "Total training time: 1.24 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 282.55, NNZs: 695, Bias: -0.116873, T: 174240, Avg. loss: 0.141955\n",
      "Total training time: 1.29 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 282.65, NNZs: 687, Bias: -0.115194, T: 182160, Avg. loss: 0.139708\n",
      "Total training time: 1.35 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 282.74, NNZs: 681, Bias: -0.115729, T: 190080, Avg. loss: 0.138068\n",
      "Total training time: 1.40 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 282.83, NNZs: 679, Bias: -0.115689, T: 198000, Avg. loss: 0.132396\n",
      "Total training time: 1.46 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 282.91, NNZs: 674, Bias: -0.116165, T: 205920, Avg. loss: 0.136488\n",
      "Total training time: 1.53 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 282.99, NNZs: 670, Bias: -0.115669, T: 213840, Avg. loss: 0.133717\n",
      "Total training time: 1.59 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 283.06, NNZs: 669, Bias: -0.114370, T: 221760, Avg. loss: 0.137500\n",
      "Total training time: 1.65 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 283.13, NNZs: 668, Bias: -0.114388, T: 229680, Avg. loss: 0.133444\n",
      "Total training time: 1.71 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 283.20, NNZs: 664, Bias: -0.114002, T: 237600, Avg. loss: 0.133911\n",
      "Total training time: 1.76 seconds.\n",
      "Convergence after 30 epochs took 1.76 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  11 out of  11 | elapsed:   22.5s finished\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  1.8min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][LibLinear][LibLinear]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  7.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline]  (step 2 of 2) Processing stackingclassifier, total=19.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paulineclaes/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('columntransformer',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('standardscaler',\n",
       "                                                  StandardScaler(with_mean=False),\n",
       "                                                  ['n_words']),\n",
       "                                                 ('tfidfvectorizer-1',\n",
       "                                                  TfidfVectorizer(lowercase=False,\n",
       "                                                                  max_df=0.8,\n",
       "                                                                  min_df=0.001,\n",
       "                                                                  ngram_range=(1,\n",
       "                                                                               2)),\n",
       "                                                  'text'),\n",
       "                                                 ('tfidfvectorizer-2',\n",
       "                                                  TfidfVectorizer(analyzer='char_wb',\n",
       "                                                                  ngram_range=(1,\n",
       "                                                                               3)),\n",
       "                                                  'text'),\n",
       "                                                 ('tfidf...\n",
       "                ('stackingclassifier',\n",
       "                 StackingClassifier(estimators=[('log_reg',\n",
       "                                                 LogisticRegression(max_iter=2500,\n",
       "                                                                    multi_class='ovr',\n",
       "                                                                    random_state=1,\n",
       "                                                                    verbose=1)),\n",
       "                                                ('sgd',\n",
       "                                                 SGDClassifier(penalty='l1',\n",
       "                                                               random_state=1,\n",
       "                                                               verbose=1)),\n",
       "                                                ('linear_svc',\n",
       "                                                 LinearSVC(random_state=1,\n",
       "                                                           verbose=1))],\n",
       "                                    final_estimator=LogisticRegression(max_iter=1000,\n",
       "                                                                       multi_class='multinomial'),\n",
       "                                    verbose=1))],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combo_stacked.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "stunning-visit",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_combo_stacked = combo_stacked.predict(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "surprising-ordinance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8145454545454546\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_dev, y_pred_combo_stacked))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ancient-maintenance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ARA       0.79      0.83      0.81       100\n",
      "         DEU       0.88      0.92      0.90       100\n",
      "         FRA       0.79      0.81      0.80       100\n",
      "         HIN       0.76      0.78      0.77       100\n",
      "         ITA       0.83      0.91      0.87       100\n",
      "         JPN       0.75      0.73      0.74       100\n",
      "         KOR       0.71      0.76      0.73       100\n",
      "         SPA       0.81      0.79      0.80       100\n",
      "         TEL       0.81      0.80      0.80       100\n",
      "         TUR       0.91      0.84      0.87       100\n",
      "         ZHO       0.92      0.79      0.85       100\n",
      "\n",
      "    accuracy                           0.81      1100\n",
      "   macro avg       0.82      0.81      0.81      1100\n",
      "weighted avg       0.82      0.81      0.81      1100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_dev, y_pred_combo_stacked))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
